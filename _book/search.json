[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "",
    "text": "About the Book\nThe R programming language is a remarkably powerful tool for data analysis and visualization, but its steep learning curve can be intimidating for some. If you just want to automate repetitive tasks or visualize your data, without the need for complex math, R for the Rest of Us is for you.\nInside you’ll find a crash course in R, a quick tour of the RStudio programming environment, and a collection of real-word applications that you can put to use right away. You’ll learn how to create informative visualizations, streamline report generation, and develop interactive websites—whether you’re a seasoned R user or have never written a line of R code.\nYou’ll also learn how to:\n\nManipulate, clean, and parse your data with tidyverse packages like dplyr and tidyr to make data science operations more user-friendly\nCreate stunning and customized plots, graphs, and charts with ggplot2 to effectively communicate your data insights\nImport geospatial data and write code to produce visually appealing maps automatically\nGenerate dynamic reports, presentations, and interactive websites with R Markdown and Quarto that seamlessly integrate code, text, and graphics\nDevelop custom functions and packages tailored to your specific needs, allowing you to extend R’s functionality and automate complex tasks\n\nUnlock a treasure trove of techniques to transform the way you work. With R for the Rest of Us, you’ll discover the power of R to get stuff done. No advanced statistics degree required.\n\n\nAbout the Author\n\nDavid Keyes is the founder and CEO of R for the Rest of Us, which offers online courses, workshops, and custom training sessions that help organizations take control of their data. He has a PhD in anthropology from UC San Diego, as well as a master’s degree in education from Ohio State, and has dedicated his professional life to teaching people to embrace R as the most powerful tool for data analysis and visualization.\n\n\nAcknowldgements\nThis book is a testament to the many members of the R community who share their knowledge freely and encourage others generously. I call myself self-taught, but really what I am is community-taught. Throughout this book, you will read about several R users from whom I have learned so much; still, many others go unmentioned. To everyone who has worked to develop R, share your knowledge about R, and make R a welcoming place, thank you.\nI’d also like to thank the team at R for the Rest of Us. Working directly with talented R users has taught me so much about what is possible with R.\nFinally, I’d like to thank people who have provided feedback as I’ve written this book. Technical reviewer Rita Giordano has helped me make sure everything works and suggested great ideas for improvement. My editor, Frances Saux, has provided fantastic input along the way. To Bill Pollock and the entire team at No Starch: thank you for taking a flyer on me and my strange idea to write a book about nonstatistical uses of a tool created for statistics.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Isn’t R Just for Statistical Analysis?\nMany people think of R as simply a tool for hardcore statistical analysis, but it can do much more than manipulate numerical values. After all, every R user must illuminate their findings and communicate their results somehow, whether that’s via data visualizations, reports, websites, or presentations. Also, the more you use R, the more you’ll find yourself wanting to automate tasks you currently do manually.\nAs a qualitatively trained anthropologist without a quantitative background, I used to feel ashamed about using R for my visualization and communication tasks. But the fact is, R is good at these jobs. The ggplot2 package is the tool of choice for many top information designers. Users around the world have taken advantage of R’s ability to automate reporting to make their work more efficient. Rather than simply replacing other tools, R can perform tasks that you’re probably already doing, like generating reports and tables, better than your existing workflow.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-this-book-is-for",
    "href": "introduction.html#who-this-book-is-for",
    "title": "Introduction",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nNo matter your background, using R can transform your work. This book is for you if you’re either a current R user keen to explore its uses for visualization and communication or a non-R user wondering if R is right for you. I’ve written R for the Rest of Us so that it should make sense whether or not you’ve ever written a line of R code. But even if you’ve written entire R programs, the book should help you learn plenty of new techniques to up your game.\nR is a great tool for anyone who works with data. Maybe you’re a researcher looking for a new way to share your results. Perhaps you’re a journalist looking to analyze public data more efficiently. Or maybe you’re a data analyst tired of working in expensive, proprietary tools. If you have to work with data, you will get value from R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#about-this-book",
    "href": "introduction.html#about-this-book",
    "title": "Introduction",
    "section": "About This Book",
    "text": "About This Book\nEach chapter focuses on one use of the R language and includes examples of real R projects that employ the techniques covered. I’ll dive into the project code, breaking the programs down to help you understand how they work, and suggest ways of going beyond the example. The book has three parts, outlined here.\nIn Part I, you’ll learn how to use R to visualize data.\n\nChapter 1: An R Programming Crash Course Introduces the RStudio programming environment and the foundational R syntax you’ll need to understand the rest of the book.\nChapter 2: Principles of Data Visualization Breaks down a visualization created for Scientific American on drought conditions in the United States. In doing so, this chapter introduces the ggplot2 package for data visualization and addresses important principles that can help you make high-quality graphics.\nChapter 3: Custom Data Visualization Themes Describes how journalists at the BBC made a custom theme for the ggplot2 data visual- ization package. As the chapter walks you through the package they created, you’ll learn how to make your own theme.\nChapter 4: Maps and Geospatial Data Explores the process of making maps in R using simple features data. You’ll learn how to write map-making code, find geospatial data, choose appropriate projections, and apply data visualization principles to make your map appealing.\nChapter 5: Designing Effective Tables Shows you how to use the gt package to make high-quality tables in R. With guidance from R table connoisseur Tom Mock, you’ll learn the design principles to present your table data effectively.\n\nPart II focuses on using R Markdown to communicate efficiently. You’ll learn how to incorporate visualizations like the ones discussed in Part I into reports, slideshow presentations, and static websites generated entirely using R code.\n\nChapter 6: R Markdown Reports Introduces R Markdown, a tool that allows you to generate a professional report in R. This chapter covers the structure of an R Markdown document, shows you how to use inline code to automatically update your report’s text when data values change, and discusses the tool’s many export options.\nChapter 7: Parameterized Reporting Covers one of the advantages of using R Markdown: the ability to produce multiple reports at the same time using a technique called parameterized reporting. You’ll see how staff members at the Urban Institute used R to generate fiscal briefs for all 50 US states. In the process, you’ll learn how parameterized reporting works and how you can use it.\nChapter 8: Slideshow Presentations Explains how to use R Markdown to make slides with the xaringan package. You’ll learn how to make your own presentations, adjust your content to fit on a slide, and add effects to your slideshow.\nChapter 9: Websites Shows you how to create your own website with R Markdown and the distill package. By examining a website about COVID-19 rates in Westchester County, New York, you’ll see how to create pages on your site, add interactivity through R packages, and deploy your website in multiple ways.\nChapter 10: Quarto Explains how to use Quarto, the next-generation version of R Markdown. You’ll learn how to use Quarto for all of the projects you previously used R Markdown for (reports, parameterized reporting, slideshow presentations, and websites).\n\nPart III focuses on ways you can use R to automate your work and share it with others.\n\nChapter 11: Automatically Accessing Online Data Explores two R packages that let you automatically import data from the internet: googlesheets4 for working with Google Sheets and tidycensus for working with US Census Bureau data. You’ll learn how the packages work and how to use them to automate the process of accessing data.\nChapter 12: Creating Functions and Packages Shows you how to create your own functions and packages and share them with others, which is one of R’s major benefits. Bundling your custom functions into a package can enable other R users to streamline their work, as you’ll read about with the packages that a group of R developers built for researchers working at the Moffitt Cancer Center.\n\nBy the end of this book, you should be able to use R for a wide range of nonstatistical tasks. You’ll know how to effectively visualize data and com- municate your findings using maps and tables. You’ll be able to integrate your results into reports using R Markdown, as well as efficiently generate slideshow presentations and websites. And you’ll understand how to automate many tedious tasks using packages others have built or ones you develop yourself. Let’s dive in!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "crash-course.html",
    "href": "crash-course.html",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "",
    "text": "Setting Up\nYou’ll need two pieces of software to use R effectively. The first is R itself, which provides the underlying computational tools that make the language work. The second is an integrated development environment (IDE) like RStudio. This coding platform simplifies working with R. The best way to understand the relationship between R and RStudio is with this analogy from Chester Ismay and Albert Kim’s book Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse: R is the engine that powers your data, and RStudio is like the dashboard that provides a user-friendly interface.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#setting-up",
    "href": "crash-course.html#setting-up",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "",
    "text": "Installing R and RStudio\nTo download R, go to https://cloud.r-project.org and choose the link for your operating system. Once you’ve installed it, open the file. This should open an interface, like the one shown in Figure 1.1, that lets you work with R on your operating system’s command line. For example, enter 2 + 2, and you should see 4.\n\n\n\n\n\nFigure 1.1: The R console\n\n\nA few brave souls work with R using only this command line, but most opt to use RStudio, which provides a way to see your files, the output of your code, and more. You can download RStudio at https://posit.co/download/rstudio-desktop/. Install RStudio as you would any other app and open it.\nExploring the RStudio Interface\nThe first time you open RStudio, you should see the three panes shown in Figure 1.2.\n\n\n\n\n\nFigure 1.2: The RStudio editor\n\n\nThe left pane should look familiar. It’s similar to the screen you saw when working in R on the command line. This is known as the console. You’ll use it to enter code and see the results. This pane has several tabs, such as Terminal and Background Jobs, for more advanced uses. For now, you’ll stick to the default tab.\nAt the bottom right, the files pane shows all of the files on your computer. You can click any file to open it within RStudio. Finally, at the top right is the environment pane, which shows the objects that are available to you when working in RStudio. Objects are discussed in “Saving Data as Objects” on page 11.\nThere is one more pane that you’ll typically use when working in RStudio, but to see it, first you need to create an R script file.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#r-script-files",
    "href": "crash-course.html#r-script-files",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "R Script Files",
    "text": "R Script Files\nIf you write all of your code in the console, you won’t have any record of it. Say you sit down today and import your data, analyze it, and then make some graphs. If you run these operations in the console, you’ll have to re-create that code from scratch tomorrow. But if you write your code in files instead, you can run it multiple times.\nR script files, which use the .R extension, save your code so you can run it later. To create an R script file, go to File &gt; New File &gt; R Script, and the script file pane should appear in the top left of RStudio, as shown in Figure 1.3. Save this file in your Documents folder as sample-code.R.\n\n\n\n\n\nFigure 1.3: The script file pane (top left)\n\n\nNow you can enter R code into the new pane to add it to your script file. For example, try entering 2 + 2 in the script file pane to perform a simple addition operation.\nTo run a script file, click Run or use the keyboard shortcut command-enter on macOS or ctrl-enter on Windows. The result (4, in this case) should show up in the console pane.\nYou now have a working programming environment. Next you’ll use it to write some simple R code.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#basic-r-syntax",
    "href": "crash-course.html#basic-r-syntax",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Basic R Syntax",
    "text": "Basic R Syntax\nIf you’re trying to learn R, you probably want to perform more complex operations than 2 + 2, but understanding the fundamentals will prepare you to do more serious data analysis tasks later in this chapter. Let’s cover some of these basics.\nArithmetic Operators\nBesides +, R supports the common arithmetic operators - for subtraction, * for multiplication, and / for division. Try entering the following in the console:\n\n2 - 1\n\n[1] 1\n\n\n\n3 * 3\n\n[1] 9\n\n\n\n16 / 4\n\n[1] 4\n\n\nAs you can see, R returns the result of each calculation you enter. You don’t have to add the spaces around operators as shown here, but doing so makes your code much more readable.\nYou can also use parentheses to perform multiple operations at once and see their result. The parentheses specify the order in which R will evaluate the expression. Try running the following:\n\n2 * (2 + 1)\n\n[1] 6\n\n\nThis code first evaluates the expression within the parentheses, 2 + 1, before multiplying the result by 2 in order to get 6.\nR also has more advanced arithmetic operators, such as ** to calculate exponents:\n\n\n[1] 8\n\n\nThis is equivalent to 23, which returns 8.\nTo get the remainder of a division operation, you can use the %% operator:\n\n10 %% 3\n\n[1] 1\n\n\nDividing 10 by 3 produces a remainder of 1, the value R returns.\nYou won’t need to use these advanced arithmetic operators for the activities in this book, but they’re good to know nonetheless.\nComparison Operators\nR also uses comparison operators, which let you test how one value compares to another. R will return either TRUE or FALSE. For example, enter 2 &gt; 1 in the console:\n\n2 &gt; 1\n\n[1] TRUE\n\n\nR should return TRUE, because 2 is greater than 1.\nOther common comparison operators include less than (&lt;), greater than or equal to (&gt;=), less than or equal to (&lt;=), equal to (==), and not equal to (!=). Here are some examples:\n\n498 == 498\n\n[1] TRUE\n\n\n\n2 != 2\n\n[1] FALSE\n\n\nWhen you enter 498 == 498 in the console, R should return TRUE because the two values are equal. If you run 2 != 2 in the console, R should return FALSE because 2 does not not equal 2.\nYou’ll rarely use comparison operators to directly test how one value compares to another; instead, you’ll use them to perform tasks like keeping only data where a value is greater than a certain threshold. You’ll see com- parison operators used in this way in “tidyverse Functions” (Section 1.6.1).\nFunctions\nYou can perform even more useful operations by making use of R’s many functions, predefined sections of code that let you efficiently do specific things. Functions have a name and a set of parentheses containing arguments, which are values that affect the function’s behavior.\nConsider the print() function, which displays information:\n\nprint(x = 1.1)\n\n[1] 1.1\n\n\nThe name of the print() function is print. Within the function’s parentheses, you specify the argument name – x, in this case — followed by the equal sign (=) and a value for the function to display. This code will print the number 1.1.\nTo separate multiple arguments, you use commas. For example, you can use the print() function’s digits argument to indicate how many digits of a number to display:\n\nprint(x = 1.1, digits = 1)\n\n[1] 1\n\n\nThis code will display only one digit (in other words, a whole number).\nUsing these two arguments allows you to do something specific (display results) while also giving you the flexibility to change the function’s behavior.\n\n\n\n\n\n\nNote\n\n\n\nFor a list of all functions built into R, see https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html.\n\n\nA common R pattern is using a function within a function. For example, if you wanted to calculate the mean, or average, of the values 10, 20, and 30, you could use the mean() function to operate on the result of the c() function like so:\n\nmean(x = c(10, 20, 30))\n\n[1] 20\n\n\nThe c() function combines multiple values into one, which is necessary because the mean() function accepts only one argument. This is why the code has two matching sets of open and close parentheses: one for mean() and a nested one for c().\nThe value after the equal sign in this example, c(10, 20, 30), tells R to use the values 10, 20, and 30 to calculate the mean. Running this code in the console returns the value 20.\nThe functions median() and mode() work with c() in the same way. To learn how to use a function and what arguments it accepts, enter ? followed by the function’s name in the console to see the function’s help file.\nNext, let’s look at how to import data for your R programs to work with.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#working-with-data",
    "href": "crash-course.html#working-with-data",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Working with Data",
    "text": "Working with Data\nR lets you do all of the same data manipulation tasks you might perform in a tool like Excel, such as calculating averages or totals. Conceptually, however, working with data in R is very different from working with Excel, where your data and analysis code live in the same place: a spreadsheet. While the data you work with in R might look similar to the data you work with in Excel, it typically comes from some external file, so you have to run code to import it.\nImporting Data\nYou’ll import data from a comma-separated values (CSV) file, a text file that holds a series of related values separated by commas. You can open CSV files using most spreadsheet applications, which use columns rather than commas as separators. For example, Figure 1.4 shows the population-by-state.csv file in Excel.\n\n\n\n\n\nFigure 1.4: The population-by-state.csv file in Excel\n\n\nTo work with this file in R, download it from https://data.rfortherestofus.com/population-by-state.csv. Save it to a location on your computer, such as your Documents folder.\nNext, to import the file into R, add a line like the following to the sample-code.R file you created earlier in this chapter, replacing my filepath with the path to the file’s location on your system:\n\nread.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nThe file argument in the read.csv() function specifies the path to the file to open.\nThe read.csv() function can accept additional optional arguments, separated by commas. For example, the following line uses the skip argument in addition to file to import the same file but skip the first row:\n\nread.csv(\n  file = \"/Users/davidkeyes/Documents/population-by-state.csv\",\n  skip = 1\n)\n\nTo learn about additional arguments for this function, enter ?read.csv() in the console to see its help file.\nAt this point, you can run the code to import your data (without the skip argument). Highlight the line you want to run in the script file pane in RStudio and click Run. You should see the following output in the console pane:\n\n\n   rank                State      Pop  Growth  Pop2018  Pop2010 growthSince2010\n1     1           California 39613493  0.0038 39461588 37319502          0.0615\n2     2                Texas 29730311  0.0385 28628666 25241971          0.1778\n3     3              Florida 21944577  0.0330 21244317 18845537          0.1644\n4     4             New York 19299981 -0.0118 19530351 19399878         -0.0051\n5     5         Pennsylvania 12804123  0.0003 12800922 12711160          0.0073\n6     6             Illinois 12569321 -0.0121 12723071 12840503         -0.0211\n7     7                 Ohio 11714618  0.0033 11676341 11539336          0.0152\n8     8              Georgia 10830007  0.0303 10511131  9711881          0.1151\n9     9       North Carolina 10701022  0.0308 10381615  9574323          0.1177\n10   10             Michigan  9992427  0.0008  9984072  9877510          0.0116\n11   11           New Jersey  8874520 -0.0013  8886025  8799446          0.0085\n12   12             Virginia  8603985  0.0121  8501286  8023699          0.0723\n13   13           Washington  7796941  0.0363  7523869  6742830          0.1563\n14   14              Arizona  7520103  0.0506  7158024  6407172          0.1737\n15   15            Tennessee  6944260  0.0255  6771631  6355311          0.0927\n16   16        Massachusetts  6912239  0.0043  6882635  6566307          0.0527\n17   17              Indiana  6805663  0.0165  6695497  6490432          0.0486\n18   18             Missouri  6169038  0.0077  6121623  5995974          0.0289\n19   19             Maryland  6065436  0.0049  6035802  5788645          0.0478\n20   20             Colorado  5893634  0.0356  5691287  5047349          0.1677\n21   21            Wisconsin  5852490  0.0078  5807406  5690475          0.0285\n22   22            Minnesota  5706398  0.0179  5606249  5310828          0.0745\n23   23       South Carolina  5277830  0.0381  5084156  4635649          0.1385\n24   24              Alabama  4934193  0.0095  4887681  4785437          0.0311\n25   25            Louisiana  4627002 -0.0070  4659690  4544532          0.0181\n26   26             Kentucky  4480713  0.0044  4461153  4348181          0.0305\n27   27               Oregon  4289439  0.0257  4181886  3837491          0.1178\n28   28             Oklahoma  3990443  0.0127  3940235  3759944          0.0613\n29   29          Connecticut  3552821 -0.0052  3571520  3579114         -0.0073\n30   30                 Utah  3310774  0.0499  3153550  2775332          0.1929\n31   31          Puerto Rico  3194374  0.0003  3193354  3721525         -0.1416\n32   32               Nevada  3185786  0.0523  3027341  2702405          0.1789\n33   33                 Iowa  3167974  0.0061  3148618  3050745          0.0384\n34   34             Arkansas  3033946  0.0080  3009733  2921964          0.0383\n35   35          Mississippi  2966407 -0.0049  2981020  2970548         -0.0014\n36   36               Kansas  2917224  0.0020  2911359  2858190          0.0207\n37   37           New Mexico  2105005  0.0059  2092741  2064552          0.0196\n38   38             Nebraska  1951996  0.0137  1925614  1829542          0.0669\n39   39                Idaho  1860123  0.0626  1750536  1570746          0.1842\n40   40        West Virginia  1767859 -0.0202  1804291  1854239         -0.0466\n41   41               Hawaii  1406430 -0.0100  1420593  1363963          0.0311\n42   42        New Hampshire  1372203  0.0138  1353465  1316762          0.0421\n43   43                Maine  1354522  0.0115  1339057  1327629          0.0203\n44   44              Montana  1085004  0.0229  1060665   990697          0.0952\n45   45         Rhode Island  1061509  0.0030  1058287  1053959          0.0072\n46   46             Delaware   990334  0.0257   965479   899593          0.1009\n47   47         South Dakota   896581  0.0204   878698   816166          0.0985\n48   48         North Dakota   770026  0.0158   758080   674715          0.1413\n49   49               Alaska   724357 -0.0147   735139   713910          0.0146\n50   50 District of Columbia   714153  0.0180   701547   605226          0.1800\n51   51              Vermont   623251 -0.0018   624358   625879         -0.0042\n52   52              Wyoming   581075  0.0060   577601   564487          0.0294\n   Percent    density\n1   0.1184   254.2929\n2   0.0889   113.8081\n3   0.0656   409.2229\n4   0.0577   409.5400\n5   0.0383   286.1704\n6   0.0376   226.3967\n7   0.0350   286.6944\n8   0.0324   188.3054\n9   0.0320   220.1041\n10  0.0299   176.7351\n11  0.0265  1206.7609\n12  0.0257   217.8776\n13  0.0233   117.3249\n14  0.0225    66.2016\n15  0.0208   168.4069\n16  0.0207   886.1845\n17  0.0203   189.9644\n18  0.0184    89.7419\n19  0.0181   624.8518\n20  0.0176    56.8653\n21  0.0175   108.0633\n22  0.0171    71.6641\n23  0.0158   175.5707\n24  0.0147    97.4271\n25  0.0138   107.0966\n26  0.0134   113.4760\n27  0.0128    44.6872\n28  0.0119    58.1740\n29  0.0106   733.7507\n30  0.0099    40.2918\n31  0.0095   923.4964\n32  0.0095    29.0195\n33  0.0095    56.7158\n34  0.0091    58.3059\n35  0.0089    63.2186\n36  0.0087    35.6808\n37  0.0063    17.3540\n38  0.0058    25.4087\n39  0.0056    22.5079\n40  0.0053    73.5443\n41  0.0042   218.9678\n42  0.0041   153.2674\n43  0.0040    43.9167\n44  0.0032     7.4547\n45  0.0032  1026.6044\n46  0.0030   508.1242\n47  0.0027    11.8265\n48  0.0023    11.1596\n49  0.0022     1.2694\n50  0.0021 11707.4262\n51  0.0019    67.6197\n52  0.0017     5.9847\n\n\nThis is R’s way of confirming that it imported the CSV file and understands the data within it. Four variables show each state’s rank (in terms of population size), name, current population, population growth between the Pop and Pop2018 variables (expressed as a percentage), and 2018 population. Several other variables are hidden in the output, but you’ll see them if you import this CSV file yourself.\nYou might think you’re ready to work with your data now, but all you’ve really done at this point is display the result of running the code that imports the data. To actually use the data, you need to save it to an object.\nSaving Data as Objects\nTo save your data for reuse, you need to create an object. For the purposes of this discussion, an object is a data structure that is stored for later use. To create an object, update your data-importing syntax so it looks like this:\nNow this line of code contains the &lt;- assignment operator, which takes what follows it and assigns it to the item on the left. To the left of the assign- ment operator is the population_data object. Put together, the whole line imports the CSV file and assigns it to an object called population_data.\nWhen you run this code, you should see population_data in your environment pane, as shown in Figure 1.5.\n\n\n\n\n\nFigure 1.5: The population_data object in the environment pane\n\n\nThis message confirms that your data import worked and that the population_data object is ready for future use. Now, instead of having to rerun the code to import the data, you can simply enter population_data in an R script file or in the console to output the data.\nData imported to an object in this way is known as a data frame. You can see that the population_data data frame has 52 observations and 9 variables. Variables are the data frame’s columns, each of which represents some value (for example, the population of each state). As you’ll see throughout the book, you can add new variables or modify existing ones using R code. The 52 observations come from the 50 states, as well as the District of Columbia and Puerto Rico.\n\npopulation_data &lt;- read.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nInstalling Packages\nThe read.csv() function you’ve been using, as well as the mean() and c() functions you saw earlier, comes from base R, the set of built-in R functions. To use base R functions, you simply enter their names. However, one of the benefits of R being an open source language is that anyone can create their own code and share it with others. R users around the world make R packages, which provide custom functions to accomplish specific goals.\nThe best analogy for understanding packages also comes from the book Statistical Inference via Data Science. The functionality in base R is like the features built into a smartphone. A smartphone can do a lot on its own, but you usually want to install additional apps for specific tasks. Packages are like apps, giving you functionality beyond what’s built into base R. In Chapter 12, you’ll create your own R package.\nYou can install packages using the install.packages() function. You’ll be working with the tidyverse package, which provides a range of functions for data import, cleaning, analysis, visualization, and more. To install it, enter install.packages(\"tidyverse\"). Typically, you’ll enter package installation code in the console rather than in a script file because you need to install a pack- age only once on your computer to access its code in the future.\nTo confirm that the tidyverse package has been installed correctly, click the Packages tab on the bottom-right pane in RStudio. Search for tidyverse, and you should see it pop up.\nNow that you’ve installed the tidyverse, you’ll put it to use. Although you need to install packages only once per computer, you need to load them each time you restart RStudio. Return to the sample-code.R file and reimport your data using a function from the tidyverse package (your filepath will look slightly different):\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nAt the top of the script, the line library(tidyverse) loads the tidyverse package. Then, the package’s read_csv() function imports the data. Note the underscore (_) in place of the period (.) in the function’s name; this differs from the base R function you used earlier. Using read_csv() to import CSV files achieves the same goal of creating an object, however — in this case, one called population_data_2. Enter population_data_2 in the console, and you should see this output:\n\npopulation_data_2 &lt;- read_csv(file = \"data/population-by-state.csv\")\n\npopulation_data_2\n\n# A tibble: 52 × 9\n    rank State       Pop  Growth Pop2018 Pop2010 growthSince2010 Percent density\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Califor… 3.96e7  0.0038  3.95e7  3.73e7          0.0615  0.118     254.\n 2     2 Texas    2.97e7  0.0385  2.86e7  2.52e7          0.178   0.0889    114.\n 3     3 Florida  2.19e7  0.033   2.12e7  1.88e7          0.164   0.0656    409.\n 4     4 New York 1.93e7 -0.0118  1.95e7  1.94e7         -0.0051  0.0577    410.\n 5     5 Pennsyl… 1.28e7  0.0003  1.28e7  1.27e7          0.0073  0.0383    286.\n 6     6 Illinois 1.26e7 -0.0121  1.27e7  1.28e7         -0.0211  0.0376    226.\n 7     7 Ohio     1.17e7  0.0033  1.17e7  1.15e7          0.0152  0.035     287.\n 8     8 Georgia  1.08e7  0.0303  1.05e7  9.71e6          0.115   0.0324    188.\n 9     9 North C… 1.07e7  0.0308  1.04e7  9.57e6          0.118   0.032     220.\n10    10 Michigan 9.99e6  0.0008  9.98e6  9.88e6          0.0116  0.0299    177.\n# ℹ 42 more rows\n\n\nThis data looks slightly different from the data you generated using the read.csv() function. For example, R shows only the first 10 rows. This variation occurs because read_csv() imports the data not as a data frame but as a data type called a tibble. Both data frames and tibbles are used to describe rectangular data like what you would see in a spreadsheet. There are some minor differences between data frames and tibbles, the most important of which is that tibbles print only the first 10 rows by default, while data frames print all rows. For the purposes of this book, the two terms are used interchangeably.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#rstudio-projects",
    "href": "crash-course.html#rstudio-projects",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nSo far, you’ve imported a CSV file from your Documents folder. But because others won’t have this exact location on their computer, your code won’t work if they try to run it. One solution to this problem is an RStudio project.\nBy working in a project, you can use relative paths to your files instead of having to write the entire filepath when calling a function to import data. Then, if you place the CSV file in your project, anyone can open it by using the file’s name, as in read_csv(file = \"population-by-state.csv\"). This makes the path easier to write and enables others to use your code.\nTo create a new RStudio project, go to File New Project. Select either New Directory or Existing Directory and choose where to put your project. If you choose New Directory, you’ll need to specify that you want to create a new project. Next, choose a name for the new directory and where it should live. (Leave the checkboxes that ask about creating a Git repository and using renv unchecked; they’re for more advanced purposes.)\nOnce you’ve created this project, you should see two major differences in RStudio’s appearance. First, the files pane no longer shows every file on your computer. Instead, it shows only files in the example-project directory. Right now, that’s just the example-project.Rproj file, which indicates that the folder contains a project. Second, at the top right of RStudio, you can see the name example-project. This label previously read Project: (None). If you want to make sure you’re working in a project, check for its name here. Figure 1.6 shows these changes.\n\n\n\n\n\nFigure 1.6: RStudio with an active project\n\n\nNow that you’ve created a project, copy the population-by-state.csv file into the example-project directory. Once you’ve done so, you should see it in the RStudio files pane.\nWith this CSV file in your project, you can now import it more easily. As before, start by loading the tidyverse package. Then, remove the reference to the Documents folder and import your data by simply using the name of the file:\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"population-by-state.csv\")\n\nThe reason you can import the population-by-state.csv file this way is that the RStudio project sets the working directory to be the root of your project. With the working directory set like this, all references to files are relative to the .Rproj file at the root of the project. Now anyone can run this code because it imports the data from a location that is guaranteed to exist on their computer.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#data-analysis-with-the-tidyverse",
    "href": "crash-course.html#data-analysis-with-the-tidyverse",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Data Analysis with the tidyverse",
    "text": "Data Analysis with the tidyverse\nNow that you’ve imported the population data, you’re ready to do a bit of analysis on it. Although I’ve been referring to the tidyverse as a single package, it’s actually a collection of packages. We’ll explore several of its functions throughout this book, but this section introduces you to its basic workflow.\ntidyverse Functions\nBecause you’ve loaded the tidyverse package, you can now access its functions. For example, the package’s summarize() function takes a data frame or tibble and calculates some piece of information for one or more of the variables in that dataset. The following code uses summarize() to calculate the mean population of all states:\n\nsummarize(.data = population_data_2, mean_population = mean(Pop))\n\nFirst, the code passes population_data_2 to the summarize() function’s .data argument to tell R to use that data frame to perform the calculation. Next, it creates a new variable called mean_population and assigns it to the output of the mean() function introduced earlier. The mean() function runs on Pop, one of the variables in the population_data_2 data frame.\nYou might be wondering why you don’t need to use the c() function within mean(), as shown earlier in this chapter. The reason is that you’re passing the function only one argument here: Pop, which contains the set of population data for which you’re calculating the mean. In this case, there’s no need to use c() to combine multiple values into one.\nRunning this code should return a tibble with a single variable (mean_population), as shown here:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        6433422.\n\n\nThe variable is of type double (dbl), which is used to hold general numeric data. Other common data types are integer (for whole numbers, such as 4, 82, and 915), character (for text values), and logical (for the TRUE/FALSE values returned from comparison operations). The mean_population variable has a value of 6433422, the mean population of all states.\nNotice also that the summarize() function creates a totally new tibble from the original population_data_2 data frame. This is why the variables from population_data_2 are no longer present in the output. This is a basic example of data analysis, but you can do a lot more with the tidyverse.\nThe tidyverse Pipe\nOne advantage of working with the tidyverse is that it uses the pipe for multi-step operations. The tidyverse pipe, which is written as %&gt;%, allows you to break steps into multiple lines. For example, you could rewrite your code using the pipe like so:\n\npopulation_data_2 %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis code says, “Start with the population_data_2 data frame, then run the summarize() function on it, creating a variable called mean_population by calculating the mean of the Pop variable.”\nNotice that the line following the pipe is indented. To make the code easier to read, RStudio automatically adds two spaces to the start of lines that follow pipes.\nThe pipe becomes even more useful when you use multiple steps in your data analysis. Say, for example, you want to calculate the mean population of the five largest states. The following code adds a line that uses the filter() function, also from the tidyverse package, to include only states where the rank variable is less than or equal to (&lt;=) 5. Then, it uses summarize() to calculate the mean of those states:\n\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nRunning this code returns the mean population of the five largest states:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        24678497\n\n\nUsing the pipe to combine functions lets you refine your data in multiple ways while keeping it readable and easy to understand. Indentation can also make your code more readable. You’ve seen only a few functions for analysis at this point, but the tidyverse has many more functions that enable you to do nearly anything you could hope to do with your data. Because of how useful the tidyverse is, it will appear in every single piece of R code you write in this book.\n\n\n\n\n\n\nR for Data Science, 2nd edition, by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund is the bible of tidyverse programming and worth reading for more details on how the package’s many functions work.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#comments",
    "href": "crash-course.html#comments",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Comments",
    "text": "Comments\nIn addition to code, R script files often contain comments — lines that begin with hash marks (#) and aren’t treated as runnable code but instead as notes for anyone reading the script. For example, you could add a comment to the code from the previous section, like so:\n\n# Calculate the mean population of the five largest states\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis comment will help others understand what is happening in the code, and it can also serve as a useful reminder for you if you haven’t worked on the code in a while. R knows to ignore any lines that begin with the hash mark instead of trying to run them.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#how-to-get-help",
    "href": "crash-course.html#how-to-get-help",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "How to Get Help",
    "text": "How to Get Help\nNow that you’ve learned the basics of how R works, you’re probably ready to dive in and write some code. When you do, though, you’re going to encounter errors. Being able to get help when you run into issues is a key part of learning to use R successfully. There are two main strategies you can use to get unstuck.\nThe first is to read the documentation for the functions you use. Remember, to access the documentation for any function, simply enter ? and then the name of the function in the console. In the bottom-right pane in Figure 1.7, for example, you can see the result of running ?read.csv.\n\n\n\n\n\nFigure 1.7: The documentation for the read.csv() function\n\n\nHelp files can be a bit hard to decipher, but essentially they describe what package the function comes from, what the function does, what argu- ments it accepts, and some examples of how to use it.\n\n\n\n\n\n\nFor additional guidance on reading documentation, I recommend the appendix of Kieran Healy’s book Data Visualization: A Practical Introduction. A free online version is available at https://socviz.co/appendix.html.\n\n\n\nThe second approach is to read the documentation websites associated with many R packages. These can be easier to read than RStudio’s help files. In addition, they often contain longer articles, known as vignettes, that provide an overview of how a given package works. Reading these can help you understand how to combine individual functions in the context of a larger project. Every package discussed in this book has a good documentation website.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#summary",
    "href": "crash-course.html#summary",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned the basics of R programming. You saw how to download and set up R and RStudio, what the various RStudio panes are for, and how R script files work. You also learned how to import CSV files and explore them in R, how to save data as objects, and how to install packages to access additional functions. Then, to make the files used in your code more accessible, you created an RStudio project. Finally, you experi- mented with tidyverse functions and the tidyverse pipe, and you learned how to get help when those functions don’t work as expected.\nNow that you understand the basics, you’re ready to start using R to work with your data. See you in Chapter 2!",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#additional-resources",
    "href": "crash-course.html#additional-resources",
    "title": "\n1  An R Programming Crash Course\n",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nKieran Healy, Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nChester Ismay and Albert Y. Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Boca Raton, FL: CRC Press, 2020), https://moderndive.com.\nDavid Keyes, “Getting Started with R,” online course, accessed November 10, 2023, https://rfortherestofus.com/courses/getting-started.\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science, 2nd ed. (Sebastopol, CA: O’Reilly Media, 2023). https://r4ds.hadley.nz/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "2  Principles of Data Visualization",
    "section": "",
    "text": "The Drought Visualization\nOther news organizations had relied on the same National Drought Center data in their stories, but Scherer and Karamanis visualized it so that it both grabs attention and communicates the scale of the phenomenon. Figure 2.1 shows a section of the final visualization (due to space constraints, I could include only four regions). The graph makes apparent the increase in drought conditions over the last two decades, especially in California and the Southwest.\nTo understand why this visualization is effective, let’s break it down.\nAt the broadest level, the data visualization is notable for its minimalist aesthetic. For example, there are no grid lines and few text labels, as well as minimal text along the axes. Scherer and Karamanis removed what statistician Edward Tufte, in his 1983 book The Visual Display of Quantitative Information (Graphics Press), calls chartjunk. Tufte wrote that extraneous elements often hinder, rather than help, our understanding of charts (and researchers and data visualization designers have generally agreed).\nNeed proof that Scherer and Karamanis’s decluttered graph is better than the alternative? Figure 2.2 shows a version with a few tweaks to the code to include grid lines and text labels on axes.\nFigure 2.1: A section of the final drought visualization, with a few tweaks made to fit this book\nFigure 2.2: The cluttered version of the drought visualization\nIt’s not just that this cluttered version looks worse; the clutter actively inhibits understanding. Rather than focusing on overall drought patterns (the point of the graph), our brains get stuck reading repetitive and unnecessary axis text.\nOne of the best ways to reduce clutter is to break a single chart into a set of component charts, as Scherer and Karamanis have done (this approach, known as faceting, will be discussed further in Section 2.4.3). Each rectangle represents one region in one year. Filtering the larger chart to show the Southwest region in 2003 produces the graph shown in Figure 2.3, where the x-axis indicates the week and the y-axis indicates the percentage of that region at different drought levels.\nFigure 2.3: A drought visualization for the Southwest in 2003\nZooming in on a single region in a single year also makes the color choices more obvious. The lightest orange bars show the percentage of the region that is abnormally dry, and the darkest purple bars show the percentage experiencing exceptional drought conditions. As you’ll see shortly, this range of colors was intentionally chosen to make differences in the drought levels visible to all readers.\nDespite the graph’s complexity, the R code that Scherer and Karamanis wrote to produce it is relatively simple, due largely to a theory called the grammar of graphics.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#the-drought-visualization",
    "href": "data-viz.html#the-drought-visualization",
    "title": "2  Principles of Data Visualization",
    "section": "The Drought Visualization",
    "text": "The Drought Visualization\nOther news organizations had relied on the same data as Scherer and Karamanis, from the National Drought Center, in their stories. But Scherer and Karamanis visualized it in a way that it both grabs attention and communicates the scale of the phenomenon. Figure @ref(fig:final-viz) shows a section of the final visualization. Covering four regions over the last two decades, the graph makes apparent increase in drought conditions, especially in California and the Southwest.\n\n\n\n\nA section of the final drought visualization, with a few tweaks made so that the plots fit in this book\n\n\n\nTo understand why this visualization is effective, let’s break it down into pieces. At the broadest level, the data visualization is notable for its minimalist aesthetic. There are, for example, no grid lines and few text labels, as well as little text along the axes. Scherer and Karamanis removed what statistician Edward Tufte, in his 1983 book The Visual Display of Quantitative Information, calls chartjunk. Tufte wrote that extraneous elements often hinder, rather than help, our understanding of charts (and researchers, as well as data visualization designers since, have generally agreed).\nNeed proof that Scherer and Karamanis’s decluttered graph is better than the alternative? Figure @ref(fig:cluttered-viz) shows a version with a few small tweaks to the code to include grid lines and text labels on axes. Prepare yourself for clutter!\n\n\n\n\nThe cluttered version of the drought visualization\n\n\n\nAgain, it’s not just that this cluttered version looks worse. The clutter actively inhibits understanding. Rather than focus on overall drought patterns (the point of the graph), our brain gets stuck reading repetitive and unnecessary axis text.\nOne of the best ways to reduce clutter is to break a single chart into what are known as small multiples. When we look closely at the data visualization, we see that it is not one chart but actually a set of charts. Each rectangle represents one region in one year. If we filter it to show the Southwest region in 2003 and add axis titles, we can see in Figure @ref(fig:viz-sw-2003) that the x axis shows the week while the y axis shows the percentage of that region at different drought levels.\n\n\n\n\nA drought visualization for the Southwest in 2003\n\n\n\nZooming in on a single region in a single year also makes the color choices more obvious. The lightest bars show the percentage of the region that is abnormally dry while the darkest bars show the percentage in exceptional drought conditions. These colors, as we’ll see shortly, were intentionally chosen to make differences in the drought levels visible to all readers. Even so, the R code that Scherer and Karamanis wrote to produce this complex graph is relatively simple, due largely to a theory called the grammar of graphics.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#the-grammar-of-graphics",
    "href": "data-viz.html#the-grammar-of-graphics",
    "title": "2  Principles of Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nWhen working in Excel, you begin by selecting the type of graph you want to make. Need a bar chart? Click the bar chart icon. Need a line chart? Click the line chart icon. If you’ve only ever made charts in Excel, this first step may seem so obvious that you’ve never even given the data visualization process much thought, but in fact there are many ways to think about graphs. For example, rather than thinking of graph types as distinct, we can recognize and use their commonalities as the starting point for making them.\nThis approach to thinking about graphs comes from the late statistician Leland Wilkinson. For years, Wilkinson thought deeply about what data visualization is and how we can describe it. In 1999 he published a book called The Grammar of Graphics (Springer) that sought to develop a consistent way of describing all graphs. In it, Wilkinson argued that we should think of plots not as distinct types, à la Excel, but as following a grammar that we can use to describe any plot. Just as English grammar tells us that a noun is typically followed by a verb (which is why “he goes” works, while the opposite, “goes he,” does not), the grammar of graphics helps us understand why certain graph types “work.”\nThinking about data visualization through the lens of the grammar of graphics helps highlight, for example, that graphs typically have some data that is plotted on the x-axis and other data that is plotted on the y-axis. This is the case whether the graph is a bar chart or a line chart, as Figure 2.4 shows.\n\n\n\n\n\n\n\nFigure 2.4: A bar chart and a line chart showing identical data\n\n\n\n\nWhile the graphs look different (and would, to the Excel user, be different types of graphs), Wilkinson’s grammar of graphics emphasizes their similarities. (Incidentally, Wilkinson’s feelings on graph-making tools like Excel became clear when he wrote that “most charting packages channel user requests into a rigid array of chart types.”)\nWhen Wilkinson wrote his book, no data visualization tool could imple- ment his grammar of graphics. This would change in 2010, when Hadley Wickham announced the ggplot2 package for R in the article “A Layered Grammar of Graphics,” published in the Journal of Computational and Graphical Statistics. By providing the tools to implement Wilkinson’s ideas, ggplot2 would come to revolutionize the world of data visualization.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#working-with-ggplot2",
    "href": "data-viz.html#working-with-ggplot2",
    "title": "2  Principles of Data Visualization",
    "section": "Working With ggplot2",
    "text": "Working With ggplot2\nThe ggplot2 R package (which I, like nearly everyone in the data visualization world, will refer to simply as ggplot) relies on the idea of plots having multiple layers. Let’s walk through some of the most important ones. We’ll begin by selecting variables to map to aesthetic properties. Then we’ll choose a geometric object to use to represent our data. Next, we’ll change the aesthetic properties of our chart (its color scheme, for example) using a scale_ function. Finally, we’ll use a theme_ function to set the overall look-and-feel of our plot.\nThe First Layer: Mapping Data to Aesthetic Properties\nWhen creating a graph with ggplot, we begin by mapping data to aesthetic properties. All this really means is that we use things like the x or y axis, color, and size (the so-called aesthetic properties) to represent variables. To make this concrete, we’ll use the data on life expectancy in Afghanistan, introduced in the previous section, to generate a plot. Access this data with the following code:\n\nlibrary(tidyverse)\n\ngapminder_10_rows &lt;- read_csv(\"https://data.rwithoutstatistics.com/gapminder_10_rows.csv\")\n\nHere’s what the gapminder_10_rows data frame looks like:\n\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n\n\nThis is a shortened version of the full gapminder data frame, which includes over 1,700 rows of data.\nIf we want to make a chart with ggplot, we need to first decide which variable to put on the x axis and which to put on the y axis. For data showing change over time, it is common to put the date on the x axis and the value of what you are showing on the y axis. That means we would use the variable year on the x axis and the variable lifeExp on the y axis. To do so, we begin by using the ggplot() function:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n)\n\nWithin this function, we tell R that we’re using the data frame gapminder_10_rows. We also map year to the x axis and lifeExp to the y axis.\nWhen we run the code, what we get in Figure 2.5 doesn’t look like much.\n\n\n\n\n\n\n\nFigure 2.5: A blank chart\n\n\n\n\nIf you look closely, however, you should see that the x axis corresponds to year and the y axis corresponds to lifeExp. Also, the values on the x and y axes match the scope of our data. In the gapminder_10_rows data frame, the first year is 1952 and the last year is 1997. The range of the x axis seems to have been created with this data in mind (because it was). Likewise, lifeExp, which goes from about 28 to about 42, will fit nicely on our y axis.\nThe Second Layer: Choosing the geoms\nAxes are nice, but we’re missing any type of visual representation of the data. To get this, we need to add the next ggplot layer: geoms. Short for geometric objects, geoms are functions that provide different ways of representing data. For example, if we want to add points to the graph, we use geom_point():\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point()\n\nNow, in Figure 2.6, we see that people in 1952 had a life expectancy of about 28 and that this value rose every year included in the data.\n\n\n\n\n\n\n\nFigure 2.6: The same chart but with points added\n\n\n\n\nLet’s say we change our mind and want to make a line chart instead. All we have to do is replace geom_point() with geom_line():\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_line()\n\nFigure 2.7 shows the result.\n\n\n\n\n\n\n\nFigure 2.7: The data as a line chart\n\n\n\n\nTo really get fancy, what if we add both geom_point() and geom_line()?\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point() +\n  geom_line()\n\nThis code generates a line chart with points, as shown in Figure 2.8.\n\n\n\n\n\n\n\nFigure 2.8: The data with points and a line\n\n\n\n\nWe can swap in geom_col() to create a bar chart:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_col()\n\nNote in Figure 2.9 that the y axis range has been automatically updated, going from 0 to 40 to account for the different geom.\n\n\n\n\n\n\n\nFigure 2.9: The data as a bar chart\n\n\n\n\nAs you can see, the difference between a line chart and a bar chart isn’t as great as the Excel chart-type picker might have us think. Both can have the same underlying properties (namely, putting years on the x axis and life expectancies on the y axis). They simply use different geometric objects to visually represent the data.\nThe Third Layer: Altering Aesthetic Properties\nBefore we return to the drought data visualization, let’s look at a few additional layers that can help us can alter the bar chart. Say we want to change the color of the bars. In the grammar of graphics approach to chart-making, this means mapping some variable to the aesthetic property of fill. (Slightly confusingly, the aesthetic property of color would, for a bar chart, change only the outline of each bar). In the same way that we mapped year to the x axis and lifeExp to the y axis, we can map fill to a variable, such as year:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col()\n\nFigure 2.10 shows the result. We see now that, for earlier years, the fill is darker, while for later years, it is lighter (the legend, added to the right of our plot, also indicates this).\n\n\n\n\n\n\n\nFigure 2.10: The same chart, now with added colors\n\n\n\n\nWhat if we wanted to change the fill colors? For that, we use a new scale layer. To do this, I’ll use the scale_fill_viridis_c() function. The c at the end of the function name refers to the fact that the data is continuous, meaning it can take any numeric value:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c()\n\nThis function changes the default palette to one that is colorblind-friendly and prints well in grayscale. The scale_fill_viridis_c() function is just one of many that start with scale_ and can alter the fill scale.\nThe Fourth Layer: Setting a Theme\nA final layer we’ll look at is the theme layer. This layer allows us to change the overall look-and-feel of plots (including the plot background, grid lines, and so on). Just as there are a number of scale_ functions, there are also a number of functions that start with theme_. Here, we’ve added theme_minimal():\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\nNotice in Figure 2.11 that this theme starts to declutter the plot.\n\n\n\n\n\n\n\nFigure 2.11: The same chart with theme_minimal() added\n\n\n\n\nBy now, you should see why Hadley Wickham described the ggplot2 package as using a layered grammar of graphics. It implements Wilkinson’s theory through the creation of multiple layers. First, we select variables to map to aesthetic properties, such as x or y axes, color, and fill. Second, we choose the geometric object (or geom) we want to use to represent our data. Third, if we want to change aesthetic properties (for example, to use a different color palette), we do this with a scale_ function. Fourth, we use a theme_ function to set the overall look-and-feel of the plot.\nWe could improve the plot we’ve been working on in many ways, but rather than add to an ugly plot, let’s instead return to the drought data visualization by Cédric Scherer and Georgios Karamanis. By walking through their code, you’ll learn lessons about making high-quality data visualization with ggplot and R.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "href": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "title": "2  Principles of Data Visualization",
    "section": "Recreating the Drought Visualization with ggplot",
    "text": "Recreating the Drought Visualization with ggplot\nfundamentals and some less-well-known tweaks that make it really shine. To understand how Scherer and Karamanis made their data visualization, we’ll start with a simplified version of their code, then build it up layer by layer, adding elements as we go.\nFirst, let’s import the data. Scherer and Karamanis do this with the import() function from the rio package:\n\nlibrary(rio)\n\ndm_perc_cat_hubs_raw &lt;- import(\"https://data.rwithoutstatistics.com/dm_export_20000101_20210909_perc_cat_hubs.json\")\n\nThis function is helpful because the data they are working with is in JSON format, which can be complicated to work with. The rio package simplifies it into just one line.\nPlotting One Region and Year\nLet’s start by looking at just one region (the Southwest) in one year (2003). First, we filter our data and save it as a new object called southwest_2003:\n\nsouthwest_2003 &lt;- dm_perc_cat_hubs %&gt;%\n  filter(hub == \"Southwest\") %&gt;%\n  filter(year == 2003)\n\nWe can take a look at this object to see the variables we have to work with by typing southwest_2003 in the console, which will return this:\n\n\n# A tibble: 255 × 7\n   date       hub       category percentage  year  week max_week\n   &lt;date&gt;     &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 2003-12-30 Southwest D0           0.0718  2003    52       52\n 2 2003-12-30 Southwest D1           0.0828  2003    52       52\n 3 2003-12-30 Southwest D2           0.269   2003    52       52\n 4 2003-12-30 Southwest D3           0.311   2003    52       52\n 5 2003-12-30 Southwest D4           0.0796  2003    52       52\n 6 2003-12-23 Southwest D0           0.0823  2003    51       52\n 7 2003-12-23 Southwest D1           0.131   2003    51       52\n 8 2003-12-23 Southwest D2           0.189   2003    51       52\n 9 2003-12-23 Southwest D3           0.382   2003    51       52\n10 2003-12-23 Southwest D4           0.0828  2003    51       52\n# ℹ 245 more rows\n\n\nThe date variable represents the start date of the week in which the observation took place. The hub variable is the region, and category is the level of drought: a value of D0 indicates the lowest level of drought, while D5 indicates the highest level. The percentage variable is the percentage of that region in that drought category, ranging from 0 to 1. The year and week variables are the observation year and week number (beginning with week 1). The max_week variable is the maximum number of weeks in a given year.\nNow we can use this southwest_2003 object for our plotting:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col()\n\nIn the ggplot() function, we tell R to put week on the x axis and percentage on the y axis. We also use the category variable for the fill color. We then use geom_col() to create a bar chart in which the fill color of each bar represents the percentage of the region in a single week at each drought level. You can see the result in in Figure 2.12.\n\n\n\n\n\n\n\nFigure 2.12: One year and region of the drought visualization\n\n\n\n\nThe colors don’t match the final version of the plot, but we can start to see the outlines of Scherer and Karamanis’s data visualization.\nChanging Aesthetic Properties\nScherer and Karamanis next selected different fill colors for their bars. To do so, they used the scale_fill_viridis_d() function. The d here means that the data to which the fill scale is being applied has discrete categories, called D0, D1, D2, D3, D4, and D5:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  )\n\nThey used the argument option = \"rocket\" to select the rocket palette (the function has several other palettes). Then they used the direction = -1 argument to reverse the order of fill colors so that darker colors mean higher drought conditions.\nScherer and Karamanis also tweaked the appearance of the x and y axes:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  )\n\nOn the x axis, they removed both the axis title (“week”) using name = NULL and the 0–50 text with guide = \"none\". On the y axis, they removed the title and text showing percentages using labels = NULL, which functionally does the same thing as guide = \"none\". They also moved the axis lines themselves to the right side using position = \"right\". These axis lines are apparent only as tick marks at this point but will become more visible later. Figure 2.13 shows the result of these tweaks.\n\n\n\n\n\n\n\nFigure 2.13: One year and region of the drought visualization with adjustments to the x and y axes\n\n\n\n\nUp to this point, we’ve focused on one of the single plots that make up the larger data visualization. But the final product that Scherer and Karamanis made is actually 176 plots visualizing 22 years and eight regions. Let’s discuss the ggplot feature they used to create all of these plots.\nFaceting the Plot\nOne of the most useful features of ggplot is what’s known as faceting (or, more commonly in the data visualization world, small multiples). Faceting takes a single plot and makes it into multiple plots using a variable. For example, think of a line chart showing life expectancy by country over time; instead of multiple lines on one plot, we might create multiple plots with one line per plot). With the facet_grid() function, we can select which variable to put in the rows and which to put in the columns of our faceted plot:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  )\n\nScherer and Karamanis put year in rows and hub (region) in columns. The switch = \"y\" argument moves the year label from the right side (where it appears by default) to the left. With this code in place, we can see the final plot coming together in Figure 2.14. Space considerations require me to include only four regions, but you get the idea.\n\n\n\n\n\n\n\nFigure 2.14: The faceted version of the drought visualization. Space considerations require me to include only four regions, but you get the idea.\n\n\n\n\nIncredibly, the broad outlines of the plot took us just 10 lines to create. The rest of the code falls into the category of small polishes. That’s not to minimize how important small polishes are (very) or the time it takes to create them (lots). It does show, however, that a little bit of ggplot goes a long way.\nApplying Small Polishes\nLet’s look at a few of the small polishes that Scherer and Karamanis made. The first is to apply a theme, as shown in Figure 2.15. They used theme_light(), which removes the default gray background and changes the font to Roboto.\nThe theme_light() function is what’s known as a complete theme. So-called complete themes change the overall look-and-feel of a plot. But Scherer and Karamanis didn’t stop there. They then used the theme() function to make additional tweaks to what theme_light() gave them:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4\n  ) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  theme_light(base_family = \"Roboto\") +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      face = \"bold\"\n    ),\n    legend.text = element_text(color = \"#2DAADA\"),\n    strip.text.x = element_text(\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThe code in the theme() function does many different things, but let’s take a look at a few of the most important. First, it moves the legend from the right side (the default) to the top of the plot. Then, an angle = 0 argument rotates the year text in the columns so that it is no longer angled. Without this argument, the years would be much less readable.\nNext, the theme() function makes the distinctive axis lines and ticks that show up on the right side of the final plot. Calling element_blank() removes all grid lines. Finally, three lines remove the borders and make each of the individual plots have a transparent background.\nKeen readers such as yourself may now be thinking, “Wait. Didn’t the individual plots have a gray background behind them?” Yes, dear reader, they did. Scherer and Karamanis made these with a separate geom, geom_rect():\n\ngeom_rect(\n  aes(\n    xmin = .5,\n    xmax = max_week + .5,\n    ymin = -0.005,\n    ymax = 1\n  ),\n  fill = \"#f4f4f9\",\n  color = NA,\n  size = 0.4\n)\n\nThey set some additional aesthetic properties specific to this geom: xmin, xmax, ymin, and ymax, which determine the boundaries of the rectangle it produces. The result is a gray background drawn behind each small multiple, as shown in Figure 2.15.\n\n\n\n\n\n\n\nFigure 2.15: Faceted version of the drought visualization with gray backgrounds behind each small multiple\n\n\n\n\nFinally, consider the tweaks made to the legend. We previously saw a simplified version of the scale_fill_viridis_d() function. Here is a more complete version:\n\nscale_fill_viridis_d(\n  option = \"rocket\",\n  direction = -1,\n  name = \"Category:\",\n  labels = c(\n    \"Abnormally Dry\",\n    \"Moderate Drought\",\n    \"Severe Drought\",\n    \"Extreme Drought\",\n    \"Exceptional Drought\"\n  )\n)\n\nThe name argument sets the legend title, and the labels argument determines the labels that show up in the legend. Figure 2.16 shows the result of these changes.\n\n\n\n\n\n\n\nFigure 2.16: Drought visualization with changes made to the legend text\n\n\n\n\nRather than D0, D1, D2, D3, and D4, we now have the legend text Abnormally Dry, Moderate Drought, Severe Drought, Extreme Drought, and Exceptional Drought.\nThe Complete Visualization Code\nWhile I’ve showed you a nearly complete version of the code that Scherer and Karamanis wrote, I made some small changes to make it easier to understand. If you’re curious, the full code is here:\n\nggplot(dm_perc_cat_hubs, aes(week, percentage)) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4,\n    show.legend = FALSE\n  ) +\n  geom_col(\n    aes(\n      fill = category,\n      fill = after_scale(addmix(\n        darken(\n          fill,\n          .05,\n          space = \"HLS\"\n        ),\n        \"#d8005a\",\n        .15\n      )),\n      color = after_scale(darken(\n        fill,\n        .2,\n        space = \"HLS\"\n      ))\n    ),\n    width = .9,\n    size = 0.12\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_x_continuous(\n    expand = c(.02, .02),\n    guide = \"none\",\n    name = NULL\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0),\n    position = \"right\",\n    labels = NULL,\n    name = NULL\n  ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    name = \"Category:\",\n    direction = -1,\n    begin = .17,\n    end = .97,\n    labels = c(\n      \"Abnormally Dry\",\n      \"Moderate Drought\",\n      \"Severe Drought\",\n      \"Extreme Drought\",\n      \"Exceptional Drought\"\n    )\n  ) +\n  guides(fill = guide_legend(\n    nrow = 2,\n    override.aes = list(size = 1)\n  )) +\n  theme_light(\n    base_size = 18,\n    base_family = \"Roboto\"\n  ) +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      size = 18,\n      face = \"bold\"\n    ),\n    legend.text = element_text(\n      color = \"#2DAADA\",\n      size = 16\n    ),\n    strip.text.x = element_text(\n      size = 16,\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      size = 18,\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThere are a few additional tweaks to colors and spacing, but most of the code reflects what you’ve seen so far.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "href": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "title": "2  Principles of Data Visualization",
    "section": "In Conclusion: ggplot is Your Data Visualization Secret Weapon",
    "text": "In Conclusion: ggplot is Your Data Visualization Secret Weapon\nYou may start to think of ggplot as a solution to all of your data visualization problems. And yes, you have a new hammer, but no, everything is not a nail. If you look at the version of the data visualization that appeared in Scientific American in November 2021, you’ll see that some of its annotations aren’t visible in our recreation. That’s because they were added in post-production. While you could have found ways to create them in ggplot, it’s often not the best use of your time. Get yourself 90 percent of the way there with ggplot and then use Illustrator, Figma, or a similar tool to finish your work.\nEven so, ggplot is a very powerful hammer, used to make plots that you’ve seen in The New York Times, FiveThirtyEight, the BBC, and other well-known news outlets. Although not the only tool that can generate high-quality data visualization, it makes the process straightforward. The graph by Scherer and Karamanis shows this in several ways:\n\nIt strips away extraneous elements, such as grid lines, to keep the focus on the data itself. Complete themes such as theme_light() and the theme() function allowed Scherer and Karamanis to create a decluttered visualization that communicates effectively.\nIt uses well-chosen colors. The scale_fill_viridis_d() function allowed them to create a color scheme that demonstrates differences between groups, is colorblind friendly, and shows up well when printed in grayscale.\nIt uses small multiples to break data from two decades and eight regions into a set of graphs that come together to create a single plot. With a single call to the facet_grid() function, Scherer and Karamanis created over 100 small multiples that the tool automatically combined into a single plot.\n\nLearning to create data visualization in ggplot involves a significant time investment. But the long-term payoff is even greater. Once you learn how ggplot works, you can look at others’ code and learn how to improve your own. By contrast, when you make a data visualization in Excel, the series of point-and-click steps disappears into the ether. To recreate a visualization you made last week, you’ll need to remember the exact steps you used, and to make someone else’s data visualization, you’ll need them to write up their process for you.\nBecause code-based data visualization tools allow you to keep that record of the steps you made, you don’t have to be the most talented designer to make high-quality data visualization with ggplot. You can study others’ code, adapt it to your own needs, and create your own data visualization that is beautiful and communicates effectively.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#learn-more",
    "href": "data-viz.html#learn-more",
    "title": "2  Principles of Data Visualization",
    "section": "Learn More",
    "text": "Learn More\nConsult the following resources to learn more about data visualization principles and the ggplot2 package:\nData Visualization: A Practical Introduction by Kieran Healy (Princeton University Press, 2018), https://socviz.co\nFundamentals of Data Visualization by Claus Wilke (O’Reilly Media, 2019). https://clauswilke.com/dataviz/\nggplot2: Elegant Graphics for Data Analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen (Springer, Forthcoming), https://ggplot2-book.org\nGraphic Design with ggplot2 by Cédric Scherer (CRC Press, Forthcoming)\n“The Glamour of Graphics,” course by Will Chase, https://rfortherestofus.com/courses/glamour/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  }
]