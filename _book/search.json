[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "",
    "text": "About the Book\nThe R programming language is a remarkably powerful tool for data analysis and visualization, but its steep learning curve can be intimidating for some. If you just want to automate repetitive tasks or visualize your data, without the need for complex math, R for the Rest of Us is for you.\nInside you’ll find a crash course in R, a quick tour of the RStudio programming environment, and a collection of real-word applications that you can put to use right away. You’ll learn how to create informative visualizations, streamline report generation, and develop interactive websites—whether you’re a seasoned R user or have never written a line of R code.\nYou’ll also learn how to:\n\nManipulate, clean, and parse your data with tidyverse packages like dplyr and tidyr to make data science operations more user-friendly\nCreate stunning and customized plots, graphs, and charts with ggplot2 to effectively communicate your data insights\nImport geospatial data and write code to produce visually appealing maps automatically\nGenerate dynamic reports, presentations, and interactive websites with R Markdown and Quarto that seamlessly integrate code, text, and graphics\nDevelop custom functions and packages tailored to your specific needs, allowing you to extend R’s functionality and automate complex tasks\n\nUnlock a treasure trove of techniques to transform the way you work. With R for the Rest of Us, you’ll discover the power of R to get stuff done. No advanced statistics degree required.\n\n\nAbout the Author\n\nDavid Keyes is the founder and CEO of R for the Rest of Us, which offers online courses, workshops, and custom training sessions that help organizations take control of their data. He has a PhD in anthropology from UC San Diego, as well as a master’s degree in education from Ohio State, and has dedicated his professional life to teaching people to embrace R as the most powerful tool for data analysis and visualization.\n\n\nAcknowldgements\nThis book is a testament to the many members of the R community who share their knowledge freely and encourage others generously. I call myself self-taught, but really what I am is community-taught. Throughout this book, you will read about several R users from whom I have learned so much; still, many others go unmentioned. To everyone who has worked to develop R, share your knowledge about R, and make R a welcoming place, thank you.\nI’d also like to thank the team at R for the Rest of Us. Working directly with talented R users has taught me so much about what is possible with R.\nFinally, I’d like to thank people who have provided feedback as I’ve written this book. Technical reviewer Rita Giordano has helped me make sure everything works and suggested great ideas for improvement. My editor, Frances Saux, has provided fantastic input along the way. To Bill Pollock and the entire team at No Starch: thank you for taking a flyer on me and my strange idea to write a book about nonstatistical uses of a tool created for statistics.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Isn’t R Just for Statistical Analysis?\nMany people think of R as simply a tool for hardcore statistical analysis, but it can do much more than manipulate numerical values. After all, every R user must illuminate their findings and communicate their results somehow, whether that’s via data visualizations, reports, websites, or presentations. Also, the more you use R, the more you’ll find yourself wanting to automate tasks you currently do manually.\nAs a qualitatively trained anthropologist without a quantitative background, I used to feel ashamed about using R for my visualization and communication tasks. But the fact is, R is good at these jobs. The ggplot2 package is the tool of choice for many top information designers. Users around the world have taken advantage of R’s ability to automate reporting to make their work more efficient. Rather than simply replacing other tools, R can perform tasks that you’re probably already doing, like generating reports and tables, better than your existing workflow.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-this-book-is-for",
    "href": "introduction.html#who-this-book-is-for",
    "title": "Introduction",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nNo matter your background, using R can transform your work. This book is for you if you’re either a current R user keen to explore its uses for visualization and communication or a non-R user wondering if R is right for you. I’ve written R for the Rest of Us so that it should make sense whether or not you’ve ever written a line of R code. But even if you’ve written entire R programs, the book should help you learn plenty of new techniques to up your game.\nR is a great tool for anyone who works with data. Maybe you’re a researcher looking for a new way to share your results. Perhaps you’re a journalist looking to analyze public data more efficiently. Or maybe you’re a data analyst tired of working in expensive, proprietary tools. If you have to work with data, you will get value from R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#about-this-book",
    "href": "introduction.html#about-this-book",
    "title": "Introduction",
    "section": "About This Book",
    "text": "About This Book\nEach chapter focuses on one use of the R language and includes examples of real R projects that employ the techniques covered. I’ll dive into the project code, breaking the programs down to help you understand how they work, and suggest ways of going beyond the example. The book has three parts, outlined here.\nIn Part I, you’ll learn how to use R to visualize data.\n\nChapter 1: An R Programming Crash Course Introduces the RStudio programming environment and the foundational R syntax you’ll need to understand the rest of the book.\nChapter 2: Principles of Data Visualization Breaks down a visualization created for Scientific American on drought conditions in the United States. In doing so, this chapter introduces the ggplot2 package for data visualization and addresses important principles that can help you make high-quality graphics.\nChapter 3: Custom Data Visualization Themes Describes how journalists at the BBC made a custom theme for the ggplot2 data visual- ization package. As the chapter walks you through the package they created, you’ll learn how to make your own theme.\nChapter 4: Maps and Geospatial Data Explores the process of making maps in R using simple features data. You’ll learn how to write map-making code, find geospatial data, choose appropriate projections, and apply data visualization principles to make your map appealing.\nChapter 5: Designing Effective Tables Shows you how to use the gt package to make high-quality tables in R. With guidance from R table connoisseur Tom Mock, you’ll learn the design principles to present your table data effectively.\n\nPart II focuses on using R Markdown to communicate efficiently. You’ll learn how to incorporate visualizations like the ones discussed in Part I into reports, slideshow presentations, and static websites generated entirely using R code.\n\nChapter 6: R Markdown Reports Introduces R Markdown, a tool that allows you to generate a professional report in R. This chapter covers the structure of an R Markdown document, shows you how to use inline code to automatically update your report’s text when data values change, and discusses the tool’s many export options.\nChapter 7: Parameterized Reporting Covers one of the advantages of using R Markdown: the ability to produce multiple reports at the same time using a technique called parameterized reporting. You’ll see how staff members at the Urban Institute used R to generate fiscal briefs for all 50 US states. In the process, you’ll learn how parameterized reporting works and how you can use it.\nChapter 8: Slideshow Presentations Explains how to use R Markdown to make slides with the xaringan package. You’ll learn how to make your own presentations, adjust your content to fit on a slide, and add effects to your slideshow.\nChapter 9: Websites Shows you how to create your own website with R Markdown and the distill package. By examining a website about COVID-19 rates in Westchester County, New York, you’ll see how to create pages on your site, add interactivity through R packages, and deploy your website in multiple ways.\nChapter 10: Quarto Explains how to use Quarto, the next-generation version of R Markdown. You’ll learn how to use Quarto for all of the projects you previously used R Markdown for (reports, parameterized reporting, slideshow presentations, and websites).\n\nPart III focuses on ways you can use R to automate your work and share it with others.\n\nChapter 11: Automatically Accessing Online Data Explores two R packages that let you automatically import data from the internet: googlesheets4 for working with Google Sheets and tidycensus for working with US Census Bureau data. You’ll learn how the packages work and how to use them to automate the process of accessing data.\nChapter 12: Creating Functions and Packages Shows you how to create your own functions and packages and share them with others, which is one of R’s major benefits. Bundling your custom functions into a package can enable other R users to streamline their work, as you’ll read about with the packages that a group of R developers built for researchers working at the Moffitt Cancer Center.\n\nBy the end of this book, you should be able to use R for a wide range of nonstatistical tasks. You’ll know how to effectively visualize data and com- municate your findings using maps and tables. You’ll be able to integrate your results into reports using R Markdown, as well as efficiently generate slideshow presentations and websites. And you’ll understand how to automate many tedious tasks using packages others have built or ones you develop yourself. Let’s dive in!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "crash-course.html",
    "href": "crash-course.html",
    "title": "1  An R Programming Crash Course",
    "section": "",
    "text": "Setting Up\nYou’ll need two pieces of software to use R effectively. The first is R itself, which provides the underlying computational tools that make the language work. The second is an integrated development environment (IDE) like RStudio. This coding platform simplifies working with R. The best way to understand the relationship between R and RStudio is with this analogy from Chester Ismay and Albert Kim’s book Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse: R is the engine that powers your data, and RStudio is like the dashboard that provides a user-friendly interface.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#setting-up",
    "href": "crash-course.html#setting-up",
    "title": "1  An R Programming Crash Course",
    "section": "",
    "text": "Installing R and RStudio\nTo download R, go to https://cloud.r-project.org and choose the link for your operating system. Once you’ve installed it, open the file. This should open an interface, like the one shown in Figure 1.1, that lets you work with R on your operating system’s command line. For example, enter 2 + 2, and you should see 4.\n\n\n\n\n\nFigure 1.1: The R console\n\n\nA few brave souls work with R using only this command line, but most opt to use RStudio, which provides a way to see your files, the output of your code, and more. You can download RStudio at https://posit.co/download/rstudio-desktop/. Install RStudio as you would any other app and open it.\nExploring the RStudio Interface\nThe first time you open RStudio, you should see the three panes shown in Figure 1.2.\n\n\n\n\n\nFigure 1.2: The RStudio editor\n\n\nThe left pane should look familiar. It’s similar to the screen you saw when working in R on the command line. This is known as the console. You’ll use it to enter code and see the results. This pane has several tabs, such as Terminal and Background Jobs, for more advanced uses. For now, you’ll stick to the default tab.\nAt the bottom right, the files pane shows all of the files on your computer. You can click any file to open it within RStudio. Finally, at the top right is the environment pane, which shows the objects that are available to you when working in RStudio. Objects are discussed in “Saving Data as Objects” on page 11.\nThere is one more pane that you’ll typically use when working in RStudio, but to see it, first you need to create an R script file.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#r-script-files",
    "href": "crash-course.html#r-script-files",
    "title": "1  An R Programming Crash Course",
    "section": "R Script Files",
    "text": "R Script Files\nIf you write all of your code in the console, you won’t have any record of it. Say you sit down today and import your data, analyze it, and then make some graphs. If you run these operations in the console, you’ll have to re-create that code from scratch tomorrow. But if you write your code in files instead, you can run it multiple times.\nR script files, which use the .R extension, save your code so you can run it later. To create an R script file, go to File &gt; New File &gt; R Script, and the script file pane should appear in the top left of RStudio, as shown in Figure 1.3. Save this file in your Documents folder as sample-code.R.\n\n\n\n\n\nFigure 1.3: The script file pane (top left)\n\n\nNow you can enter R code into the new pane to add it to your script file. For example, try entering 2 + 2 in the script file pane to perform a simple addition operation.\nTo run a script file, click Run or use the keyboard shortcut command-enter on macOS or ctrl-enter on Windows. The result (4, in this case) should show up in the console pane.\nYou now have a working programming environment. Next you’ll use it to write some simple R code.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#basic-r-syntax",
    "href": "crash-course.html#basic-r-syntax",
    "title": "1  An R Programming Crash Course",
    "section": "Basic R Syntax",
    "text": "Basic R Syntax\nIf you’re trying to learn R, you probably want to perform more complex operations than 2 + 2, but understanding the fundamentals will prepare you to do more serious data analysis tasks later in this chapter. Let’s cover some of these basics.\nArithmetic Operators\nBesides +, R supports the common arithmetic operators - for subtraction, * for multiplication, and / for division. Try entering the following in the console:\n\n2 - 1\n\n[1] 1\n\n\n\n3 * 3\n\n[1] 9\n\n\n\n16 / 4\n\n[1] 4\n\n\nAs you can see, R returns the result of each calculation you enter. You don’t have to add the spaces around operators as shown here, but doing so makes your code much more readable.\nYou can also use parentheses to perform multiple operations at once and see their result. The parentheses specify the order in which R will evaluate the expression. Try running the following:\n\n2 * (2 + 1)\n\n[1] 6\n\n\nThis code first evaluates the expression within the parentheses, 2 + 1, before multiplying the result by 2 in order to get 6.\nR also has more advanced arithmetic operators, such as ** to calculate exponents:\n\n\n[1] 8\n\n\nThis is equivalent to 23, which returns 8.\nTo get the remainder of a division operation, you can use the %% operator:\n\n10 %% 3\n\n[1] 1\n\n\nDividing 10 by 3 produces a remainder of 1, the value R returns.\nYou won’t need to use these advanced arithmetic operators for the activities in this book, but they’re good to know nonetheless.\nComparison Operators\nR also uses comparison operators, which let you test how one value compares to another. R will return either TRUE or FALSE. For example, enter 2 &gt; 1 in the console:\n\n2 &gt; 1\n\n[1] TRUE\n\n\nR should return TRUE, because 2 is greater than 1.\nOther common comparison operators include less than (&lt;), greater than or equal to (&gt;=), less than or equal to (&lt;=), equal to (==), and not equal to (!=). Here are some examples:\n\n498 == 498\n\n[1] TRUE\n\n\n\n2 != 2\n\n[1] FALSE\n\n\nWhen you enter 498 == 498 in the console, R should return TRUE because the two values are equal. If you run 2 != 2 in the console, R should return FALSE because 2 does not not equal 2.\nYou’ll rarely use comparison operators to directly test how one value compares to another; instead, you’ll use them to perform tasks like keeping only data where a value is greater than a certain threshold. You’ll see com- parison operators used in this way in “tidyverse Functions” (Section 1.6.1).\nFunctions\nYou can perform even more useful operations by making use of R’s many functions, predefined sections of code that let you efficiently do specific things. Functions have a name and a set of parentheses containing arguments, which are values that affect the function’s behavior.\nConsider the print() function, which displays information:\n\nprint(x = 1.1)\n\n[1] 1.1\n\n\nThe name of the print() function is print. Within the function’s parentheses, you specify the argument name – x, in this case — followed by the equal sign (=) and a value for the function to display. This code will print the number 1.1.\nTo separate multiple arguments, you use commas. For example, you can use the print() function’s digits argument to indicate how many digits of a number to display:\n\nprint(x = 1.1, digits = 1)\n\n[1] 1\n\n\nThis code will display only one digit (in other words, a whole number).\nUsing these two arguments allows you to do something specific (display results) while also giving you the flexibility to change the function’s behavior.\n\n\n\n\n\n\nNote\n\n\n\nFor a list of all functions built into R, see https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html.\n\n\nA common R pattern is using a function within a function. For example, if you wanted to calculate the mean, or average, of the values 10, 20, and 30, you could use the mean() function to operate on the result of the c() function like so:\n\nmean(x = c(10, 20, 30))\n\n[1] 20\n\n\nThe c() function combines multiple values into one, which is necessary because the mean() function accepts only one argument. This is why the code has two matching sets of open and close parentheses: one for mean() and a nested one for c().\nThe value after the equal sign in this example, c(10, 20, 30), tells R to use the values 10, 20, and 30 to calculate the mean. Running this code in the console returns the value 20.\nThe functions median() and mode() work with c() in the same way. To learn how to use a function and what arguments it accepts, enter ? followed by the function’s name in the console to see the function’s help file.\nNext, let’s look at how to import data for your R programs to work with.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#working-with-data",
    "href": "crash-course.html#working-with-data",
    "title": "1  An R Programming Crash Course",
    "section": "Working with Data",
    "text": "Working with Data\nR lets you do all of the same data manipulation tasks you might perform in a tool like Excel, such as calculating averages or totals. Conceptually, however, working with data in R is very different from working with Excel, where your data and analysis code live in the same place: a spreadsheet. While the data you work with in R might look similar to the data you work with in Excel, it typically comes from some external file, so you have to run code to import it.\nImporting Data\nYou’ll import data from a comma-separated values (CSV) file, a text file that holds a series of related values separated by commas. You can open CSV files using most spreadsheet applications, which use columns rather than commas as separators. For example, Figure 1.4 shows the population-by-state.csv file in Excel.\n\n\n\n\n\nFigure 1.4: The population-by-state.csv file in Excel\n\n\nTo work with this file in R, download it from https://data.rfortherestofus.com/population-by-state.csv. Save it to a location on your computer, such as your Documents folder.\nNext, to import the file into R, add a line like the following to the sample-code.R file you created earlier in this chapter, replacing my filepath with the path to the file’s location on your system:\n\nread.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nThe file argument in the read.csv() function specifies the path to the file to open.\nThe read.csv() function can accept additional optional arguments, separated by commas. For example, the following line uses the skip argument in addition to file to import the same file but skip the first row:\n\nread.csv(\n  file = \"/Users/davidkeyes/Documents/population-by-state.csv\",\n  skip = 1\n)\n\nTo learn about additional arguments for this function, enter ?read.csv() in the console to see its help file.\nAt this point, you can run the code to import your data (without the skip argument). Highlight the line you want to run in the script file pane in RStudio and click Run. You should see the following output in the console pane:\n\n\n   rank                State      Pop  Growth  Pop2018  Pop2010 growthSince2010\n1     1           California 39613493  0.0038 39461588 37319502          0.0615\n2     2                Texas 29730311  0.0385 28628666 25241971          0.1778\n3     3              Florida 21944577  0.0330 21244317 18845537          0.1644\n4     4             New York 19299981 -0.0118 19530351 19399878         -0.0051\n5     5         Pennsylvania 12804123  0.0003 12800922 12711160          0.0073\n6     6             Illinois 12569321 -0.0121 12723071 12840503         -0.0211\n7     7                 Ohio 11714618  0.0033 11676341 11539336          0.0152\n8     8              Georgia 10830007  0.0303 10511131  9711881          0.1151\n9     9       North Carolina 10701022  0.0308 10381615  9574323          0.1177\n10   10             Michigan  9992427  0.0008  9984072  9877510          0.0116\n11   11           New Jersey  8874520 -0.0013  8886025  8799446          0.0085\n12   12             Virginia  8603985  0.0121  8501286  8023699          0.0723\n13   13           Washington  7796941  0.0363  7523869  6742830          0.1563\n14   14              Arizona  7520103  0.0506  7158024  6407172          0.1737\n15   15            Tennessee  6944260  0.0255  6771631  6355311          0.0927\n16   16        Massachusetts  6912239  0.0043  6882635  6566307          0.0527\n17   17              Indiana  6805663  0.0165  6695497  6490432          0.0486\n18   18             Missouri  6169038  0.0077  6121623  5995974          0.0289\n19   19             Maryland  6065436  0.0049  6035802  5788645          0.0478\n20   20             Colorado  5893634  0.0356  5691287  5047349          0.1677\n21   21            Wisconsin  5852490  0.0078  5807406  5690475          0.0285\n22   22            Minnesota  5706398  0.0179  5606249  5310828          0.0745\n23   23       South Carolina  5277830  0.0381  5084156  4635649          0.1385\n24   24              Alabama  4934193  0.0095  4887681  4785437          0.0311\n25   25            Louisiana  4627002 -0.0070  4659690  4544532          0.0181\n26   26             Kentucky  4480713  0.0044  4461153  4348181          0.0305\n27   27               Oregon  4289439  0.0257  4181886  3837491          0.1178\n28   28             Oklahoma  3990443  0.0127  3940235  3759944          0.0613\n29   29          Connecticut  3552821 -0.0052  3571520  3579114         -0.0073\n30   30                 Utah  3310774  0.0499  3153550  2775332          0.1929\n31   31          Puerto Rico  3194374  0.0003  3193354  3721525         -0.1416\n32   32               Nevada  3185786  0.0523  3027341  2702405          0.1789\n33   33                 Iowa  3167974  0.0061  3148618  3050745          0.0384\n34   34             Arkansas  3033946  0.0080  3009733  2921964          0.0383\n35   35          Mississippi  2966407 -0.0049  2981020  2970548         -0.0014\n36   36               Kansas  2917224  0.0020  2911359  2858190          0.0207\n37   37           New Mexico  2105005  0.0059  2092741  2064552          0.0196\n38   38             Nebraska  1951996  0.0137  1925614  1829542          0.0669\n39   39                Idaho  1860123  0.0626  1750536  1570746          0.1842\n40   40        West Virginia  1767859 -0.0202  1804291  1854239         -0.0466\n41   41               Hawaii  1406430 -0.0100  1420593  1363963          0.0311\n42   42        New Hampshire  1372203  0.0138  1353465  1316762          0.0421\n43   43                Maine  1354522  0.0115  1339057  1327629          0.0203\n44   44              Montana  1085004  0.0229  1060665   990697          0.0952\n45   45         Rhode Island  1061509  0.0030  1058287  1053959          0.0072\n46   46             Delaware   990334  0.0257   965479   899593          0.1009\n47   47         South Dakota   896581  0.0204   878698   816166          0.0985\n48   48         North Dakota   770026  0.0158   758080   674715          0.1413\n49   49               Alaska   724357 -0.0147   735139   713910          0.0146\n50   50 District of Columbia   714153  0.0180   701547   605226          0.1800\n51   51              Vermont   623251 -0.0018   624358   625879         -0.0042\n52   52              Wyoming   581075  0.0060   577601   564487          0.0294\n   Percent    density\n1   0.1184   254.2929\n2   0.0889   113.8081\n3   0.0656   409.2229\n4   0.0577   409.5400\n5   0.0383   286.1704\n6   0.0376   226.3967\n7   0.0350   286.6944\n8   0.0324   188.3054\n9   0.0320   220.1041\n10  0.0299   176.7351\n11  0.0265  1206.7609\n12  0.0257   217.8776\n13  0.0233   117.3249\n14  0.0225    66.2016\n15  0.0208   168.4069\n16  0.0207   886.1845\n17  0.0203   189.9644\n18  0.0184    89.7419\n19  0.0181   624.8518\n20  0.0176    56.8653\n21  0.0175   108.0633\n22  0.0171    71.6641\n23  0.0158   175.5707\n24  0.0147    97.4271\n25  0.0138   107.0966\n26  0.0134   113.4760\n27  0.0128    44.6872\n28  0.0119    58.1740\n29  0.0106   733.7507\n30  0.0099    40.2918\n31  0.0095   923.4964\n32  0.0095    29.0195\n33  0.0095    56.7158\n34  0.0091    58.3059\n35  0.0089    63.2186\n36  0.0087    35.6808\n37  0.0063    17.3540\n38  0.0058    25.4087\n39  0.0056    22.5079\n40  0.0053    73.5443\n41  0.0042   218.9678\n42  0.0041   153.2674\n43  0.0040    43.9167\n44  0.0032     7.4547\n45  0.0032  1026.6044\n46  0.0030   508.1242\n47  0.0027    11.8265\n48  0.0023    11.1596\n49  0.0022     1.2694\n50  0.0021 11707.4262\n51  0.0019    67.6197\n52  0.0017     5.9847\n\n\nThis is R’s way of confirming that it imported the CSV file and understands the data within it. Four variables show each state’s rank (in terms of population size), name, current population, population growth between the Pop and Pop2018 variables (expressed as a percentage), and 2018 population. Several other variables are hidden in the output, but you’ll see them if you import this CSV file yourself.\nYou might think you’re ready to work with your data now, but all you’ve really done at this point is display the result of running the code that imports the data. To actually use the data, you need to save it to an object.\nSaving Data as Objects\nTo save your data for reuse, you need to create an object. For the purposes of this discussion, an object is a data structure that is stored for later use. To create an object, update your data-importing syntax so it looks like this:\nNow this line of code contains the &lt;- assignment operator, which takes what follows it and assigns it to the item on the left. To the left of the assign- ment operator is the population_data object. Put together, the whole line imports the CSV file and assigns it to an object called population_data.\nWhen you run this code, you should see population_data in your environment pane, as shown in Figure 1.5.\n\n\n\n\n\nFigure 1.5: The population_data object in the environment pane\n\n\nThis message confirms that your data import worked and that the population_data object is ready for future use. Now, instead of having to rerun the code to import the data, you can simply enter population_data in an R script file or in the console to output the data.\nData imported to an object in this way is known as a data frame. You can see that the population_data data frame has 52 observations and 9 variables. Variables are the data frame’s columns, each of which represents some value (for example, the population of each state). As you’ll see throughout the book, you can add new variables or modify existing ones using R code. The 52 observations come from the 50 states, as well as the District of Columbia and Puerto Rico.\n\npopulation_data &lt;- read.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nInstalling Packages\nThe read.csv() function you’ve been using, as well as the mean() and c() functions you saw earlier, comes from base R, the set of built-in R functions. To use base R functions, you simply enter their names. However, one of the benefits of R being an open source language is that anyone can create their own code and share it with others. R users around the world make R packages, which provide custom functions to accomplish specific goals.\nThe best analogy for understanding packages also comes from the book Statistical Inference via Data Science. The functionality in base R is like the features built into a smartphone. A smartphone can do a lot on its own, but you usually want to install additional apps for specific tasks. Packages are like apps, giving you functionality beyond what’s built into base R. In Chapter 12, you’ll create your own R package.\nYou can install packages using the install.packages() function. You’ll be working with the tidyverse package, which provides a range of functions for data import, cleaning, analysis, visualization, and more. To install it, enter install.packages(\"tidyverse\"). Typically, you’ll enter package installation code in the console rather than in a script file because you need to install a pack- age only once on your computer to access its code in the future.\nTo confirm that the tidyverse package has been installed correctly, click the Packages tab on the bottom-right pane in RStudio. Search for tidyverse, and you should see it pop up.\nNow that you’ve installed the tidyverse, you’ll put it to use. Although you need to install packages only once per computer, you need to load them each time you restart RStudio. Return to the sample-code.R file and reimport your data using a function from the tidyverse package (your filepath will look slightly different):\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nAt the top of the script, the line library(tidyverse) loads the tidyverse package. Then, the package’s read_csv() function imports the data. Note the underscore (_) in place of the period (.) in the function’s name; this differs from the base R function you used earlier. Using read_csv() to import CSV files achieves the same goal of creating an object, however — in this case, one called population_data_2. Enter population_data_2 in the console, and you should see this output:\n\npopulation_data_2 &lt;- read_csv(file = \"data/population-by-state.csv\")\n\npopulation_data_2\n\n# A tibble: 52 × 9\n    rank State       Pop  Growth Pop2018 Pop2010 growthSince2010 Percent density\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Califor… 3.96e7  0.0038  3.95e7  3.73e7          0.0615  0.118     254.\n 2     2 Texas    2.97e7  0.0385  2.86e7  2.52e7          0.178   0.0889    114.\n 3     3 Florida  2.19e7  0.033   2.12e7  1.88e7          0.164   0.0656    409.\n 4     4 New York 1.93e7 -0.0118  1.95e7  1.94e7         -0.0051  0.0577    410.\n 5     5 Pennsyl… 1.28e7  0.0003  1.28e7  1.27e7          0.0073  0.0383    286.\n 6     6 Illinois 1.26e7 -0.0121  1.27e7  1.28e7         -0.0211  0.0376    226.\n 7     7 Ohio     1.17e7  0.0033  1.17e7  1.15e7          0.0152  0.035     287.\n 8     8 Georgia  1.08e7  0.0303  1.05e7  9.71e6          0.115   0.0324    188.\n 9     9 North C… 1.07e7  0.0308  1.04e7  9.57e6          0.118   0.032     220.\n10    10 Michigan 9.99e6  0.0008  9.98e6  9.88e6          0.0116  0.0299    177.\n# ℹ 42 more rows\n\n\nThis data looks slightly different from the data you generated using the read.csv() function. For example, R shows only the first 10 rows. This variation occurs because read_csv() imports the data not as a data frame but as a data type called a tibble. Both data frames and tibbles are used to describe rectangular data like what you would see in a spreadsheet. There are some minor differences between data frames and tibbles, the most important of which is that tibbles print only the first 10 rows by default, while data frames print all rows. For the purposes of this book, the two terms are used interchangeably.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#rstudio-projects",
    "href": "crash-course.html#rstudio-projects",
    "title": "1  An R Programming Crash Course",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nSo far, you’ve imported a CSV file from your Documents folder. But because others won’t have this exact location on their computer, your code won’t work if they try to run it. One solution to this problem is an RStudio project.\nBy working in a project, you can use relative paths to your files instead of having to write the entire filepath when calling a function to import data. Then, if you place the CSV file in your project, anyone can open it by using the file’s name, as in read_csv(file = \"population-by-state.csv\"). This makes the path easier to write and enables others to use your code.\nTo create a new RStudio project, go to File New Project. Select either New Directory or Existing Directory and choose where to put your project. If you choose New Directory, you’ll need to specify that you want to create a new project. Next, choose a name for the new directory and where it should live. (Leave the checkboxes that ask about creating a Git repository and using renv unchecked; they’re for more advanced purposes.)\nOnce you’ve created this project, you should see two major differences in RStudio’s appearance. First, the files pane no longer shows every file on your computer. Instead, it shows only files in the example-project directory. Right now, that’s just the example-project.Rproj file, which indicates that the folder contains a project. Second, at the top right of RStudio, you can see the name example-project. This label previously read Project: (None). If you want to make sure you’re working in a project, check for its name here. Figure 1.6 shows these changes.\n\n\n\n\n\nFigure 1.6: RStudio with an active project\n\n\nNow that you’ve created a project, copy the population-by-state.csv file into the example-project directory. Once you’ve done so, you should see it in the RStudio files pane.\nWith this CSV file in your project, you can now import it more easily. As before, start by loading the tidyverse package. Then, remove the reference to the Documents folder and import your data by simply using the name of the file:\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"population-by-state.csv\")\n\nThe reason you can import the population-by-state.csv file this way is that the RStudio project sets the working directory to be the root of your project. With the working directory set like this, all references to files are relative to the .Rproj file at the root of the project. Now anyone can run this code because it imports the data from a location that is guaranteed to exist on their computer.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#data-analysis-with-the-tidyverse",
    "href": "crash-course.html#data-analysis-with-the-tidyverse",
    "title": "1  An R Programming Crash Course",
    "section": "Data Analysis with the tidyverse",
    "text": "Data Analysis with the tidyverse\nNow that you’ve imported the population data, you’re ready to do a bit of analysis on it. Although I’ve been referring to the tidyverse as a single package, it’s actually a collection of packages. We’ll explore several of its functions throughout this book, but this section introduces you to its basic workflow.\ntidyverse Functions\nBecause you’ve loaded the tidyverse package, you can now access its functions. For example, the package’s summarize() function takes a data frame or tibble and calculates some piece of information for one or more of the variables in that dataset. The following code uses summarize() to calculate the mean population of all states:\n\nsummarize(.data = population_data_2, mean_population = mean(Pop))\n\nFirst, the code passes population_data_2 to the summarize() function’s .data argument to tell R to use that data frame to perform the calculation. Next, it creates a new variable called mean_population and assigns it to the output of the mean() function introduced earlier. The mean() function runs on Pop, one of the variables in the population_data_2 data frame.\nYou might be wondering why you don’t need to use the c() function within mean(), as shown earlier in this chapter. The reason is that you’re passing the function only one argument here: Pop, which contains the set of population data for which you’re calculating the mean. In this case, there’s no need to use c() to combine multiple values into one.\nRunning this code should return a tibble with a single variable (mean_population), as shown here:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        6433422.\n\n\nThe variable is of type double (dbl), which is used to hold general numeric data. Other common data types are integer (for whole numbers, such as 4, 82, and 915), character (for text values), and logical (for the TRUE/FALSE values returned from comparison operations). The mean_population variable has a value of 6433422, the mean population of all states.\nNotice also that the summarize() function creates a totally new tibble from the original population_data_2 data frame. This is why the variables from population_data_2 are no longer present in the output. This is a basic example of data analysis, but you can do a lot more with the tidyverse.\nThe tidyverse Pipe\nOne advantage of working with the tidyverse is that it uses the pipe for multi-step operations. The tidyverse pipe, which is written as %&gt;%, allows you to break steps into multiple lines. For example, you could rewrite your code using the pipe like so:\n\npopulation_data_2 %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis code says, “Start with the population_data_2 data frame, then run the summarize() function on it, creating a variable called mean_population by calculating the mean of the Pop variable.”\nNotice that the line following the pipe is indented. To make the code easier to read, RStudio automatically adds two spaces to the start of lines that follow pipes.\nThe pipe becomes even more useful when you use multiple steps in your data analysis. Say, for example, you want to calculate the mean population of the five largest states. The following code adds a line that uses the filter() function, also from the tidyverse package, to include only states where the rank variable is less than or equal to (&lt;=) 5. Then, it uses summarize() to calculate the mean of those states:\n\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nRunning this code returns the mean population of the five largest states:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        24678497\n\n\nUsing the pipe to combine functions lets you refine your data in multiple ways while keeping it readable and easy to understand. Indentation can also make your code more readable. You’ve seen only a few functions for analysis at this point, but the tidyverse has many more functions that enable you to do nearly anything you could hope to do with your data. Because of how useful the tidyverse is, it will appear in every single piece of R code you write in this book.\n\n\n\n\n\n\nR for Data Science, 2nd edition, by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund is the bible of tidyverse programming and worth reading for more details on how the package’s many functions work.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#comments",
    "href": "crash-course.html#comments",
    "title": "1  An R Programming Crash Course",
    "section": "Comments",
    "text": "Comments\nIn addition to code, R script files often contain comments — lines that begin with hash marks (#) and aren’t treated as runnable code but instead as notes for anyone reading the script. For example, you could add a comment to the code from the previous section, like so:\n\n# Calculate the mean population of the five largest states\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis comment will help others understand what is happening in the code, and it can also serve as a useful reminder for you if you haven’t worked on the code in a while. R knows to ignore any lines that begin with the hash mark instead of trying to run them.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#how-to-get-help",
    "href": "crash-course.html#how-to-get-help",
    "title": "1  An R Programming Crash Course",
    "section": "How to Get Help",
    "text": "How to Get Help\nNow that you’ve learned the basics of how R works, you’re probably ready to dive in and write some code. When you do, though, you’re going to encounter errors. Being able to get help when you run into issues is a key part of learning to use R successfully. There are two main strategies you can use to get unstuck.\nThe first is to read the documentation for the functions you use. Remember, to access the documentation for any function, simply enter ? and then the name of the function in the console. In the bottom-right pane in Figure 1.7, for example, you can see the result of running ?read.csv.\n\n\n\n\n\nFigure 1.7: The documentation for the read.csv() function\n\n\nHelp files can be a bit hard to decipher, but essentially they describe what package the function comes from, what the function does, what argu- ments it accepts, and some examples of how to use it.\n\n\n\n\n\n\nFor additional guidance on reading documentation, I recommend the appendix of Kieran Healy’s book Data Visualization: A Practical Introduction. A free online version is available at https://socviz.co/appendix.html.\n\n\n\nThe second approach is to read the documentation websites associated with many R packages. These can be easier to read than RStudio’s help files. In addition, they often contain longer articles, known as vignettes, that provide an overview of how a given package works. Reading these can help you understand how to combine individual functions in the context of a larger project. Every package discussed in this book has a good documentation website.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#summary",
    "href": "crash-course.html#summary",
    "title": "1  An R Programming Crash Course",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned the basics of R programming. You saw how to download and set up R and RStudio, what the various RStudio panes are for, and how R script files work. You also learned how to import CSV files and explore them in R, how to save data as objects, and how to install packages to access additional functions. Then, to make the files used in your code more accessible, you created an RStudio project. Finally, you experi- mented with tidyverse functions and the tidyverse pipe, and you learned how to get help when those functions don’t work as expected.\nNow that you understand the basics, you’re ready to start using R to work with your data. See you in Chapter 2!",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#additional-resources",
    "href": "crash-course.html#additional-resources",
    "title": "1  An R Programming Crash Course",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nKieran Healy, Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nChester Ismay and Albert Y. Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Boca Raton, FL: CRC Press, 2020), https://moderndive.com.\nDavid Keyes, “Getting Started with R,” online course, accessed November 10, 2023, https://rfortherestofus.com/courses/getting-started.\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science, 2nd ed. (Sebastopol, CA: O’Reilly Media, 2023). https://r4ds.hadley.nz/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "2  Principles of Data Visualization",
    "section": "",
    "text": "The Drought Visualization\nOther news organizations had relied on the same National Drought Center data in their stories, but Scherer and Karamanis visualized it so that it both grabs attention and communicates the scale of the phenomenon. Figure 2.1 shows a section of the final visualization (due to space constraints, I could include only four regions). The graph makes apparent the increase in drought conditions over the last two decades, especially in California and the Southwest.\nTo understand why this visualization is effective, let’s break it down.\nAt the broadest level, the data visualization is notable for its minimalist aesthetic. For example, there are no grid lines and few text labels, as well as minimal text along the axes. Scherer and Karamanis removed what statistician Edward Tufte, in his 1983 book The Visual Display of Quantitative Information (Graphics Press), calls chartjunk. Tufte wrote that extraneous elements often hinder, rather than help, our understanding of charts (and researchers and data visualization designers have generally agreed).\nNeed proof that Scherer and Karamanis’s decluttered graph is better than the alternative? Figure 2.2 shows a version with a few tweaks to the code to include grid lines and text labels on axes.\nFigure 2.1: A section of the final drought visualization, with a few tweaks made to fit this book\nFigure 2.2: The cluttered version of the drought visualization\nIt’s not just that this cluttered version looks worse; the clutter actively inhibits understanding. Rather than focusing on overall drought patterns (the point of the graph), our brains get stuck reading repetitive and unnecessary axis text.\nOne of the best ways to reduce clutter is to break a single chart into a set of component charts, as Scherer and Karamanis have done (this approach, known as faceting, will be discussed further in Section 2.4.3). Each rectangle represents one region in one year. Filtering the larger chart to show the Southwest region in 2003 produces the graph shown in Figure 2.3, where the x-axis indicates the week and the y-axis indicates the percentage of that region at different drought levels.\nFigure 2.3: A drought visualization for the Southwest in 2003\nZooming in on a single region in a single year also makes the color choices more obvious. The lightest orange bars show the percentage of the region that is abnormally dry, and the darkest purple bars show the percentage experiencing exceptional drought conditions. As you’ll see shortly, this range of colors was intentionally chosen to make differences in the drought levels visible to all readers.\nDespite the graph’s complexity, the R code that Scherer and Karamanis wrote to produce it is relatively simple, due largely to a theory called the grammar of graphics.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#the-grammar-of-graphics",
    "href": "data-viz.html#the-grammar-of-graphics",
    "title": "2  Principles of Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nWhen working in Excel, you begin by selecting the type of graph you want to make. Need a bar chart? Click the bar chart icon. Need a line chart? Click the line chart icon. If you’ve only ever made charts in Excel, this first step may seem so obvious that you’ve never even given the data visualization process much thought, but in fact there are many ways to think about graphs. For example, rather than thinking of graph types as distinct, we can recognize and use their commonalities as the starting point for making them.\nThis approach to thinking about graphs comes from the late statistician Leland Wilkinson. For years, Wilkinson thought deeply about what data visualization is and how we can describe it. In 1999 he published a book called The Grammar of Graphics (Springer) that sought to develop a consistent way of describing all graphs. In it, Wilkinson argued that we should think of plots not as distinct types, à la Excel, but as following a grammar that we can use to describe any plot. Just as English grammar tells us that a noun is typically followed by a verb (which is why “he goes” works, while the opposite, “goes he,” does not), the grammar of graphics helps us understand why certain graph types “work.”\nThinking about data visualization through the lens of the grammar of graphics helps highlight, for example, that graphs typically have some data that is plotted on the x-axis and other data that is plotted on the y-axis. This is the case whether the graph is a bar chart or a line chart, as Figure 2.4 shows.\n\n\n\n\n\n\n\nFigure 2.4: A bar chart and a line chart showing identical data\n\n\n\n\nWhile the graphs look different (and would, to the Excel user, be different types of graphs), Wilkinson’s grammar of graphics emphasizes their similarities. (Incidentally, Wilkinson’s feelings on graph-making tools like Excel became clear when he wrote that “most charting packages channel user requests into a rigid array of chart types.”)\nWhen Wilkinson wrote his book, no data visualization tool could imple- ment his grammar of graphics. This would change in 2010, when Hadley Wickham announced the ggplot2 package for R in the article “A Layered Grammar of Graphics,” published in the Journal of Computational and Graphical Statistics. By providing the tools to implement Wilkinson’s ideas, ggplot2 would come to revolutionize the world of data visualization.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#working-with-ggplot2",
    "href": "data-viz.html#working-with-ggplot2",
    "title": "2  Principles of Data Visualization",
    "section": "Working With ggplot2",
    "text": "Working With ggplot2\nThe ggplot2 R package (which I, like nearly everyone in the data visualization world, will refer to simply as ggplot) relies on the idea of plots having multiple layers. This section will walk you through some of the most impor- tant ones. You’ll begin by selecting variables to map to aesthetic properties. Then you’ll choose a geometric object to use to represent your data. Next, you’ll change the aesthetic properties of your chart (its color scheme, for example) using a scale_ function. Finally, you’ll use a theme_ function to set the overall look and feel of your plot.\nMapping Data to Aesthetic Properties\nTo create a graph with ggplot, you begin by mapping data to aesthetic properties. All this really means is that you use elements like the x- or y-axis, color, and size (the so-called aesthetic properties) to represent variables. You’ll use the data on life expectancy in Afghanistan, introduced in Figure 2.4, to generate a plot. To access this data, enter the following code:\n\nlibrary(tidyverse)\n\ngapminder_10_rows &lt;- read_csv(\"https://data.rwithoutstatistics.com/gapminder_10_rows.csv\")\n\nThis code first loads the tidyverse package, introduced in Chapter 1, and then uses the read_csv() function to access data from the book’s website and assign it to the gapminder_10_rows object.\nThe resulting gapminder_10_rows tibble looks like this:\n\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n\n\nThis output is a shortened version of the full which includes over 1,700 rows of data.\nBefore making a chart with ggplot, you need to decice which variable to put on the x-axis and which to put on the y-axis. For data showing change over time, it’s common to put the date (in this case, year) on the x-axis and the changing value (in this case, lifeExp) on the y-axis. To do so, define the ggplot() function as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n)\n\nThis function contains numerous arguments. Each argument goes on its own line, for the sake of readability, separated by commas. The data argument tells R to use the data frame gapminder_10_rows, and the mapping argument maps year to the x-axis and lifeExp to the y-axis.\nRunning this code produces the chart in Figure 2.5, which doesn’t look like much yet.\n\n\n\n\n\n\n\nFigure 2.5: A blank chart that maps year values to the x-axis and life expectancy values to the y-axis\n\n\n\n\nNotice that the x-axis corresponds to year and the y-axis corresponds to lifeExp, and the values on both axes match the scope of the data. In the gapminder_10_rows data frame, the first year is 1952 and the last year is 1997. The range of the x-axis has been created with this data in mind. Likewise, the values for lifeExp, which go from about 28 to about 42, will fit nicely on the y-axis.\nChoosing the Geometric Objects\nAxes are nice, but the graph is missing any type of visual representation of the data. To get this, you need to add the next ggplot layer: geoms. Short for geometric objects, geoms are functions that provide different ways of representing data. For example, to add points to the graph, you use geom_point():\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point()\n\nNow the graph shows that people in 1952 had a life expectancy of about 28 and that this value rose every year in the dataset (see Figure 2.6).\n\n\n\n\n\n\n\nFigure 2.6: The life expectancy chart with points added\n\n\n\n\nSay you change your mind and want to make a line chart instead. All you have to do is replace geom_point() with geom_line() like so:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_line()\n\nFigure 2.7 shows the result.\n\n\n\n\n\n\n\nFigure 2.7: The data as a line chart\n\n\n\n\nTo really get fancy, you could add both geom_point() and geom_line() as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point() +\n  geom_line()\n\nThis code generates a line chart with points, as shown in Figure 2.8.\n\n\n\n\n\n\n\nFigure 2.8: The same data with both points and a line\n\n\n\n\nYou can swap in geom_col() to create a bar chart:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_col()\n\nNotice in Figure 2.9 that the y-axis range has been automatically updated, going from 0 to 40 to account for the different geom.\n\n\n\n\n\n\n\nFigure 2.9: The life expectancy data as a bar chart\n\n\n\n\nAs you can see, the difference between a line chart and a bar chart isn’t as great as the Excel chart-type picker might have you believe. Both can have the same underlying properties (namely, years on the x-axis and life expectancies on the y-axis). They simply use different geometric objects to visually represent the data.\nMany geoms are built into ggplot. In addition to geom_bar(), geom_point(), and geom_line(), the geoms geom_histogram(), geom_boxplot(), and geom_area() are among the most commonly used. To see all geoms, visit the ggplot documentation website at https://ggplot2.tidyverse.org/reference/index.html#geoms.\nAltering Aesthetic Properties\nBefore we return to the drought data visualization, let’s look at a few additional layers you can use to alter the bar chart. Say you want to change the color of the bars. In the grammar of graphics approach to chart-making, this means mapping some variable to the aesthetic property of fill. (For a bar chart, the aesthetic property of color would change only the outline of each bar.) In the same way that you mapped year to the x-axis and lifeExp to the y-axis, you can map fill to a variable, such as year:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col()\n\nFigure 2.10 shows the result. Now the fill is darker for earlier years and lighter for later years (as also indicated by the legend, added to the right of the plot).\n\n\n\n\n\n\n\nFigure 2.10: The same chart, now with added colors\n\n\n\n\nTo change the fill colors, use a new scale layer with the scale_fill _viridis_c() function (the c at the end of the function name refers to the fact that the data is continuous, meaning it can take any numeric value):\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c()\n\nThis function changes the default palette to one that is colorblind-friendly and prints well in grayscale. The scale_fill_viridis_c() function is just one of many that start with scale_ and can alter the fill scale. Chapter 11 of ggplot2: Elegant Graphics for Data Analysis, 3rd edition, discusses various color and fill scales. You can read it online at https://ggplot2-book.org/scales-colour.html.\nThe Fourth Layer: Setting a Theme\nThe final layer we’ll look at is the theme layer, which allows you to change the overall look and feel of your plots (including their background and grid lines). As with the scale_ functions, a number of functions also start with theme_. Add theme_minimal() as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\nThis theme starts to declutter the plot, as you can see in Figure 2.11.\n\n\n\n\n\n\n\nFigure 2.11: The same chart with theme_minimal() added\n\n\n\n\nBy now, you should see why Hadley Wickham described the ggplot2 package as using a layered grammar of graphics. It implements Wilkinson’s theory by creating multiple layers: first, variables to map to aesthetic prop- erties; second, geoms to represent the data; third, the scale_ function to adjust aesthetic properties; and finally, the theme_ function to set the plot’s overall look and feel.\nYou could still improve this plot in many ways, but instead let’s return to the drought data visualization by Scherer and Karamanis. By walking through their code, you’ll learn about making high-quality data visualiza- tion with ggplot and R.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "href": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "title": "2  Principles of Data Visualization",
    "section": "Recreating the Drought Visualization with ggplot",
    "text": "Recreating the Drought Visualization with ggplot\nThe drought visualization code relies on a combination of ggplot fundamentals and some lesser-known tweaks that make it really shine. To understand how Scherer and Karamanis made their data visualization, we’ll start with a simplified version of their code, then build it up layer by layer, adding elements as we go.\nFirst, you’ll import the data. Scherer and Karamanis did a bunch of data wrangling on the raw data, but I’ve saved the simplified output for you. Because it’s in JavaScript Object Notation (JSON) format, Scherer and Karamanis use the import() function from the rio package, which simplifies the process of importing JSON data:\n\nlibrary(rio)\n\nddm_perc_cat_hubs &lt;- import(\"https://data.rfortherestofus.com/dm_perc_cat_hubs.json\")\n\nJSON is a common format for data used in web applications, though it’s far less common in R, where it can be complicated to work with. Luckily, the rio package simplifies its import.\nPlotting One Region and Year\nScherer and Karamanis’s final plot consists of many years and regions. To see how they created it, we’ll start by looking at just the Southwest region in 2003.\nFirst, you need to create a data frame. You’ll use the filter() function twice: the first time to keep only data for the Southwest region, and the second time to keep only data from 2003. In both cases, you use the following syntax:\n\nfilter(variable_name == value)\n\nThis tells R to keep only observations where variable_name is equal to some value. The code starts with the dm_perc_cat_hubs_raw data frame before filtering it and then saving it as a new object called southwest_2003:\n\nsouthwest_2003 &lt;- dm_perc_cat_hubs %&gt;%\n  filter(hub == \"Southwest\") %&gt;%\n  filter(year == 2003)\n\nTo take a look at this object and see the variables you have to work with, enter southwest_2003 in the console, which should return this output:\n\n\n# A tibble: 255 × 7\n   date       hub       category percentage  year  week max_week\n   &lt;date&gt;     &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 2003-12-30 Southwest D0           0.0718  2003    52       52\n 2 2003-12-30 Southwest D1           0.0828  2003    52       52\n 3 2003-12-30 Southwest D2           0.269   2003    52       52\n 4 2003-12-30 Southwest D3           0.311   2003    52       52\n 5 2003-12-30 Southwest D4           0.0796  2003    52       52\n 6 2003-12-23 Southwest D0           0.0823  2003    51       52\n 7 2003-12-23 Southwest D1           0.131   2003    51       52\n 8 2003-12-23 Southwest D2           0.189   2003    51       52\n 9 2003-12-23 Southwest D3           0.382   2003    51       52\n10 2003-12-23 Southwest D4           0.0828  2003    51       52\n# ℹ 245 more rows\n\n\nThe date variable represents the start date of the week in which the observation took place. The hub variable is the region, and category is the level of drought: a value of D0 indicates the lowest level of drought, while D5 indicates the highest level. The percentage variable is the percentage of that region in that drought category, ranging from 0 to 1. The year and week variables are the observation year and week number (beginning with week 1). The max_week variable is the maximum number of weeks in a given year.\nNow you can use this southwest_2003 object for your plot:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col()\n\nThe ggplot() function tells R to put week on the x-axis and percentage on the y-axis, as well as to use the category variable for the fill color. The geom_col() function creates a bar chart in which each bar’s fill color represents the percentage of the region at each drought level for that particular week, as shown in Figure 2.12.\n\n\n\n\n\n\n\nFigure 2.12: One year (2003) and region (Southwest) of the drought visualization\n\n\n\n\nThe colors, which include bright pinks, blues, greens, and reds, don’t match the final version of the plot, but you can start to see the outlines of Scherer and Karamanis’s data visualization.\nChanging Aesthetic Properties\nScherer and Karamanis next selected different fill colors for their bars. To do so, they used the scale_fill_viridis_d() function. The d here means that the data to which the fill scale is being applied has discrete categories (D0, D1, D2, D3, D4, and D5):\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  )\n\nThey used the argument option = \"rocket\" to select the rocket palette, whose colors range from cream to nearly black. You could use several other palettes within the scale_fill_viridis_d() function; see them at https://sjmgarnier.github.io/viridisLite/reference/viridis.html.\nThen they used the direction = -1 argument to reverse the order of fill colors so that darker colors mean higher drought conditions.\nScherer and Karamanis also tweaked the appearance of the x- and y-axes:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  )\n\nOn the x axis, they removed both the axis title (“week”) using name = NULL and the 0–50 text with guide = \"none\". On the y axis, they removed the title and text showing percentages using labels = NULL, which functionally does the same thing as guide = \"none\". They also moved the axis lines themselves to the right side using position = \"right\". These axis lines are apparent only as tick marks at this point but will become more visible later. Figure 2.13 shows the result of these tweaks.\nOn the x-axis, they removed both the axis title (“week”) using name = NULL and the axis labels (the weeks numbered 0 to 50) with guide = \"none\". On the y-axis, they removed the title and text showing percentages using labels = NULL, which functionally does the same thing as guide = \"none\". They also moved the axis lines themselves to the right side using position = \"right\". These axis lines are apparent only as tick marks at this point but will become more visible later. Figure 2.13 shows the result of these tweaks.\n\n\n\n\n\n\n\nFigure 2.13: The 2003 drought data for the Southwest with adjustments to the x- and y-axes\n\n\n\n\nUp to this point, we’ve focused on one of the single plots that make up the larger data visualization. But the final product that Scherer and Karamanis made is actually 176 plots visualizing 22 years and 8 regions. Let’s discuss the ggplot feature they used to create all of these plots.\nFaceting the Plot\nOne of ggplot’s most useful capabilities is faceting (or, as it’s more commonly known in the data visualization world, small multiples). Faceting uses a variable to break down a single plot into multiple plots. For example, think of a line chart showing life expectancy by country over time; instead of multiple lines on one plot, faceting would create multiple plots with one line per plot. To specify which variable to put in the rows and which to put in the columns of your faceted plot, you use the facet_grid() function, as Scherer and Karamanis did in their code:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  )\n\nScherer and Karamanis put year in rows and hub (region) in columns. The switch = \"y\" argument moves the year label from the right side (where it appears by default) to the left. With this code in place, you can see the final plot coming together in Figure 2.14.\n\n\n\n\n\n\n\nFigure 2.14: The faceted version of the drought visualization\n\n\n\n\nIncredibly, the broad outlines of the plot took just 10 lines of code to create. The rest of the code falls into the category of small polishes. That’s not to minimize how important small polishes are (very) or the time it takes to create them (a lot). It does show, however, that a little bit of ggplot goes a long way.\nAdding Final Polishes\nNow let’s look at a few of the small polishes that Scherer and Karamanis made. The first is to apply a theme. They used theme_light(), which removes the default gray background and changes the font to Roboto using the base_family argument.\nThe theme_light() function is what’s known as a complete theme, one that changes the overall look and feel of a plot. The ggplot package has multiple complete themes that you can use (they’re listed at https://ggplot2.tidyverse.org/reference/index.html#themes). Individuals and organizations also make their own themes, as you’ll do in Chapter 3. For a discussion of which themes you might consider using, see my blog post at https://rfortherestofus.com/2019/08/themes-to-improve-your-ggplot-figures.\nScherer and Karamanis didn’t stop by simply applying theme_light(). They also used the theme() function to make additional tweaks to the plot’s design:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4\n  ) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  theme_light(base_family = \"Roboto\") +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      face = \"bold\"\n    ),\n    legend.text = element_text(color = \"#2DAADA\"),\n    strip.text.x = element_text(\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThe code in the theme() function does many different things, but let’s look at a few of the most important. First, it moves the legend from the right side (the default) to the top of the plot. Then, the angle = 0 argument rotates the year text in the columns from vertical to horizontal. Without this argument, the years would be much less legible.\nThe theme() function also makes the distinctive axis lines and ticks that appear on the right side of the final plot. Calling element_blank() removes all grid lines. Finally, this code removes the borders and gives each individual plot a transparent background.\nYou might be thinking, Wait. Didn’t the individual plots have a gray back- ground behind them? Yes, dear reader, they did. Scherer and Karamanis made these with a separate geom, geom_rect():\n\ngeom_rect(\n  aes(\n    xmin = .5,\n    xmax = max_week + .5,\n    ymin = -0.005,\n    ymax = 1\n  ),\n  fill = \"#f4f4f9\",\n  color = NA,\n  size = 0.4\n)\n\nThey also set some additional aesthetic properties specific to this geom — xmin, xmax, ymin, and ymax — which determine the boundaries of the rectangle it produces. The result is a gray background behind each small multiple, as shown in Figure 2.15.\n\n\n\n\n\n\n\nFigure 2.15: Faceted version of the drought visualization with gray backgrounds behind each small multiple\n\n\n\n\nFinally, Scherer and Karamanis made some tweaks to the legend. Previously you saw a simplified version of the scale_fill_viridis_d() function. Here’s a more complete version:\n\nscale_fill_viridis_d(\n  option = \"rocket\",\n  direction = -1,\n  name = \"Category:\",\n  labels = c(\n    \"Abnormally Dry\",\n    \"Moderate Drought\",\n    \"Severe Drought\",\n    \"Extreme Drought\",\n    \"Exceptional Drought\"\n  )\n)\n\nThe name argument sets the legend title, and the labels argument specifies the labels that show up in the legend. Figure 2.16 shows the result of these changes.\n\n\n\n\n\n\n\nFigure 2.16: The drought visualization with changes to the legend text\n\n\n\n\nRather than D0, D1, D2, D3, and D4, the legend text now reads Abnormally Dry, Moderate Drought, Severe Drought, Extreme Drought, and Exceptional Drought — much more user-friendly categories.\nThe Complete Visualization Code\nWhile I’ve shown you a nearly complete version of the code that Scherer and Karamanis wrote, I made some small changes to make it easier to understand. If you’re curious, the full code is here:\n\nggplot(dm_perc_cat_hubs, aes(week, percentage)) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4,\n    show.legend = FALSE\n  ) +\n  geom_col(\n    aes(\n      fill = category,\n      fill = after_scale(addmix(\n        darken(\n          fill,\n          .05,\n          space = \"HLS\"\n        ),\n        \"#d8005a\",\n        .15\n      )),\n      color = after_scale(darken(\n        fill,\n        .2,\n        space = \"HLS\"\n      ))\n    ),\n    width = .9,\n    size = 0.12\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_x_continuous(\n    expand = c(.02, .02),\n    guide = \"none\",\n    name = NULL\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0),\n    position = \"right\",\n    labels = NULL,\n    name = NULL\n  ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    name = \"Category:\",\n    direction = -1,\n    begin = .17,\n    end = .97,\n    labels = c(\n      \"Abnormally Dry\",\n      \"Moderate Drought\",\n      \"Severe Drought\",\n      \"Extreme Drought\",\n      \"Exceptional Drought\"\n    )\n  ) +\n  guides(fill = guide_legend(\n    nrow = 2,\n    override.aes = list(size = 1)\n  )) +\n  theme_light(\n    base_size = 18,\n    base_family = \"Roboto\"\n  ) +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      size = 18,\n      face = \"bold\"\n    ),\n    legend.text = element_text(\n      color = \"#2DAADA\",\n      size = 16\n    ),\n    strip.text.x = element_text(\n      size = 16,\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      size = 18,\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThere are a few additional tweaks to color and spacing, but most of the code reflects what you’ve seen so far.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "href": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "title": "2  Principles of Data Visualization",
    "section": "In Conclusion: ggplot is Your Data Visualization Secret Weapon",
    "text": "In Conclusion: ggplot is Your Data Visualization Secret Weapon\nYou may be thinking that ggplot is the solution to all of your data visualization problems. And yes, you have a new hammer, but not everything is a nail. If you look at the version of the data visualization that appeared in Scientific American in November 2021, you’ll see that some of its annotations aren’t visible in our re-creation. That’s because they were added in post-production. While you could have found ways to create them in ggplot, it’s often not the best use of your time. Get yourself 90 percent of the way there with ggplot and then use Illustrator, Figma, or a similar tool to finish your work.\nEven so, ggplot is a very powerful hammer, used to make plots that you’ve seen in the New York Times, FiveThirtyEight, the BBC, and other well-known news outlets. Although it’s not the only tool that can generate high-quality data visualizations, it makes the process straightforward. The graph by Scherer and Karamanis shows this in several ways:\n\nIt strips away extraneous elements, such as grid lines, to keep the focus on the data itself. Complete themes such as theme_light() and the theme() function allowed Scherer and Karamanis to create a decluttered visualization that communicates effectively.\nIt uses well-chosen colors. The scale_fill_viridis_d() function allowed them to create a color scheme that demonstrates differences between groups, is colorblind-friendly, and shows up well when printed in grayscale.\nIt uses faceting to break down data from two decades and eight regions into a set of graphs that come together to create a single plot. With a single call to the facet_grid() function, Scherer and Karamanis created over 100 small multiples that the tool automatically combined into a single plot.\n\nLearning to create data visualizations in ggplot involves a significant time investment. But the long-term payoff is even greater. Once you learn how ggplot works, you can look at others’ code and learn how to improve your own. By contrast, when you make a data visualization in Excel, the series of point-and-click steps disappears into the ether. To re-create a visualization you made last week, you’ll need to remember the exact steps you used, and to make someone else’s data visualization, you’ll need them to write up their process for you.\nBecause code-based data visualization tools allow you to keep a record of the steps you made, you don’t have to be the most talented designer to make high-quality data visualizations with ggplot. You can study others’ code, adapt it to your own needs, and create your own data visualization that not only is beautiful but also communicates effectively.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#additional-resources",
    "href": "data-viz.html#additional-resources",
    "title": "2  Principles of Data Visualization",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWill Chase, “The Glamour of Graphics,” online course, accessed November 6, 2023, https://rfortherestofus.com/courses/glamour/.\nKieran Healy, Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nCédric Scherer, Graphic Design with ggplot2 (Boca Raton, FL: CRC Press, forthcoming).\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis, 3rd ed. (New York: Springer, forthcoming), https://ggplot2-book.org.\nClaus Wilke, Fundamentals of Data Visualization (Sebastopol, CA: O’Reilly Media, 2019), https://clauswilke.com/dataviz/.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "custom-theme.html",
    "href": "custom-theme.html",
    "title": "3  Custom Data Visualization Themes",
    "section": "",
    "text": "Styling a Plot with a Custom Theme\nThe bbplot package has two functions: bbc_style() and finalise_plot(). The latter deals with tasks like adding the BBC logo and saving plots in the correct dimensions. For now, let’s look at the bbc_style() function, which applies a custom ggplot theme to make all the plots look consistent and follow BBC style guidelines.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "custom-theme.html#styling-a-plot-with-a-custom-theme",
    "href": "custom-theme.html#styling-a-plot-with-a-custom-theme",
    "title": "3  Custom Data Visualization Themes",
    "section": "",
    "text": "An Example Plot\nTo see how this function works, you’ll create a plot showing population data about several penguin species. You’ll be using the palmerpenguins package, which contains data about penguins living on three islands in Antarctica. For a sense of what this data looks like, load the palmerpenguins and tidyverse packages:\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nNow you have data you can work with in an object called penguins. Here’s what the first 10 rows look like:\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nTo get the data in a more usable format, you’ll count how many penguins live on each island with the count() function from the dplyr package (one of several packages that are loaded with the tidyverse):\n\npenguins %&gt;%\n  count(island)\n\nThis gives you some simple data that you can use for plotting:\n\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\n\nYou’ll use this data multiple times in the chapter, so save it as an object called penguins_summary like so:\n\npenguins_summary &lt;- penguins %&gt;%\n  count(island)\n\nNow you’re ready to create a plot. Before you see what bbplot does, make a plot with the ggplot defaults:\n\npenguins_plot &lt;- ggplot(\n  data = penguins_summary,\n  aes(\n    x = island,\n    y = n,\n    fill = island\n  )\n) +\n  geom_col() +\n  labs(\n    title = \"Number of Penguins\",\n    subtitle = \"Islands are in Antarctica\",\n    caption = \"Data from palmerpenguins package\"\n  )\n\nThis code tells R to use the penguins_summary data frame, putting the island on the x-axis and the count of the number of penguins (n) on the y-axis, and making each bar a different color with the fill aesthetic prop- erty. Since you’ll modify this plot multiple times, saving it as an object called penguins_plot simplifies the process. Figure 3.1 shows the resulting plot.\n\n\n\n\n\n\n\nFigure 3.1: A chart with the default theme\n\n\n\n\nThis isn’t the most aesthetically pleasing chart. The gray background is ugly, the y-axis title is hard to read because it’s angled, and the text size overall is quite small. But don’t worry, you’ll be improving it soon.\nThe BBC’s Custom Theme\nNow that you have a basic plot to work with, you’ll start making it look like a BBC chart. To do this, you need to install the bbplot package. First, install the remotes package using install.packages(\"remotes\") so that you can access packages from remote sources. Then, run the following code to install bbplot from the GitHub repository at https://github.com/bbc/bbplot:\n\nlibrary(remotes)\ninstall_github(\"bbc/bbplot\")\n\nOnce you’ve installed the bbplot package, load it and apply the bbc_style() function to the penguins_plot as follows:\n\nlibrary(bbplot)\n\npenguins_plot +\n  bbc_style()\n\nFigure 3.2 shows the result.\n\n\n\n\n\n\n\nFigure 3.2: The same chart with BBC style\n\n\n\n\nVastly different, right? The font size is larger, the legend is on top, there are no axis titles, the grid lines are stripped down, and the background is white. Let’s look at these changes one by one.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "custom-theme.html#the-bbc-theme-components",
    "href": "custom-theme.html#the-bbc-theme-components",
    "title": "3  Custom Data Visualization Themes",
    "section": "The BBC Theme Components",
    "text": "The BBC Theme Components\nYou’ve just seen the difference that the bbc_style() function makes to a basic chart. This section walks you through the function’s code, with some minor tweaks for readability. Functions are discussed further in Chapter 12.\nFunction Definition\nThe first line gives the function a name and indicates that what follows is, in fact, a function definition:\n\nbbc_style &lt;- function() {\n  font &lt;- \"Helvetica\"\n  \n  ggplot2::theme(\n\nThe code then defines a variable called font and assigns it the value Helvetica. This allows later sections to simply use font rather than repeating Helvetica multiple times. If the BBC team ever wanted to use a different font, they could change Helvetica here to, say, Comic Sans and it would update the font for all of the BBC plots (though I suspect higher-ups at the BBC might not be on board with that choice).\nHistorically, working with custom fonts in R was notoriously tricky, but recent changes have made the process much simpler. To ensure that custom fonts such as Helvetica work in ggplot, first install the systemfonts and ragg packages by running this code in the console:\n\ninstall.packages(c(\"systemfonts\", \"ragg\"))\n\nThe systemfonts package allows R to directly access fonts you’ve installed on your computer, and ragg allows ggplot to use those fonts when generating plots.\nNext, select Tools &gt; Global Options from RStudio’s main menu bar. Click the Graphics menu at the top of the interface and, under the Backend option, select AGG. This change should ensure that RStudio renders the previews of any plots with the ragg package. With these changes in place, you should be able to use any fonts you’d like (assuming you have them installed) in the same way that the bbc_style() function uses Helvetica.\nAfter specifying the font to use, the code calls ggplot’s theme() function. Rather than first loading ggplot with library(ggplot2) and then calling its theme() function, the ggplot2::theme() syntax indicates in one step that the theme() function comes from the ggplot2 package. You’ll write code in this way when making an R package in Chapter 12.\nNearly all of the code in bbc_style() exists within this theme() function. Remember from Chapter 2 that theme() makes additional tweaks to an existing theme; it isn’t a complete theme like theme_light(), which will change the whole look and feel of your plot. In other words, by jumping straight into the theme() function, bbc_style() makes adjustments to the ggplot defaults. As you’ll see, the bbc_style() function does a lot of tweaking.\nText\nThe first code section within the theme() function formats the text:\n\nplot.title = ggplot2::element_text(\n  family = font,\n  size = 28,\n  face = \"bold\",\n  color = \"#222222\"\n),\nplot.subtitle = ggplot2::element_text(\n  family = font,\n  size = 22,\n  margin = ggplot2::margin(9, 0, 9, 0)\n),\nplot.caption = ggplot2::element_blank(),\n\nTo make changes to the title, subtitle, and caption, it follows this pattern:\n\nAREA_OF_CHART = ELEMENT_TYPE(\n  PROPERTY = VALUE\n)\n\nFor each area, this code specifies the element type: element_text(), element_line(), element_rect(), or element_blank(). Within the element type is where you assign values to properties—for example, setting the font family (the property) to Helvetica (the value). The bbc_style() function uses the various element_ functions to make tweaks, as you’ll see later in this chapter.\n\n\n\n\n\n\nFor additional ways to customize pieces of your plots, see the ggplot2 package documentation (https://ggplot2.tidyverse.org/reference/element.html), which provides a comprehensive list.\n\n\n\nOne of the main adjustments the bbc_style() function makes is bumping up the font size to help with legibility, especially when plots made with the bbplot package are viewed on smaller mobile devices. The code first formats the title (with plot.title) using Helvetica 28-point bold font in a nearly black color (the hex code #222222). The subtitle (plot.subtitle) is 22-point Helvetica.\nThe bbc_style() code also adds some spacing between the title and subtitle with the margin() function, specifying the value in points for the top (9), right (0), bottom (9), and left (0) sides. Finally, the element_blank() function removes the default caption (set through the caption argument in the labs() function), “Data from palmer penguins package.” (As mentioned earlier, the finalise_plot() function in the bbplot package adds elements, including an updated caption and the BBC logo, to the bottom of the plots.)\nFigure 3.3 shows these changes.\n\n\n\n\n\n\n\nFigure 3.3: The penguin chart with only the text formatting changed\n\n\n\n\nWith these changes in place, you’re on your way to the BBC look.\nLegend\nNext up is formatting the legend, positioning it above the plot and left- aligning its text:\n\nlegend.position = \"top\",\nlegend.text.align = 0,\nlegend.background = element_blank(),\nlegend.title = element_blank(),\nlegend.key = element_blank(),\nlegend.text = element_text(\n  family = font,\n  size = 18,\n  color = \"#222222\"\n),\n\nThis code removes the legend background (which would show up only if the background color of the entire plot weren’t white), the title, and the legend key (the borders on the boxes that show the island names, just barely visible in Figure 3.3). Finally, the code sets the legend’s text to 18-point Helvetica with the same nearly black color. Figure 3.4 shows the result.\n\n\n\n\n\n\n\nFigure 3.4: The penguin chart with changes to the legend\n\n\n\n\nThe legend is looking better, but now it’s time to format the rest of the chart so it matches.\nAxes\nThe code first removes the axis titles because they tend to take up a lot of chart real estate, and you can use the title and subtitle to clarify what the axes show:\n\naxis.title = ggplot2::element_blank(),\naxis.text = ggplot2::element_text(\n  family = font,\n  size = 18,\n  color = \"#222222\"\n),\naxis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),\naxis.ticks = ggplot2::element_blank(),\naxis.line = ggplot2::element_blank(),\n\nAll text on the axes becomes 18-point Helvetica and nearly black. The text on the x-axis (Biscoe, Dream, and Torgersen) gets a bit of spacing around it. Finally, both axes’ ticks and lines are removed. Figure 3.5 shows these changes, although the removal of the axis lines doesn’t make a difference to the display here.\n\n\n\n\n\n\n\nFigure 3.5: The penguin chart with axis formatting changes\n\n\n\n\nThe axis text matches the legend text, and the axis tick marks and lines are gone.\nGrid Lines\nNow for the grid lines:\n\npanel.grid.minor = ggplot2::element_blank(),\npanel.grid.major.y = ggplot2::element_line(color = \"#cbcbcb\"),\npanel.grid.major.x = ggplot2::element_blank(),\n\nThe approach here is fairly straightforward: this code removes minor grid lines for both axes, removes major grid lines on the x-axis, and keeps major grid lines on the y-axis but makes them a light gray (the #cbcbcb hex code). Figure 3.6 shows the result.\n\n\n\n\n\n\n\nFigure 3.6: Our chart with tweaks to the grid lines\n\n\n\n\nBackground\nThe previous iteration of our plot still had a gray background. The bbc_style() function removes this with the following code.\n\npanel.background = ggplot2::element_blank(),\n\nThe plot without the gray background is seen in Figure @ref(fig:penguins-plot-no-bg).\n\n\n\n\n\n\n\nFigure 3.7: The chart with the gray background removed\n\n\n\n\nYou’ve nearly re-created the penguin plot using the bbc_style() function.\nSmall Multiples\nThe bbc_style() function contains a bit more code to modify strip.background and strip.text. In ggplot, the strip refers to the text above faceted charts like the ones discussed in Chapter 2. Next, you’ll turn your penguin chart into a faceted chart to see these components of the BBC’s theme. I’ve used the code from the bbc_style() function, minus the sections that deal with small multiples, to make Figure 3.8.\n\n\n\n\n\n\n\nFigure 3.8: The faceted chart with no changes to the strip text formatting\n\n\n\n\nUsing the facet_wrap() function to make a small multiples chart leaves you with one chart per island, but by default, the text above each small multiple is noticeably smaller than the rest of the chart. What’s more, the gray background behind the text stands out because you’ve already removed the gray background from the other parts of the chart. The consistency you’ve worked toward is now compromised, with small text that is out of proportion to the other chart text and a gray background that sticks out like a sore thumb.\nThe following code changes the strip text above each small multiple:\n\nstrip.background = ggplot2::element_rect(fill = \"white\"),\nstrip.text = ggplot2::element_text(size = 22, hjust = 0)\n\nThis code removes the background (or, more accurately, colors it white). Then it makes the text larger, bold, and left-aligned using hjust = 0. Note that I did have to make the text size slightly smaller than in the actual chart to fit the book, and I added code to make it bold. Figure 3.9 shows the result.\n\n\n\n\n\n\n\nFigure 3.9: The small multiples chart in the BBC style\n\n\n\n\nIf you look at any chart on the BBC website, you’ll see how similar it looks to your own. The tweaks in the bbc_style() function to the text formatting, legends, axes, grid lines, and backgrounds show up in charts viewed by millions of people worldwide.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "custom-theme.html#color",
    "href": "custom-theme.html#color",
    "title": "3  Custom Data Visualization Themes",
    "section": "Color",
    "text": "Color\nYou might be thinking, Wait, what about the color of the bars? Doesn’t the theme change those? This is a common point of confusion, but the answer is that it doesn’t. The documentation for the theme() function explains why this is the case: “Themes are a powerful way to customize the non-data components of your plots: i.e. titles, labels, fonts, background, gridlines, and legends.” In other words, ggplot themes change the elements of the chart that aren’t mapped to data.\nPlots, on the other hand, use color to communicate information about data. In the faceted chart, for instance, the fill property is mapped to the island (Biscoe is salmon, Dream is green, and Torgersen is blue). As you saw in Chapter 2, you can change the fill using the various scale_fill_ functions. In the world of ggplot, these scale_ functions control color, while the custom themes control the chart’s overall look and feel.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "custom-theme.html#summary",
    "href": "custom-theme.html#summary",
    "title": "3  Custom Data Visualization Themes",
    "section": "Summary",
    "text": "Summary\nWhen Stylianou and Guibourg started developing a custom theme for the BBC, they had one question: Would they be able to create graphs in R that could go directly onto the BBC website? Using ggplot, they succeeded. The bbplot package allowed them to make plots with a consistent look and feel that followed BBC standards and, most important, did not require a designer’s help.\nYou can see many of the principles of high-quality data visualization discussed in Chapter 2 in this custom theme. In particular, the removal of extraneous elements (axis titles and grid lines, for instance) helps keep the focus on the data itself. And because applying the theme requires users to add only a single line to their ggplot code, it was easy to get others on board. They had only to append bbc_style() to their code to produce a BBC-style plot.\nOver time, others at the BBC noticed the data journalism team’s production-ready graphs and wanted to make their own. The team members set up R trainings for their colleagues and developed a “cookbook” (https://bbc.github.io/rcookbook/) showing how to make various types of charts. Soon, the quality and quantity of BBC’s data visualization exploded. Stylianou told me, “I don’t think there’s been a day where someone at the BBC hasn’t used the package to produce a graphic.”\nNow that you’ve seen how custom ggplot themes work, try making one of your own. After all, once you’ve written the code, you can apply it with only one line of code.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "custom-theme.html#additional-resources",
    "href": "custom-theme.html#additional-resources",
    "title": "3  Custom Data Visualization Themes",
    "section": "Additional Resources",
    "text": "Additional Resources\nConsult the following resources to learn more about how the BBC created and used their custom theme:\n\nBBC Visual and Data Journalism cookbook for R graphics (2019), https://bbc.github.io/rcookbook/\n“How the BBC Visual and Data Journalism team works with graphics in R” by the BBC Visual and Data Journalism team (2019), https://medium.com/bbc-visual-and-data-journalism/how-the-bbc-visual-and-data-journalism-team-works-with-graphics-in-r-ed0b35693535",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "4  Maps and Geospatial Data",
    "section": "",
    "text": "A Brief Primer on Geospatial Data\nYou don’t need to be a GIS expert to make maps, but you do need to understand a few things about how geospatial data works, starting with its two main types: vector and raster. Vector data uses points, lines, and polygons to represent the world. Raster data, which often comes from digital photo- graphs, ties each pixel in an image to a specific geographic location. Vector data tends to be easier to work with, and you’ll be using it exclusively in this chapter.\nIn the past, working with geospatial data meant mastering competing standards, each of which required learning a different approach. Today, though, most people use the simple features model (often abbreviated as sf) for working with vector geospatial data, which is easier to understand. For example, to import simple features data about the state of Wyoming, enter the following:\nlibrary(sf)\n\nwyoming &lt;- read_sf(\"https://data.rwithoutstatistics.com/wyoming.geojson\")\nAnd then you can look at the data like so:\nwyoming\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -111.0546 ymin: 40.99477 xmax: -104.0522 ymax: 45.00582\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 2\n  NAME                                                                  geometry\n  &lt;chr&gt;                                                            &lt;POLYGON [°]&gt;\n1 Wyoming ((-106.3212 40.99912, -106.3262 40.99927, -106.3265 40.99927, -106.33…\nThe output has two columns, one for the state name (NAME) and another called geometry. This data looks like the data frames you’ve seen before, aside from two major differences.\nFirst, there are five lines of metadata above the data frame. At the top is a line stating that the data contains one feature and one field. A feature is a row of data, and a field is any column containing nonspatial data. Second, the simple features data contains geographical data in a variable called geometry. Because the geometry column must be present for a data frame to be geospatial data, it isn’t counted as a field. Let’s look at each part of this simple features data.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#a-brief-primer-on-geospatial-data",
    "href": "maps.html#a-brief-primer-on-geospatial-data",
    "title": "4  Maps and Geospatial Data",
    "section": "",
    "text": "The Geometry Type\nThe geometry type represents the shape of the geospatial data you’re working with and is typically shown in all caps. In this case, the relatively simple POLYGON type represents a single polygon. You can use ggplot to display this data by calling geom_sf(), a special geom designed to work with simple features data:\n\nlibrary(tidyverse)\n\nwyoming %&gt;%\n  ggplot() +\n  geom_sf()\n\nFigure 4.1 shows the resulting map of Wyoming. It may not look like much, but I wasn’t the one who made Wyoming a nearly perfect rectangle!\n\n\n\n\n\n\n\nFigure 4.1: A map of Wyoming generated using POLYGON simple features data\n\n\n\n\nOther geometry types used in simple features data include POINT, to display elements such as a pin on a map that represents a single location. For example, the map in Figure 4.2 uses POINT data to show a single electric vehicle charging station in Wyoming.\n\n\n\n\n\n\n\nFigure 4.2: A map of Wyoming containing POINT simple features data\n\n\n\n\nThe LINESTRING geometry type is for a set of points that can be con- nected with lines and is often used to represent roads. Figure 4.3 shows a map that uses LINESTRING data to represent a section of US Highway 30 that runs through Wyoming.\n\n\n\n\n\n\n\nFigure 4.3: A road represented using LINESTRING simple features data\n\n\n\n\nEach of these geometry types has a MULTI variation (MULTIPOINT, MULTILINESTRING, and MULTIPOLYGON) that combines multiple instances of the type in one row of data. For example, Figure 4.4 uses MULTIPOINT data to show all electric vehicle charging stations in Wyoming.\n\n\n\n\n\n\n\nFigure 4.4: Using MULTIPOINT data to represent multiple electric vehicle charging stations\n\n\n\n\nLikewise, you can use MULTILINESTRING data to show not just one road but all major roads in Wyoming (Figure 4.5).\n\n\n\n\n\n\n\nFigure 4.5: Using MULTILINESTRING data to represent several roads\n\n\n\n\nFinally, you could use MULTIPOLYGON data, for example, to depict a state made up of multiple polygons. The following data represents the 23 counties in the state of Wyoming:\n\n\nSimple feature collection with 23 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -111.0546 ymin: 40.99477 xmax: -104.0522 ymax: 45.00582\nGeodetic CRS:  WGS 84\n# A tibble: 23 × 2\n   NAME                                                                 geometry\n   &lt;chr&gt;                                                      &lt;MULTIPOLYGON [°]&gt;\n 1 Lincoln     (((-110.5417 42.28652, -110.5417 42.28638, -110.5417 42.28471, -…\n 2 Fremont     (((-109.3258 42.86878, -109.3258 42.86894, -109.3261 42.86956, -…\n 3 Uinta       (((-110.5849 41.57916, -110.5837 41.57916, -110.5796 41.57917, -…\n 4 Big Horn    (((-107.5034 44.64004, -107.5029 44.64047, -107.5023 44.64078, -…\n 5 Hot Springs (((-108.1563 43.47063, -108.1563 43.45961, -108.1564 43.45961, -…\n 6 Washakie    (((-107.6841 44.1664, -107.684 44.1664, -107.684 44.1664, -107.6…\n 7 Converse    (((-105.9232 43.49501, -105.9152 43.49503, -105.9072 43.49505, -…\n 8 Sweetwater  (((-109.5651 40.99839, -109.5652 40.99839, -109.5656 40.99839, -…\n 9 Crook       (((-104.4611 44.18075, -104.4612 44.18075, -104.4643 44.18075, -…\n10 Carbon      (((-106.3227 41.38265, -106.3227 41.38245, -106.3227 41.38242, -…\n# ℹ 13 more rows\n\n\nAs you can see on the second line, the geometry type of this data is MULTIPOLYGON. In addition, the repeated MULTIPOLYGON text in the geometry column indicates that each row contains a shape of type MULTIPOLYGON. Figure 4.6 shows a map made with this data.\n\n\n\n\n\n\n\nFigure 4.6: A map of Wyoming counties\n\n\n\n\nNotice that the map is made up entirely of polygons.\nThe Dimensions\nNext, the geospatial data frame contains the data’s dimensions, or the type of geospatial data you’re working with. In the Wyoming example, it looks like Dimension: XY, meaning the data is two-dimensional, as in the case of all the geospatial data used in this chapter. There are two other dimensions (Z and M) that you’ll see much more rarely. I’ll leave them for you to investigate further.\nBounding Box\nThe penultimate element in the metadata is the bounding box, which represents the smallest area in which you can fit all of your geospatial data. For the wyoming object, it looks like this:\nBounding box:  xmin: -111.0569 ymin: 40.99475 xmax: -104.0522 ymax: 45.0059\nThe ymin value of 40.99475 and ymax value of 45.0059 represent the lowest and highest latitudes, respectively, that the state’s polygon can fit into. The x-values do the same for the longitude. Bounding boxes are calculated auto- matically, and typically you don’t have to worry about altering them.\nThe Coordinate Reference System\nThe last piece of metadata specifies the coordinate reference system used to project the data when it’s plotted. The challenge with representing any geospatial data is that you’re displaying information about the three-dimensional Earth on a two-dimensional map. Doing so requires choosing a coordinate reference system that determines what type of correspondence, or projection, to use when making the map.\nThe data for the Wyoming counties map includes the line Geodetic CRS: WGS 84, indicating the use of a coordinate reference system known as WGS84. To see a different projection, check out the same map using the Albers equal-area conic convenience projection. While Wyoming looked perfectly horizontal in Figure 4.6, the version in Figure 4.7 appears to be tilted.\n\n\n\n\n\n\n\nFigure 4.7: A map of Wyoming counties using the Albers equal-area conic convenience projection\n\n\n\n\nIf you’re wondering how to change projections when making maps of your own, fear not: you’ll see how to do this when we look at Madjid’s map in the next section. And if you want to know how to choose appropriate projections for your maps, check out “Using Appropriate Projections” (Section 4.3.3).\nThe geometry Column\nIn addition to the metadata, simple features data differs from traditional data frames in another respect: its geometry column. As you might have guessed from the name, this column holds the data needed to draw the maps.\nTo understand how this works, consider the connect-the-dots drawings you probably completed as a kid. As you added lines to connect one point to the next, the subject of your drawing became clearer. The geometry column is similar. It has a set of numbers, each of which corresponds to a point. If you’re using LINESTRING/MULTILINESTRING or POLYGON/MULTIPOLYGON simple features data, ggplot uses the numbers in the geometry column to draw each point and then adds lines to connect the points. If you’re using POINT/MULTIPOINT data, it draws the points but doesn’t connect them.\nOnce again, thanks to R, you never have to worry about these details or look in any depth at the geometry column.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#re-creating-the-covid-map",
    "href": "maps.html#re-creating-the-covid-map",
    "title": "4  Maps and Geospatial Data",
    "section": "Re-creating the COVID Map",
    "text": "Re-creating the COVID Map\nNow that you understand the basics of geospatial data, let’s walk through the code Madjid used to make his COVID-19 map. Shown in Figure 4-8, it makes use of the geometry types, dimensions, bounding boxes, projections, and the geometry column just discussed.\n\n\n\n\n\n\n\nFigure 4.8: Abdoul Madjid’s map of COVID-19 in the United States in 2021\n\n\n\n\nI’ve made some small modifications to the code to make the final map fit on the page. You’ll begin by loading a few packages:\n\nlibrary(tidyverse)\nlibrary(albersusa)\nlibrary(sf)\nlibrary(zoo)\nlibrary(colorspace)\n\nThe albersusa package will give you access to geospatial data. Install it as follows:\n\nremotes::install_github(\"hrbrmstr/albersusa\")\n\nYou can install all of the other packages using the standard install.packages() code. You’ll use the tidyverse to import data, manipulate it, and plot it with ggplot. The sf package will enable you to change the coordinate reference system and use an appropriate projection for the data. The zoo package has functions for calculating rolling averages, and the colorspace package gives you a color scale that highlights the data well.\nImporting the Data\nNext, you’ll import the data you need: COVID-19 rates by state over time, state populations, and geospatial information. Madjid imported each of these pieces of data separately and then merged them, and you’ll do the same.\nThe COVID-19 data comes directly from the New York Times, which publishes daily case rates by state as a CSV file on its GitHub account. To import it, enter the following:\n\ncovid_data &lt;- \n  read_csv(\"https://data.rwithoutstatistics.com/covid-us-states.csv\") %&gt;%\n  select(-fips)\n\nFederal Information Processing Standards (FIPS) are numeric codes used to represent states, but you’ll reference states by their names instead, so the line select(-fips) drops the fips variable.\nLooking at this data, you can see the arrival of the first COVID-19 cases in the United States in January 2020:\n\n\n# A tibble: 61,102 × 4\n   date       state      cases deaths\n   &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 2020-01-21 Washington     1      0\n 2 2020-01-22 Washington     1      0\n 3 2020-01-23 Washington     1      0\n 4 2020-01-24 Illinois       1      0\n 5 2020-01-24 Washington     1      0\n 6 2020-01-25 California     1      0\n 7 2020-01-25 Illinois       1      0\n 8 2020-01-25 Washington     1      0\n 9 2020-01-26 Arizona        1      0\n10 2020-01-26 California     2      0\n# ℹ 61,092 more rows\n\n\nMadjid’s map shows per capita rates (the rates per 100,000 people) rather than absolute rates (the rates without consideration for a state’s population). So, to re-create his maps, you also need to obtain data on each state’s population. Download this data as a CSV as follows:\n\nusa_states &lt;- \n  read_csv(\"https://data.rwithoutstatistics.com/population-by-state.csv\") %&gt;%\n  select(State, Pop)\n\nThis code imports the data, keeps the State and Pop (population) variables, and saves the data as an object called usa_states. Here’s what usa_states looks like:\n\n\n# A tibble: 52 × 2\n   State               Pop\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 California     39613493\n 2 Texas          29730311\n 3 Florida        21944577\n 4 New York       19299981\n 5 Pennsylvania   12804123\n 6 Illinois       12569321\n 7 Ohio           11714618\n 8 Georgia        10830007\n 9 North Carolina 10701022\n10 Michigan        9992427\n# ℹ 42 more rows\n\n\nFinally, import the geospatial data and save it as an object called usa_states_geom like so:\n\nusa_states_geom &lt;- usa_sf() %&gt;%\n  select(name) %&gt;%\n  st_transform(us_laea_proj)\n\nThe usa_sf() function from the albersusa package gives you simple features data for all US states. Conveniently, it places Alaska and Hawaii at a position and scale that make them easy to see. This data includes multiple variables, but because you need only the state names, this code keeps just the name variable.\nThe st_transform() function from the sf package changes the coordinate reference system. The one used here comes from the us_laea_proj object in the albersusa package. This is the Albers equal-area conic convenience projection you used earlier to change the appearance of the Wyoming counties map.\nCalculating Daily COVID-19 Cases\nThe covid_data data frame lists cumulative COVID-19 cases by state, but not the number of cases per day, so the next step is to calculate that number:\n\ncovid_cases &lt;- covid_data %&gt;%\n  group_by(state) %&gt;%\n  mutate(\n    pd_cases = lag(cases)\n  ) %&gt;%\n  replace_na(list(pd_cases = 0)) %&gt;%\n  mutate(\n    daily_cases = case_when(\n      cases &gt; pd_cases ~ cases - pd_cases,\n      TRUE ~ 0\n    )\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(state, date)\n\nThe group_by() function calculates totals for each state, then creates a new variable called pd_cases, which represents the number of cases in the previous day (the lag() function is used to assign data to this variable). Some days don’t have case counts for the previous day, so set this value to 0 using the replace_na() function.\nNext, this code creates a new variable called daily_cases. To set the value of this variable, use the case_when() function to create a condition: if the cases variable (which holds the cases on that day) is greater than the pd_cases variable (which holds cases from one day prior), then daily_cases is equal to cases minus pd_cases. Otherwise, you set daily_cases to be equal to 0.\nFinally, because you grouped the data by state at the beginning of the code, now you need to remove this grouping using the ungroup() function before arranging the data by state and date.\nHere’s the resulting covid_cases data frame:\n\n\n# A tibble: 61,102 × 6\n   date       state   cases deaths pd_cases daily_cases\n   &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 2020-03-13 Alabama     6      0        0           6\n 2 2020-03-14 Alabama    12      0        6           6\n 3 2020-03-15 Alabama    23      0       12          11\n 4 2020-03-16 Alabama    29      0       23           6\n 5 2020-03-17 Alabama    39      0       29          10\n 6 2020-03-18 Alabama    51      0       39          12\n 7 2020-03-19 Alabama    78      0       51          27\n 8 2020-03-20 Alabama   106      0       78          28\n 9 2020-03-21 Alabama   131      0      106          25\n10 2020-03-22 Alabama   157      0      131          26\n# ℹ 61,092 more rows\n\n\nIn the next step, you’ll make use of the new daily_cases variable.\nCalculating Incidence Rates\nYou’re not quite done calculating values. The data that Madjid used to make his map didn’t include daily case counts. Instead, it contained a five-day rolling average of cases per 100,000 people. A rolling average is the average case rate in a certain time period. Quirks of reporting (for example, not reporting on weekends but instead rolling Saturday and Sunday cases into Monday) can make the value for any single day less reliable. Using a rolling average smooths out these quirks. Generate this data as follows:\n\ncovid_cases %&gt;%\n  mutate(roll_cases = rollmean(\n    daily_cases,\n    k = 5,\n    fill = NA\n  ))\n\nThis code creates a new data frame called covid_cases_rm (where rm stands for rolling mean). The first step in its creation is to use the rollmean() function from the zoo package to create a roll_cases variable, which holds the average number of cases in the five-day period surrounding a single date. The k argument is the number of days for which you want to calculate the rolling average (5, in this case), and the fill argument determines what happens in cases like the first day, where you can’t calculate a five-day rolling mean because there are no days prior to this day (Madjid set these values to NA).\nAfter calculating roll_cases, you need to calculate per capita case rates. To do this, you need population data, so join the population data from the usa_states data frame with the covid_cases data like so:\n\ncovid_cases_rm &lt;- covid_cases %&gt;%\n  mutate(roll_cases = rollmean(\n    daily_cases,\n    k = 5,\n    fill = NA\n  )) %&gt;%\n  left_join(\n    usa_states,\n    by = c(\"state\" = \"State\")\n  ) %&gt;%\n  drop_na(Pop)\n\nTo drop rows with missing population data, you call the drop_na() function with the Pop variable as an argument. In practice, this removes several US territories (American Samoa, Guam, the Northern Mariana Islands, and the Virgin Islands).\nNext, you create a per capita case rate variable called incidence_rate by multiplying the roll_cases variable by 100,000 and then dividing it by the population of each state:\n\ncovid_cases_rm &lt;- covid_cases_rm %&gt;%\n  mutate(incidence_rate = 10^5 * roll_cases / Pop) %&gt;%\n  mutate(\n    incidence_rate = cut(\n      incidence_rate,\n      breaks = c(seq(0, 50, 5), Inf),\n      include.lowest = TRUE\n    ) %&gt;%\n      factor(labels = paste0(\"&gt;\", seq(0, 50, 5)))\n  )\n\nRather than keeping raw values (for example, on June 29, 2021, Florida had a rate of 57.77737 cases per 100,000 people), you use the cut() function to convert the values into categories: values of &gt;0 (greater than zero), values of &gt;5 (greater than five), and values of &gt;50 (greater than 50).\nThe last step is to filter the data so it includes only 2021 data (the only year depicted in Madjid’s map) and then select just the variables (state, date, and incidence_rate) you’ll need to create the map:\nHere’s the final covid_cases_rm data frame:\n\n\n# A tibble: 18,980 × 3\n   state   date       incidence_rate\n   &lt;chr&gt;   &lt;date&gt;     &lt;fct&gt;         \n 1 Alabama 2021-01-01 &gt;50           \n 2 Alabama 2021-01-02 &gt;50           \n 3 Alabama 2021-01-03 &gt;50           \n 4 Alabama 2021-01-04 &gt;50           \n 5 Alabama 2021-01-05 &gt;50           \n 6 Alabama 2021-01-06 &gt;50           \n 7 Alabama 2021-01-07 &gt;50           \n 8 Alabama 2021-01-08 &gt;50           \n 9 Alabama 2021-01-09 &gt;50           \n10 Alabama 2021-01-10 &gt;50           \n# ℹ 18,970 more rows\n\n\nYou now have a data frame that you can combine with your geospatial data.\nAdding Geospatial Data\nYou’ve used two of the three data sources (COVID-19 case data and state population data) to create the covid_cases_rm data frame you’ll need to make the map. Now it’s time to use the third data source: the geospatial data you saved as usa_states_geom. Simple features data allows you to merge regular data frames and geospatial data (another point in its favor):\n\nusa_states_geom %&gt;%\n  left_join(covid_cases_rm, by = c(\"name\" = \"state\"))\n\nThis code merges the covid_cases_rm data frame into the geospatial data, matching the name variable from usa_states_geom to the state variable in covid_cases_rm.\nNext, you create a new variable called fancy_date to format the date nicely (for example, Jan. 01 instead of 2021-01-01):\n\nusa_states_geom_covid &lt;- usa_states_geom %&gt;%\n  left_join(covid_cases_rm, by = c(\"name\" = \"state\")) %&gt;%\n  mutate(fancy_date = fct_inorder(format(date, \"%b. %d\"))) %&gt;%\n  relocate(fancy_date, .before = incidence_rate)\n\nThe format() function does the formatting, while the fct_inorder() function makes the fancy_date variable sort data by date (rather than, say, alphabetically, which would put August before January). Last, the relocate() function puts the fancy_date column next to the date column.\nSave this data frame as usa_states_geom_covid and take a look at the result:\n\n\nSimple feature collection with 18615 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2100000 ymin: -2500000 xmax: 2516374 ymax: 732103.3\nProjected CRS: +proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\nFirst 10 features:\n      name       date fancy_date incidence_rate                       geometry\n1  Arizona 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((-1111066 -8...\n2  Arizona 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((-1111066 -8...\n3  Arizona 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((-1111066 -8...\n4  Arizona 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((-1111066 -8...\n5  Arizona 2021-01-05    Jan. 05            &gt;50 MULTIPOLYGON (((-1111066 -8...\n6  Arizona 2021-01-06    Jan. 06            &gt;50 MULTIPOLYGON (((-1111066 -8...\n7  Arizona 2021-01-07    Jan. 07            &gt;50 MULTIPOLYGON (((-1111066 -8...\n8  Arizona 2021-01-08    Jan. 08            &gt;50 MULTIPOLYGON (((-1111066 -8...\n9  Arizona 2021-01-09    Jan. 09            &gt;50 MULTIPOLYGON (((-1111066 -8...\n10 Arizona 2021-01-10    Jan. 10            &gt;50 MULTIPOLYGON (((-1111066 -8...\n\n\nYou can see the metadata and geometry columns discussed earlier in the chapter.\nMaking the Map\nIt took a lot of work to end up with the surprisingly simple usa_states_geom_covid data frame. While the data may be simple, the code Madjid used to make his map is quite complex. This section walks you through it in pieces.\nThe final map is actually multiple maps, one for each day in 2021. Combining 365 days makes for a large final product, so instead of showing the code for every single day, filter the usa_states_geom_covid to show just the first six days in January:\n\nusa_states_geom_covid_six_days &lt;- usa_states_geom_covid %&gt;%\n  filter(date &lt;= as.Date(\"2021-01-06\"))\n\nSave the result as a data frame called usa_states_geom_covid_six_days. Here’s what this data looks like:\n\n\nSimple feature collection with 306 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2100000 ymin: -2500000 xmax: 2516374 ymax: 732103.3\nProjected CRS: +proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\nFirst 10 features:\n       name       date fancy_date incidence_rate                       geometry\n1   Arizona 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((-1111066 -8...\n2   Arizona 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((-1111066 -8...\n3   Arizona 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((-1111066 -8...\n4   Arizona 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((-1111066 -8...\n5   Arizona 2021-01-05    Jan. 05            &gt;50 MULTIPOLYGON (((-1111066 -8...\n6   Arizona 2021-01-06    Jan. 06            &gt;50 MULTIPOLYGON (((-1111066 -8...\n7  Arkansas 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((557903.1 -1...\n8  Arkansas 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((557903.1 -1...\n9  Arkansas 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((557903.1 -1...\n10 Arkansas 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((557903.1 -1...\n\n\nMadjid’s map is giant, as it includes all 365 days. The size of a few elements have been changed so that they fit in this book.\nGenerating the Basic Map\nWith your six days of data, you’re ready to make some maps. Madjid’s map-making code has two main parts: generating the basic map, then tweaking its appearance. First, you’ll revisit the three lines of code used to make the Wyoming maps, with some adornments to improve the quality of the visualization:\n\nusa_states_geom_covid_six_days %&gt;%\n  ggplot() +\n  geom_sf(\n    aes(fill = incidence_rate),\n    size = .05,\n    color = \"grey55\"\n  ) +\n  facet_wrap(\n    vars(fancy_date),\n    strip.position = \"bottom\"\n  )\n\nThe geom_sf() function plots the geospatial data, modifying a couple of arguments: size = .05 makes the state borders less prominent and color = \"grey55\" sets them to a medium-gray color. Then, the facet_wrap() function is used for the faceting (that is, to make one map for each day). The vars(fancy _date) code specifies that the fancy_date variable should be used for the faceted maps, and strip.position = \"bottom\" moves the labels Jan. 01, Jan. 02, and so on to the bottom of the maps. Figure Figure 4.9 shows the result.\n\n\n\n\n\n\n\nFigure 4.9: A map showing the incidence rate of COVID-19 for the first six days of 2021\n\n\n\n\nHaving generated the basic map, let’s now make it look good.\nApplying Data Visualization Principles\nFrom now on, all of the code that Madjid uses is to improve the appearance of the maps. Many of the tweaks shown here should be familiar if you’ve read Chapter 2, highlighting a benefit of making maps with ggplot: you can apply the same data visualization principles you learned about when making charts.\n\nusa_states_geom_covid_six_days %&gt;%\n  ggplot() +\n  geom_sf(\n    aes(fill = incidence_rate),\n    size = .05,\n    color = \"transparent\"\n  ) +\n  facet_wrap(\n    vars(fancy_date),\n    strip.position = \"bottom\"\n  ) +\n  scale_fill_discrete_sequential(\n    palette = \"Rocket\",\n    name = \"COVID-19 INCIDENCE RATE\",\n    guide = guide_legend(\n      title.position = \"top\",\n      title.hjust = .5,\n      title.theme = element_text(\n        family = \"Times New Roman\",\n        size = rel(9),\n        margin = margin(\n          b = .1,\n          unit = \"cm\"\n        )\n      ),\n      nrow = 1,\n      keyheight = unit(.3, \"cm\"),\n      keywidth = unit(.3, \"cm\"),\n      label.theme = element_text(\n        family = \"Times New Roman\",\n        size = rel(6),\n        margin = margin(\n          r = 5,\n          unit = \"pt\"\n        )\n      )\n    )\n  ) +\n  labs(\n    title = \"2021 · A pandemic year\",\n    caption = \"Incidence rates are calculated for 100,000 people in each state.\n                  Inspired from a graphic in the DIE ZEIT newspaper of November 18, 2021.\n                  Data from NY Times · Tidytuesday Week-1 2022 · Abdoul ISSA BIDA.\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(\n      family = \"Times New Roman\",\n      color = \"#111111\"\n    ),\n    plot.title = element_text(\n      size = rel(2.5),\n      face = \"bold\",\n      hjust = 0.5,\n      margin = margin(\n        t = .25,\n        b = .25,\n        unit = \"cm\"\n      )\n    ),\n    plot.caption = element_text(\n      hjust = .5,\n      face = \"bold\",\n      margin = margin(\n        t = .25,\n        b = .25,\n        unit = \"cm\"\n      )\n    ),\n    strip.text = element_text(\n      size = rel(0.75),\n      face = \"bold\"\n    ),\n    legend.position = \"top\",\n    legend.box.spacing = unit(.25, \"cm\"),\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = margin(\n      t = .25,\n      r = .25,\n      b = .25,\n      l = .25,\n      unit = \"cm\"\n    ),\n    plot.background = element_rect(\n      fill = \"#e5e4e2\",\n      color = NA\n    )\n  )\n\nThe scale_fill_discrete_sequential() function, from the colorspace package, sets the color scale. This code uses the rocket palette (the same palette that Cédric Scherer and Georgios Karamanis used in Chapter 2) and changes the legend title to “COVID-19 INCIDENCE RATE.” The guide_legend() function adjusts the position, alignment, and text properties of the title. The code then puts the colored squares in one row, adjusts their height and width, and tweaks the text properties of the labels (&gt;0, &gt;5, and so on).\nNext, the labs() function adds a title and caption. Following theme_minimal(), the theme() function makes some design tweaks, including setting the font and text color; making the title and caption bold; and adjusting their size, alignment, and margins. The code then adjusts the size of the strip text (Jan. 01, Jan. 02, and so on) and makes it bold, puts the legend at the top of the maps, and adds a bit of spacing around it. Grid lines, as well as the longitude and latitude lines, are removed, and then the entire visual- ization gets a bit of padding and a light gray background.\nThere you have it! Figure 4.10 shows the re-creation of his COVID-19 map.\n\n\n\n\n\n\n\nFigure 4.10: The re-creation of Abdoul Madjid’s map\n\n\n\n\nFrom data import and data cleaning to analysis and visualization, you’ve seen how Madjid made a beautiful map in R.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#making-your-own-maps",
    "href": "maps.html#making-your-own-maps",
    "title": "4  Maps and Geospatial Data",
    "section": "Making Your Own Maps",
    "text": "Making Your Own Maps\nYou may now be wondering, Okay, great, but how do I actually make my own maps? In this section you’ll learn where you can find geospatial data, how to choose a projection, and how to prepare the data for mapping.\nThere are two ways to access simple features geospatial data. The first is to import raw data, and the second is to access it with R functions.\nImporting Raw Data\nGeospatial data can come in various formats. While ESRI shapefiles (with the .shp extension) are the most common, you might also encounter GeoJSON files (.geojson) like the ones we used in the Wyoming example at the beginning of this chapter, KML files (.kml), and others. Chapter 8 of Geocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow discusses this range of formats.\nThe good news is that a single function can read pretty much any type of geospatial data: read_sf() from the sf package. Say you’ve downloaded geospatial data about US state boundaries from the website &lt;geojson.xyz&gt; in GeoJSON format, then saved it in the data folder as states.geojson. To import this data, use the read_sf() function like so:\n\nus_states &lt;- read_sf(dsn = \"https://data.rfortherestofus.com/states.geojson\")\n\nThe dsn argument (which stands for data source name) tells read_sf() where to find the file. You save the data as the object us_states.\nAccessing Geospatial Data Using R Functions\nSometimes you’ll have to work with raw data in this way, but not always. That’s because certain R packages provide functions for accessing geospatial data. Madjid used the usa_sf() function from the albersusa package to acquire his data. Another package for accessing geospatial data related to the United States, tigris, has a number of well-named functions for different types of data. For example, load the tigris package and run the states() function like so:\n\nlibrary(tigris)\n\nstates_tigris &lt;- states(\n  cb = TRUE,\n  resolution = \"20m\",\n  progress_bar = FALSE\n)\n\nThe cb = TRUE argument opts out of using the most detailed shapefile and sets the resolution to a more manageable 20m (1:20 million). Without these changes, the resulting shapefile would be large and slow to work with. Setting progress_bar = FALSE hides the messages that tigris generates as it loads data. The result is saved as states_tigris. The tigris package has functions to get geospatial data about counties, census tracts, roads, and more.\nIf you’re looking for data outside the United States, the rnaturalearth package provides functions for importing geospatial data from across the world. For example, use ne_countries() to retrieve geospatial data about various countries:\n\nlibrary(rnaturalearth)\n\nafrica_countries &lt;- ne_countries(\n  returnclass = \"sf\",\n  continent = \"Africa\"\n)\n\nThis code uses two arguments: returnclass = \"sf\" to get data in simple features format, and continent = \"Africa\" to get only countries on the African continent. If you save the result to an object called africa_countries, you can plot the data on a map as follows:\n\nafrica_countries %&gt;%\n  ggplot() +\n  geom_sf()\n\nFigure 4.11 shows the resulting map.\n\n\n\n\n\n\n\nFigure 4.11: A map of Africa made with data from the rnaturalearth package\n\n\n\n\nIf you can’t find an appropriate package, you can always fall back on using read_sf() from the sf package.\nUsing Appropriate Projections\nOnce you have access to geospatial data, you need to decide which projection to use. If you’re looking for a simple answer to this question, you’ll be disappointed. As Geocomputation with R puts it, “The question of which CRS [to use] is tricky, and there is rarely a ‘right’ answer.”\nIf you’re overwhelmed by the task of choosing a projection, the crsuggest package from Kyle Walker can give you ideas. Its suggest_top_crs() function returns a coordinate reference system that is well suited for your data. Load crsuggest and try it out on your africa_countries data:\n\nlibrary(crsuggest)\n\nafrica_countries %&gt;%\n  suggest_top_crs()\n\nThe suggest_top_crs() function should return projection number 28232. Pass this value to the st_transform() function to change the projection before you plot:\n\nafrica_countries %&gt;%\n  st_transform(28232) %&gt;%\n  ggplot() +\n  geom_sf()\n\nWhen run, this code generates the map in Figure 4.12.\n\n\n\n\n\n\n\nFigure 4.12: A map of Africa made with projection number 28232\n\n\n\n\nAs you can see, you’ve successfully mapped Africa with a different projection.\nWrangling Your Geospatial Data\nThe ability to merge traditional data frames with geospatial data is a huge benefit of working with simple features data. Remember that for his COVID-19 map, Madjid analyzed traditional data frames before merging them with geospatial data. But because simple features data acts just like traditional data frames, you can just as easily apply the data-wrangling and analysis functions from the tidyverse directly to a simple features object. To see how this works, revisit the africa_countries simple features data and select two variables (name and pop_est) to see the name and population of the countries:\n\nafrica_countries %&gt;%\n  select(name, pop_est)\n\nThe output looks like the following:\n\n\nSimple feature collection with 51 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999\nGeodetic CRS:  WGS 84\nFirst 10 features:\n              name  pop_est                       geometry\n2         Tanzania 58005463 MULTIPOLYGON (((33.90371 -0...\n3        W. Sahara   603253 MULTIPOLYGON (((-8.66559 27...\n12 Dem. Rep. Congo 86790567 MULTIPOLYGON (((29.34 -4.49...\n13         Somalia 10192317 MULTIPOLYGON (((41.58513 -1...\n14           Kenya 52573973 MULTIPOLYGON (((39.20222 -4...\n15           Sudan 42813238 MULTIPOLYGON (((24.56737 8....\n16            Chad 15946876 MULTIPOLYGON (((23.83766 19...\n26    South Africa 58558270 MULTIPOLYGON (((16.34498 -2...\n27         Lesotho  2125268 MULTIPOLYGON (((28.97826 -2...\n49        Zimbabwe 14645468 MULTIPOLYGON (((31.19141 -2...\n\n\nSay you want to make a map showing which African countries have pop- ulations larger than 20 million. First, you’ll need to calculate this value for each country. To do so, use the mutate() and if_else() functions, which will return TRUE if a country’s population is over 20 million and FALSE otherwise, and then store the result in a variable called population_above_20_million:\n\nafrica_countries %&gt;%\n  select(name, pop_est) %&gt;%\n  mutate(population_above_20_million = if_else(pop_est &gt; 20000000, TRUE, FALSE))\n\nYou can then take this code and pipe it into ggplot, setting the fill aes- thetic property to be equal to population_above_20_million:\n\nafrica_countries %&gt;%\n  select(name, pop_est) %&gt;%\n  mutate(population_above_20_million = if_else(pop_est &gt; 20000000, TRUE, FALSE)) %&gt;%\n  ggplot(aes(fill = population_above_20_million)) +\n  geom_sf()\n\nThis code generates the map shown in Figure 4-13.\n\n\n\n\n\n\n\nFigure 4.13: A map of Africa that highlights countries with populations above 20 million people\n\n\n\n\nThis is a basic example of the data wrangling and analysis you can perform on simple features data. The larger lesson is this: any skill you’ve developed for working with data in R will serve you well when working with geospatial data.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#summary",
    "href": "maps.html#summary",
    "title": "4  Maps and Geospatial Data",
    "section": "Summary",
    "text": "Summary\nIn this short romp through the world of mapmaking in R, you learned the basics of simple features geospatial data, reviewed how Abdoul Madjid applied this knowledge to make his map, explored how to get your own geo- spatial data, and saw how to project it appropriately to make your own maps.\nR may very well be the best tool for making maps. It also lets you use the skills you’ve developed for working with traditional data frames and the ggplot code to make your visualizations look great. After all, Madjid isn’t a GIS expert, but he combined a basic understanding of geospatial data, fun- damental R skills, and knowledge of data visualization principles to make a beautiful map. Now it’s your turn to do the same.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#additional-resources",
    "href": "maps.html#additional-resources",
    "title": "4  Maps and Geospatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nKieran Healy, “Draw Maps,” in Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nAndrew Heiss, “Lessons on Space from Data Visualization: Use R, ggplot2, and the Principles of Graphic Design to Create Beautiful and Truthful Visualizations of Data,” online course, last updated July 11, 2022, https://datavizs22.classes.andrewheiss.com/content/12-content/.\nRobin Lovelace, Jakub Nowosad, and Jannes Muenchow, Geocomputation with R (Boca Raton, FL: CRC Press, 2019), https://r.geocompx.org.\nKyle Walker, Analyzing US Census Data: Methods, Maps, and Models in R (Boca Raton, FL: CRC Press, 2023). https://walker-data.com/census-r/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "5  Designing Effective Tables",
    "section": "",
    "text": "Creating a Data Frame\nYou will begin by creating a data frame that you can use to make tables throughout this chapter. First, load the packages you need (the tidyverse for general data manipulation functions, gapminder for the data you’ll use, gt to make the tables, and gtExtras to do some table formatting):\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(gt)\nlibrary(gtExtras)\nAs you saw in Chapter 2, the gapminder package provides country-level demographic statistics. To make a data frame for your table, you’ll use just a few countries (the first four, in alphabetical order: Afghanistan, Albania, Algeria, and Angola) and three years (1952, 1972, and 1992). The gapminder data has many years, but these will suffice to demonstrate table-making principles. The following code creates a data frame called gdp:\ngdp &lt;- gapminder %&gt;%\n  filter(country %in% c(\"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\")) %&gt;%\n  select(country, year, gdpPercap) %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  pivot_wider(\n    id_cols = country,\n    names_from = year,\n    values_from = gdpPercap\n  ) %&gt;%\n  select(country, `1952`, `1972`, `1992`) %&gt;%\n  rename(Country = country)\nHere’s what gdp looks like:\n# A tibble: 4 × 4\n  Country     `1952` `1972` `1992`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan   779.   740.   649.\n2 Albania      1601.  3313.  2497.\n3 Algeria      2449.  4183.  5023.\n4 Angola       3521.  5473.  2628.\nNow that you have some data, you’ll use it to make a table.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#table-design-principles",
    "href": "tables.html#table-design-principles",
    "title": "5  Designing Effective Tables",
    "section": "Table Design Principles",
    "text": "Table Design Principles\nUnsurprisingly, the principles of good table design are similar to those for data visualization more generally. This section covers six of the most important ones.\nMinimize Clutter\nYou can minimize clutter in your tables by removing unnecessary elements. For example, one common source of table clutter is grid lines, as shown in Figure 5.1.\n\n\n\n\n\n\n\nFigure 5.1: A table with grid lines everywhere can be distracting.\n\n\n\n\nHaving grid lines around every single cell in your table is unnecessary and distracts from the goal of communicating clearly. A table with minimal or even no grid lines (Figure 5.2) is a much more effective communication tool.\n\n\n\n\n\n\n\nFigure 5.2: A table with only horizontal grid lines is more effective.\n\n\n\n\nI mentioned that gt uses good table design principles by default, and this is a great example. The second table, with minimal grid lines, requires just two lines of code—piping the gdp data into the gt() function, which creates a table:\n\ngdp %&gt;%\n  gt()\n\nTo add grid lines to every part of the example, you’d have to add more code. Here, the code that follows the gt() function adds grid lines:\n\ngdp %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(\n      side = \"all\",\n      color = \"black\",\n      weight = px(1),\n      style = \"solid\"\n    ),\n    locations = list(\n      cells_body(\n        everything()\n      ),\n      cells_column_labels(\n        everything()\n      )\n    )\n  ) %&gt;%\n  opt_table_lines(extent = \"none\")\n\nSince I don’t recommend taking this approach, I won’t walk you through this code. However, if you wanted to remove additional grid lines, you could do so like this:\n\ngdp %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  )\n\nThe tab_style() function uses a two-step approach. First, it identifies the style to modify (in this case, the borders), then it specifies where to apply these modifications. Here, tab_style() tells R to modify the borders using the cell_borders() function, making the borders transparent, and to apply this transformation to the cells_body() location (versus, say, the cells_column_labels() for only the first row).\n\n\n\n\n\n\nTo see all options, check out the list of so-called helper functions on the gt package documentation website at https://gt.rstudio.com/reference/index.html#helper-functions.\n\n\n\nRunning this code outputs a table with no grid lines at all in the body (Figure 5.3).\n\n\n\n\n\n\n\nFigure 5.3: A clean-looking table with grid lines only on the header row and the bottom\n\n\n\n\nSave this table as an object called table_no_gridlines so that you can add to it later.\nDifferentiate the Header from the Body\nWhile reducing clutter is an important goal, going too far can have negative consequences. A table with no grid lines at all can make it hard to differen- tiate between the header row and the table body. Consider Figure 5.4, for example.\n\n\n\n\n\n\n\nFigure 5.4: An unclear table with all grid lines removed\n\n\n\n\nBy making the header row bold, you can make it stand out better:\n\ntable_no_gridlines %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )\n\nStarting with the table_no_gridlines object, this code applies formatting with the tab_style() function in two steps. First, it specifies that it wants to alter the text style by using the cell_text() function to set the weight to bold. Second, it sets the location for this transformation to the header row using the cells_column_labels() function. Figure 5.5 shows what the table looks like with its header row bolded.\n\n\n\n\n\n\n\nFigure 5.5: Making the header row more obvious using bold\n\n\n\n\nSave this table as table_bold_header in order to add further formatting.\n\ntable_bold_header &lt;- table_no_gridlines %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )\n\nAlign Appropriately\nA third principle of high-quality table design is appropriate alignment. Specifically, numbers in tables should be right-aligned. Tom Mock explains that left-aligning or center-aligning numbers “impairs the ability to clearly compare numbers and decimal places. Right alignment lets you align decimal places and numbers for easy parsing.”\nLet’s look at this principle in action. In Figure 5.6, the 1952 column is left-aligned, the 1972 column is center-aligned, and the 1992 column is right-aligned.\n\n\n\n\n\n\n\nFigure 5.6: Comparing numerical data aligned to the left (1952), center (1972), and right (1992)\n\n\n\n\nYou can see how much easier it is to compare the values in the 1992 column than those in the other two columns. In both the 1952 and 1972 columns, it’s challenging to compare the values because the numbers in the same position (the tens place, for example) aren’t aligned vertically. In the 1992 column, however, the number in the tens place in Afghanistan (4) aligns with the number in the tens place in Albania (9) and all other coun- tries, making it much easier to scan the table.\nAs with other tables, you actually have to override the defaults to get the gt package to misalign the columns, as demonstrated in the following code:\n\ntable_bold_header %&gt;%\n  cols_align(\n    align = \"left\",\n    columns = 2\n  ) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = 3\n  ) %&gt;%\n  cols_align(\n    align = \"right\",\n    columns = 4\n  )\n\nBy default, gt will right-align numeric values. Don’t change anything, and you’ll be golden.\nRight alignment is best practice for numeric columns, but for text col- umns, use left alignment. As Jon Schwabish points out in his article “Ten Guidelines for Better Tables” in the Journal of Benefit-Cost Analysis, it’s much easier to read longer text cells when they are left-aligned. To see the ben- efit of left-aligning text, add a country with a long name to your table. I’ve added Bosnia and Herzegovina and saved this as a data frame called gdp_with_bosnia. You’ll see that I’m using nearly the same code I used previously to create the gdp data frame:\nHere’s what the gdp_with_bosnia data frame looks like:\n\ngdp_with_bosnia\n\n# A tibble: 5 × 4\n  Country                `1952` `1972` `1992`\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan              779.   740.   649.\n2 Albania                 1601.  3313.  2497.\n3 Algeria                 2449.  4183.  5023.\n4 Angola                  3521.  5473.  2628.\n5 Bosnia and Herzegovina   974.  2860.  2547.\n\n\nNow take the gdp_with_bosnia data frame and create a table with the Country column center-aligned. In the table in Figure 5.7, it’s hard to scan the country names, and that center-aligned column just looks a bit weird.\n\n\n\n\n\n\n\nFigure 5.7: Center-aligned text can be hard to read, especially when it includes longer values.\n\n\n\n\nThis is another example where you have to change the gt defaults to mess things up. In addition to right-aligning numeric columns by default, gt left-aligns character columns. As long as you don’t touch anything, you’ll get the alignment you’re looking for.\nIf you ever do want to override the default alignments, you can use the cols_align() function. For example, here’s how to make the table with center-aligned country names:\n\ngdp_with_bosnia %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  cols_align(\n    columns = \"Country\",\n    align = \"center\"\n  )\n\nThe columns argument tells gt which columns to align, and the align argument selects the alignment (left, right, or center).\nUse the Correct Level of Precision\nIn all of the tables you’ve made so far, you’ve used the data exactly as it came to you. The data in the numeric columns, for example, extends to four decimal places—almost certainly too many. Having more decimal places makes a table harder to read, so you should always strike a balance between what Jon Schwabish describes as “necessary precision and a clean, spare table.”\nHere’s a good rule of thumb: if adding more decimal places would change some action, keep them; otherwise, take them out. In my experience, people tend to leave too many decimal places in, putting too much importance on a very high degree of accuracy (and, in the process, reducing the legibility of their tables).\nIn the GDP table, you can use the fmt_currency() function to format the numeric values:\n\ntable_bold_header %&gt;%\n  fmt_currency(\n    columns = c(`1952`, `1972`, `1992`),\n    decimals = 0\n  )\n\nThe gt package has a whole series of functions for formatting values in tables, all of which start with fmt_. This code applies fmt_currency() to the 1952, 1972, and 1992 columns, then uses the decimals argument to tell fmt_currency() to format the values with zero decimal places. After all, the difference between a GDP of $779.4453 and $779 is unlikely to lead to different decisions.\nThis produces values formatted as dollars. The fmt_currency() function automatically adds a thousands-place comma to make the values even easier to read (Figure 5.8).\n\n\n\n\n\n\n\nFigure 5.8: Rounding dollar amounts to whole numbers and adding dollar signs can simplify data.\n\n\n\n\nSave your table for reuse as table_whole_numbers.\nUse Color Intentionally\nSo far, our table hasn’t used any color. We’ll add some now to highlight outlier values. Especially for readers who want to scan your table, highlighting outliers with color can help significantly. Let’s make the highest value in the year 1952 a different color. To do this, we again use the tab_style() function:\n\ntable_whole_numbers %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1952`,\n      rows = `1952` == max(`1952`)\n    )\n  )\n\nThis function uses cell_text() to change the color of the text to orange and make it bold. Within the cells_body() function, the locations() function specifies the columns and rows to which the changes will apply. The columns argument is simply set to the year whose values are being changed, but setting the rows requires a more complicated formula. The code rows =1952== max(1952) applies the text transformation to rows whose value is equal to the maximum value in that year.\nRepeating this code for the 1972 and 1992 columns generates the result shown in Figure 5.9 (which represents the orange values in grayscale for print purposes).\n\n\n\n\n\n\n\nFigure 5.9: Using color to highlight important values, such as the largest number in each year\n\n\n\n\nThe gt package makes it straightforward to add color to highlight outlier values.\nAdd a Data Visualization Where Appropriate\nAdding color to highlight outliers is one way to help guide the reader’s attention. Another way is to incorporate graphs into tables. Tom Mock developed an add-on package for gt called gtExtras that makes it possible to do just this. For example, say you want to show how the GDP of each country changes over time. To do that, you can add a new column that visualizes this trend using a sparkline (essentially, a simple line chart):\n\ngdp_with_trend &lt;- gdp %&gt;%\n  group_by(Country) %&gt;%\n  mutate(Trend = list(c(`1952`, `1972`, `1992`))) %&gt;%\n  ungroup()\n\nThe gt_plt_sparkline() function requires you to provide the values needed to make the sparkline in a single column. To accomplish this, the code creates a variable called Trend, using group_by() and mutate(), to hold a list of the values for each country. For Afghanistan, for example, Trend would contain 779.4453145, 739.9811058, and 649.3413952. Save this data as an object called gdp_with_trend.\nNow you create your table as before but add the gt_plt_sparkline() function to the end of the code. Within this function, specify which column to use to create the sparkline (Trend) as follows:\n\ngdp_with_trend %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  fmt_currency(\n    columns = c(`1952`, `1972`, `1992`),\n    decimals = 0\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1952`,\n      rows = `1952` == max(`1952`)\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1972`,\n      rows = `1972` == max(`1972`)\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1992`,\n      rows = `1992` == max(`1992`)\n    )\n  ) %&gt;%\n  gt_plt_sparkline(\n    column = Trend,\n    label = FALSE,\n    palette = c(\"black\", \"transparent\", \"transparent\", \"transparent\", \"transparent\")\n  )\n\nSetting label = FALSE removes text labels that gt_plt_sparkline() adds by default, then adds a palette argument to make the sparkline black and all other elements of it transparent. (By default, the function will make different parts of the sparkline different colors.) The stripped-down sparkline in Figure 5.10 allows the reader to see the trend for each country at a glance.\n\n\n\n\n\n\n\nFigure 5.10: A table with sparklines can show changes in data over time.\n\n\n\n\nThe gtExtras package can do much more than merely create sparklines. Its set of theme functions allows you to make your tables look like those published by FiveThirtyEight, the New York Times, the Guardian, and other news outlets.\nAs an example, try removing the formatting you’ve applied so far and instead use the gt_theme_538() function to style the table. Then take a look at tables on the FiveThirtyEight website. You should see similarities to the one in Figure 5.11.\n\n\n\n\n\n\n\nFigure 5.11: A table redone in the FiveThirtyEight style\n\n\n\n\nAdd-on packages like gtExtras are common in the table-making landscape. If you’re working with the reactable package to make interactive tables, for example, you can also use the reactablefmtr to add interactive sparklines, themes, and more. You’ll learn more about making interactive tables in Chapter 9.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#summary",
    "href": "tables.html#summary",
    "title": "5  Designing Effective Tables",
    "section": "Summary",
    "text": "Summary\nMany of the tweaks you made to your table in this chapter are quite subtle. Changes like removing excess grid lines, bolding header text, right-aligning numeric values, and adjusting the level of precision can often go unnoticed, but if you skip them, your table will be far less effective. The final product isn’t flashy, but it does communicate clearly.\nYou used the gt package to make your high-quality table, and as you’ve repeatedly seen, this package has good defaults built in. Often, you don’t need to change much in your code to make effective tables. But no matter which package you use, it’s essential to treat tables as worthy of just as much thought as other kinds of data visualization.\nIn Chapter 6, you’ll learn how to create reports using R Markdown, which can integrate your tables directly into the final document. What’s better than using just a few lines of code to make publication-ready tables?",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#additional-resources",
    "href": "tables.html#additional-resources",
    "title": "5  Designing Effective Tables",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThomas Mock, “10+ Guidelines for Better Tables in R,” The MockUp, September 4, 2020, https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/.\nAlbert Rapp, “Creating Beautiful Tables in R with {gt},” November 27, 2022, https://gt.albert-rapp.de.\nJon Schwabish, “Ten Guidelines for Better Tables,” Journal of Benefit-Cost Analysis 11, no. 2 (2020), https://doi.org/10.1017/bca.2020.11.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "6  R Markdown Reports",
    "section": "",
    "text": "Creating an R Markdown Document\nTo create an R Markdown document in RStudio, go to File &gt; New File &gt; R Markdown. Choose a title, author, and date, as well as your default output format (HTML, PDF, or Word). These values can be changed later. Click OK, and RStudio will create an R Markdown document with some placeholder content, as shown in Figure 6.1.\nFigure 6.1: The placeholder content in a new R Markdown document\nThe Knit menu at the top of RStudio converts an R Markdown document to the format you selected when creating it. In this example, the output format is set to be Word, so RStudio will create a Word document when you knit.\nDelete the document’s placeholder content. In the next section, you’ll replace it with your own.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#document-structure",
    "href": "rmarkdown.html#document-structure",
    "title": "6  R Markdown Reports",
    "section": "Document Structure",
    "text": "Document Structure\nTo explore the structure of an R Markdown document, you’ll create a report about penguins using data from the palmerpenguins package intro- duced in Chapter 3. I’ve separated the data by year, and you’ll use just the 2007 data. Figure 6.2 shows the complete R Markdown document, with boxes surrounding each section.\n\n\n\n\n\n\n\nFigure 6.2: Components of an R Markdown document\n\n\n\n\nThe YAML Metadata\nThe YAML section is the very beginning of an R Markdown document. The name YAML comes from the recursive acronym YAML ain’t markup language, whose meaning isn’t important for our purposes. Three dashes indicate its beginning and end, and the text inside of it contains metadata about the R Markdown document:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput: word_document\n---\n\nAs you can see, the YAML provides the title, author, date, and output format. All elements of the YAML are given in key: value syntax, where each key is a label for a piece of metadata (for example, the title) followed by the value.\nThe R Code Chunks\nR Markdown documents have a different structure from the R script files you might be familiar with (those with the .R extension). R script files treat all content as code unless you comment out a line by putting a hash mark (#) in front of it. In the following listing, the first line is a comment, and the second line is code:\n\n```{r}\n# Import our data\ndata &lt;- read_csv(\"data.csv\")\n```\n\nIn R Markdown, the situation is reversed. Everything after the YAML is treated as text unless you specify otherwise by creating code chunks. These start with three backticks (```), followed by the lowercase letter r surrounded by curly brackets ({}). Another three backticks indicate the end of the code chunk:\n\n```{r}\nlibrary(tidyverse)\n```\n\nIf you’re working in RStudio, code chunks should have a light gray background. R Markdown treats anything in the code chunk as R code when you knit. For example, this code chunk will produce a histogram in the final Word document:\n\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nFigure 6.3 shows the resulting histogram.\n\n\n\n\n\n\n\nFigure 6.3: A simple histogram generated by an R Markdown code chunk\n\n\n\n\nA code chunk at the top of each R Markdown document, known as the setup code chunk, gives instructions for what should happen when knitting a document. It contains the following options:\n\necho Do you want to show the code itself in the knitted document?\ninclude Do you want to show the output of the code chunk?\nmessage Do you want to include any messages that code might generate? For example, this message shows up when you run library(tidyverse):\n\n── Attaching core tidyverse packages ───── tidyverse 1.3.2.9000 ──\n✔ dplyr     1.0.10     ✔ readr     2.1.3 \n✔ forcats   0.5.2      ✔ stringr   1.5.0 \n✔ ggplot2   3.4.0      ✔ tibble    3.1.8 \n✔ lubridate 1.9.0      ✔ tidyr     1.2.1 \n✔ purrr     1.0.1      \n── Conflicts───── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nwarning Do you want to include any messages that the code might generate? For example, here’s the message you get when creating a histogram using geom_histogram():\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nTo see the full list of code chunk options, visit https://yihui.org/knitr/options/.\n\n\n\nIn cases where you’re using R Markdown to generate a report for a non-R user, you likely would want to hide the code, messages, and warnings but show the output (which would include any visualizations you generate). The following setup code chunk does this:\n\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(\n  include = TRUE,\n  echo = FALSE,\n  message = FALSE,\n  warning = FALSE\n)\n```\n\nThe include = FALSE option on the first line applies to the setup code chunk itself. It tells R Markdown not to include the output of the setup code chunk when knitting. The options within knitr::opts_chunk$set() apply to all future code chunks. However, you can also override these global code chunk options on individual chunks. If you wanted your Word document to show both the plot itself and the code used to make it, for example, you could set echo = TRUE for that code chunk only:\n\n```{r echo = TRUE}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nBecause include is already set to TRUE within knitr::opts_chunk$set() in the setup code chunk, you don’t need to specify it again.\nMarkdown Text\nMarkdown is a way to style text. If you were writing directly in Word, you could just press the B button to make text bold, for example, but R doesn’t have such a button. If you want your knitted Word document to include bold text, you need to use Markdown to indicate this style in the document.\nMarkdown text sections (which have a white background in RStudio) will be converted into formatted text in the Word document after knitting. Figure 6.4 highlights the equivalent sections in the R Markdown and Word documents.\n\n\n\n\n\n\n\nFigure 6.4: Markdown text in R Markdown and its equivalent in a knitted Word document\n\n\n\n\nThe text # Introduction in R Markdown gets converted to a first-level heading, while ## Bill Length becomes a second-level heading. By adding hashes, you can create up to six levels of headings. In RStudio, headings are easy to find because they show up in blue.\nText without anything before it becomes body text in Word. To create italic text, add single asterisks around it (*like this*). To make text bold, use double asterisks (**as shown here**).\nYou can make bulleted lists by placing a dash at the beginning of a line and adding your text after it:\n\n- Adelie\n- Gentoo\n- Chinstrap\n\nTo make ordered lists, replace the dashes with numbers. You can either number each line consecutively or, as done below, repeat 1. In the knitted document, the proper numbers will automatically generate.\n\n1. Adelie\n1. Gentoo\n1. Chinstrap\n\nFormatting text in Markdown might seem more complicated than doing so in Word. But if you want to switch from a multi-tool workflow to a reproducible R Markdown–based workflow, you need to remove all manual actions from the process so that you can easily repeat it in the future.\nInline R Code\nR Markdown documents can also include little bits of code within Markdown text. To see how this inline code works, take a look at the following sentence in the R Markdown document:\n\nThe average bill length is `r average_bill_length` millimeters.\n\nInline R code begins with a backtick and the lowercase letter r and ends with another backtick. In this example, the code tells R to print the value of the variable average_bill_length, which is defined as follows in the code chunk before the inline code:\n\n```{r}\naverage_bill_length &lt;- penguins %&gt;%\n  summarize(avg_bill_length = mean(\n    bill_length_mm,\n    na.rm = TRUE\n  )) %&gt;%\n  pull(avg_bill_length)\n```\n\nThis code calculates the average bill length and saves it as average_bill_length. Having created this variable, you can now use it in the inline code. As a result, the Word document includes the sentence “The average bill length is 43.9219298.”\nOne benefit of using inline R code is that you avoid having to copy and paste values, which is error-prone. Inline R code also makes it possible to automatically calculate values on the fly whenever you reknit the R Markdown document with new data. To see how this works, you’ll make a new report using data from 2008. To do this, you need to change only one line, the one that reads the data:\n\npenguins &lt;- read_csv(\"https://data.rfortherestofus.com/penguins-2008.csv\")\n\nNow that you’ve switched penguins-2007.csv to penguins-2008.csv, you can reknit the report and produce a new Word document, complete with updated results. Figure 6.5 shows the new document.\n\n\n\n\n\n\n\nFigure 6.5: The knitted Word document with 2008 data\n\n\n\n\nThe new histogram is based on the 2008 data, as is the average bill length of 43.5412281. These values update automatically because every time you press Knit, the code is rerun, regenerating plots and recalculating values. As long as the data you use has a consistent structure, updating a report requires just a click of the Knit button.\nRunning Code Chunks Interactively\nYou can run the code in an R Markdown document in two ways. The first is by knitting the entire document. The second is to run code chunks manually (also known as interactively) by pressing the green play button at the top right of a code chunk. The down arrow next to the green play button will run all code until that point. You can see these buttons in Figure 6.6.\n\n\n\n\n\n\n\nFigure 6.6: The buttons on code chunks in RStudio\n\n\n\n\nYou can also use command-enter on macOS or ctrl-enter on Windows to run sections of code, as in an R script file. Running code interactively is a good way to test that portions of it work before you knit the entire document.\nThe one downside to running code interactively is that you can sometimes make mistakes that cause your R Markdown document to fail to knit. That is because, in order to knit, an R Markdown document must contain all the code it uses. If you’re working interactively and, say, load data from a separate file, you won’t be able to knit your document. When working in R Markdown, always keep all your code within a single document.\nThe code must also appear in the right order. An R Markdown document that looks like this, for example, will give you an error if you try to knit it:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput: word_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  include = TRUE,\n  echo = FALSE,\n  message = FALSE,\n  warning = FALSE\n)\n```\n\n```{r}\npenguins &lt;- read_csv(\"https://data.rfortherestofus.com/penguins-2008.csv\")\n```\n\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n```{r}\nlibrary(tidyverse)\n```\n\nThis error happens because you are attempting to use tidyverse functions like read_csv(), as well as various ggplot functions, before you load the tidyverse package.\nAlison Hill, a research scientist and one of the most prolific R Markdown educators, tells her students to knit early and often. This practice makes it easier to isolate issues that make knitting fail. Hill describes her typical R Markdown workflow as spending 75 percent of her time working on a new document and 25 percent of her time knitting to check that the R Markdown document works.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#quarto",
    "href": "rmarkdown.html#quarto",
    "title": "6  R Markdown Reports",
    "section": "Quarto",
    "text": "Quarto\nIn 2022, Posit released a publishing tool similar to R Markdown. Known as Quarto, this tool takes what R Markdown has done for R and extends it to other languages, including Python, Julia, and Observable JS. As I write this book, Quarto is gaining traction. Luckily, the concepts you’ve learned in this chapter apply to Quarto as well. Quarto documents have a YAML section, code chunks, and Markdown text. You can export Quarto documents to HTML, PDF, and Word. However, R Markdown and Quarto documents have some syntactic differences, which are explored further in Chapter 10.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#summary",
    "href": "rmarkdown.html#summary",
    "title": "6  R Markdown Reports",
    "section": "Summary",
    "text": "Summary\nYou started this chapter by considering the scenario of a report that needs to be regenerated monthly. You learned how you can use R Markdown to reproduce this report every month without changing your code. Even if you lost the final Word document, you could quickly re-create it.\nBest of all, working with R Markdown makes it possible to do in seconds what would have previously taken hours. When making a single report requires three tools and five steps, you may not want to work on it. But, as Alison Hill has pointed out, with R Markdown you can even work on reports before you receive all of the data. You could simply write code that works with partial data and rerun it with the final data at any time.\nThis chapter has just scratched the surface of what R Markdown can do. The next chapter will show you how to use it to instantly generate hundreds of reports. Magic indeed!",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#additional-resources",
    "href": "rmarkdown.html#additional-resources",
    "title": "6  R Markdown Reports",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nYihui Xie, J. J. Allaire, and Garrett Grolemund, R Markdown: The Definitive Guide (Boca Raton, FL: CRC Press, 2019), https://bookdown.org/yihui/rmarkdown/.\nYihui Xie, Christophe Dervieux, and Emily Riederer, R Markdown Cookbook (Boca Raton, FL: CRC Press, 2021), https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html",
    "href": "parameterized-reporting.html",
    "title": "7  Parameterized Reporting",
    "section": "",
    "text": "Report Templates in R Markdown\nIf you’ve ever had to create multiple reports at the same time, you know how frustrating it can be, especially if you’re using the multi-tool workflow described in Chapter 6. Making just one report can take a long time. Multiply that work by 10, 20, or, in the case of the Urban Institute team, 51, and it can quickly feel overwhelming. Fortunately, with parameterized reporting, you can generate thousands of reports at once using the following workflow:\nYou’ll begin by creating a report template for one state. I’ve taken the code that the Urban Institute staff used to make their state fiscal briefs and simplified it significantly. All of the packages used are ones you’ve seen in previous chapters, with the exception of the urbnthemes package. This package contains a custom ggplot theme. It can be installed by running remotes::install_github(\"UrbanInstitute/urbnthemes\") in the console. Instead of focusing on fiscal data, I’ve used data you may be more familiar with: COVID-19 rates from mid-2022. Here’s the R Markdown document:\n---\ntitle: \"Urban Institute COVID Report\"\noutput: html_document\nparams:\nstate: \"Alabama\"\n---\n  \n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo = FALSE,\n  warning = FALSE,\n  message = FALSE\n)\n```\n\n```{r}\nlibrary(tidyverse)\nlibrary(urbnthemes)\nlibrary(scales)\n```\n\n# `r params$state`\n\n```{r}\ncases &lt;- tibble(state.name) %&gt;%\n  rbind(state.name = \"District of Columbia\") %&gt;%\n  left_join(\n    read_csv(\n      \"united_states_covid19_cases_deaths_and_testing_by_state.csv\",\n      skip = 2\n    ),\n    by = c(\"state.name\" = \"State/Territory\")\n  ) %&gt;%\n  select(\n    total_cases = `Total Cases`,\n    state.name,\n    cases_per_100000 = `Case Rate per 100000`\n  ) %&gt;%\n  mutate(cases_per_100000 = parse_number(cases_per_100000)) %&gt;%\n  mutate(case_rank = rank(-cases_per_100000, ties.method = \"min\"))\n```\n\n```{r}\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nstate_cases_per_100000 &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(cases_per_100000) %&gt;%\n  comma()\n\nstate_cases_rank &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(case_rank)\n```\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\n```{r fig.height = 8}\nset_urbn_defaults(style = \"print\")\n\ncases %&gt;%\n  mutate(highlight_state = if_else(state.name == params$state, \"Y\", \"N\")) %&gt;%\n  mutate(state.name = fct_reorder(state.name, cases_per_100000)) %&gt;%\n  ggplot(aes(\n    x = cases_per_100000,\n    y = state.name,\n    fill = highlight_state\n  )) +\n  geom_col() +\n  scale_x_continuous(labels = comma_format()) +\n  theme(legend.position = \"none\") +\n  labs(\n    y = NULL,\n    x = \"Cases per 100,000\"\n  )\n```\nThe text and charts in the report come from the cases data frame, shown here:\n# A tibble: 51 × 4\n   total_cases state.name  cases_per_100000 case_rank\n   &lt;chr&gt;       &lt;chr&gt;                  &lt;dbl&gt;     &lt;int&gt;\n 1 1302945     Alabama                26573        18\n 2 246345      Alaska                 33675         2\n 3 2025435     Arizona                27827        10\n 4 837154      Arkansas               27740        12\n 5 9274208     California             23472        36\n 6 1388702     Colorado               24115        34\n 7 766172      Connecticut            21490        43\n 8 264376      Delaware               27150        13\n 9 5965411     Florida                27775        11\n10 2521664     Georgia                23750        35\n# ℹ 41 more rows\nWhen you knit the document, you end up with the simple HTML file shown in Figure 7.1.\nFigure 7.1: The Alabama report generated via R Markdown\nYou should recognize the R Markdown document’s YAML, R code chunks, inline code, and Markdown text from Chapter 6.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#how-parameterized-reporting-works",
    "href": "parameterized-reporting.html#how-parameterized-reporting-works",
    "title": "7  Parameterized Reporting",
    "section": "",
    "text": "Make a report template in R Markdown\nAdd a parameter (for example, one representing US states) in the YAML of your R Markdown document to represent the values that will change between reports\nUse that parameter to generate a report for one state, to make sure you can knit your document\nCreate a separate R script file that sets the value of the parameter and then knits a report\nRun this script for all states\n\n\n\n\n\n\n\n\nUsing Parameters\nIn R Markdown, parameters are variables that we set in the YAML to allow us to create multiple reports. Take a look at these two lines in the YAML:\n\nparams:\n  state: \"Alabama\"\n\nThis code defines a variable, in this case state. We can then use this variable throughout the rest of the R Markdown document using this syntax: params$variable_name, replacing variable_name with state or any name you set in the YAML. For example, take a look at this inline R code:\n\n# `r params$state`\n\nAny instance of params$state will be converted to “Alabama” when we knit it. The line shown here becomes the first-level heading visible in Figure @ref(fig:alabama-covid-report). This parameter shows up again in the following code:\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\nWhen we knit the document, we see the following text:\n\nIn the state of Alabama, there were 26,573 cases per 100,000 people in the last seven days. This puts Alabama at number 18 of 50 states and the District of Columbia.\n\nThis text is automatically generated. The inline R code `r state_text` prints the value of the variable state_text. And state_text is determined by this if_else() statement:\n\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nIf the value of params$states is the District of Columbia, this code makes state_text equal to “the District of Columbia”. If params$state does not equal District of Columbia, then state_text gets the value “state of”, followed by the state name. This allows us to put state_text in a sentence and have it work no matter whether our state parameter is a state or the District of Columbia.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#generating-numbers-with-parameters",
    "href": "parameterized-reporting.html#generating-numbers-with-parameters",
    "title": "7  Parameterized Reporting",
    "section": "Generating Numbers with Parameters",
    "text": "Generating Numbers with Parameters\nYou can also use parameters to generate numeric values to include in the text. For example, to calculate the values of the state_cases_per_100000 and state_cases_rank variables dynamically, use the state parameter, as shown here:\n\nstate_cases_per_100000 &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(cases_per_100000) %&gt;%\n  comma()\n\nstate_cases_rank &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(case_rank)\n\nFirst, this code filters the cases data frame (which contains data for all states) to keep only the data for the state in params$state. Then, the pull() function gets a single value from that data, and the comma() function from the scales package applies formatting to make state_cases_per_100000 display as 26,573 (rather than 26573). Finally, the state_cases_per_100000 and state_case_rank variables are integrated into the inline R code.\nIncluding Parameters in Visualization Code\nThe params$state parameter is used in other places as well, such as to highlight a state in the report’s bar chart. To see how to accomplish this, look at the following section from the last code chunk:\n\ncases %&gt;%\n  mutate(highlight_state = if_else(state.name == params$state, \"Y\", \"N\"))\n\nThis code creates a variable called highlight_state. Within the cases data frame, the code checks whether state.name is equal to params$state. If it is, highlight_state gets the value Y. If not, it gets N. Here’s what the relevant columns look like after you run these two lines:\n\n\n# A tibble: 51 × 2\n   state.name  highlight_state\n   &lt;chr&gt;       &lt;chr&gt;          \n 1 Alabama     Y              \n 2 Alaska      N              \n 3 Arizona     N              \n 4 Arkansas    N              \n 5 California  N              \n 6 Colorado    N              \n 7 Connecticut N              \n 8 Delaware    N              \n 9 Florida     N              \n10 Georgia     N              \n# ℹ 41 more rows\n\n\nLater, the ggplot code uses the highlight_state variable for the bar chart’s fill aesthetic property, highlighting the state in params$state in yellow and coloring the other states blue. Figure 7.2 shows the chart with Alabama highlighted.\n\n\n\n\n\n\n\nFigure 7.2: Highlighting data in a bar chart using parameters\n\n\n\n\nAs you’ve seen, setting a parameter in the YAML allows you to dynamically generate text and charts in the knitted report. But you’ve generated only one report so far. How can you create all 51 reports? Your first thought might be to manually update the YAML by changing the parameter’s value from “Alabama” to, say, “Alaska” and then knitting the document again. While you could follow this process for all states, it would be tedious, which is what you’re trying to avoid. Instead, you can automate the report generation.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#creating-an-r-script",
    "href": "parameterized-reporting.html#creating-an-r-script",
    "title": "7  Parameterized Reporting",
    "section": "Creating an R Script",
    "text": "Creating an R Script\nTo automatically generate multiple reports based on the template you’ve created, you’ll use an R script that changes the value of the parameters in the R Markdown document and then knits it. You’ll begin by creating an R script file named render.R.\nKnitting the Document with Code\nYour script needs to be able to knit an R Markdown document. While you’ve seen how to do this using the Knit button, you can do the same thing with code. Load the rmarkdown package and then use its render() function as shown here:\n\nlibrary(rmarkdown)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alaska.html\",\n  params = list(state = \"Alaska\")\n)\n\nThis function generates an HTML document called urban-covid-budget-report.html. By default, the generated file has the same name as the R Markdown (.Rmd) document, with a different extension. The output_file argument assigns the file a new name, and the params argument specifies parameters that will override those in the R Markdown document itself. For example, this code tells R to use Alaska for the state parameter and save the resulting HTML file as Alaska.html.\nThis approach to generating reports works, but to create all 51 reports, you’d have to manually change the state name in the YAML and update the render() function before running it for each report. In the next section, you’ll update your code to make it more efficient.\nCreating a Tibble with Parameter Data\nTo write code that generates all your reports automatically, first you must create a vector (in colloquial terms, a list of items) of all the state names and the District of Columbia. To do this, you’ll use the built-in dataset state.name, which has all 50 state names in a vector:\n\nstate &lt;- tibble(state.name) %&gt;%\n  rbind(\"District of Columbia\") %&gt;%\n  pull(state.name)\n\nThis code turns state.name into a tibble and then uses the rbind() function to add the District of Columbia to the list. The pull() function gets one single column and saves it as state. Here’s what the state vector looks like:\n\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"Florida\"             \n[10] \"Georgia\"              \"Hawaii\"               \"Idaho\"               \n[13] \"Illinois\"             \"Indiana\"              \"Iowa\"                \n[16] \"Kansas\"               \"Kentucky\"             \"Louisiana\"           \n[19] \"Maine\"                \"Maryland\"             \"Massachusetts\"       \n[22] \"Michigan\"             \"Minnesota\"            \"Mississippi\"         \n[25] \"Missouri\"             \"Montana\"              \"Nebraska\"            \n[28] \"Nevada\"               \"New Hampshire\"        \"New Jersey\"          \n[31] \"New Mexico\"           \"New York\"             \"North Carolina\"      \n[34] \"North Dakota\"         \"Ohio\"                 \"Oklahoma\"            \n[37] \"Oregon\"               \"Pennsylvania\"         \"Rhode Island\"        \n[40] \"South Carolina\"       \"South Dakota\"         \"Tennessee\"           \n[43] \"Texas\"                \"Utah\"                 \"Vermont\"             \n[46] \"Virginia\"             \"Washington\"           \"West Virginia\"       \n[49] \"Wisconsin\"            \"Wyoming\"              \"District of Columbia\"\n\n\nRather than use render() with the input and output_file arguments, as you did earlier, you can pass it the params argument to give it parameters to use when knitting. To do so, create a tibble with the information needed to render all 51 reports and save it as an object called reports, which you’ll pass to the render() function, as follows:\n\nreports &lt;- tibble(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = str_glue(\"{state}.html\"),\n  params = map(state, ~ list(state = .))\n)\n\nThis code generates a tibble with 51 rows and 3 variables. In all rows, the input variable is set to the name of the R Markdown document. The value of output_file is set with str_glue() to be equal to the name of the state, followed by .html (for example, Alabama.html).\nThe params variable is the most complicated of the three. It is what’s known as a named list. This data structure puts the data in the state: state _name format needed for the R Markdown document’s YAML. The map() function from the purrr package creates the named list, telling R to set the value of each row as state = \"Alabama\", then state = \"Alaska\", and so on, for all of the states. You can see these variables in the reports tibble:\n\n\n# A tibble: 51 × 3\n   input                         output_file      params          \n   &lt;chr&gt;                         &lt;glue&gt;           &lt;list&gt;          \n 1 urban-covid-budget-report.Rmd Alabama.html     &lt;named list [1]&gt;\n 2 urban-covid-budget-report.Rmd Alaska.html      &lt;named list [1]&gt;\n 3 urban-covid-budget-report.Rmd Arizona.html     &lt;named list [1]&gt;\n 4 urban-covid-budget-report.Rmd Arkansas.html    &lt;named list [1]&gt;\n 5 urban-covid-budget-report.Rmd California.html  &lt;named list [1]&gt;\n 6 urban-covid-budget-report.Rmd Colorado.html    &lt;named list [1]&gt;\n 7 urban-covid-budget-report.Rmd Connecticut.html &lt;named list [1]&gt;\n 8 urban-covid-budget-report.Rmd Delaware.html    &lt;named list [1]&gt;\n 9 urban-covid-budget-report.Rmd Florida.html     &lt;named list [1]&gt;\n10 urban-covid-budget-report.Rmd Georgia.html     &lt;named list [1]&gt;\n# ℹ 41 more rows\n\n\nThe params variable shows up as &lt;named list&gt;, but if you open the tibble in the RStudio viewer (click reports in the Environment tab), you can see the output more clearly, as shown in Figure 7.3.\n\n\n\n\n\n\n\nFigure 7.3: The named list column in the RStudio viewer\n\n\n\n\nThis view allows you to see the named list in the params variable, with the state variable equal to the name of each state.\nOnce you’ve created the reports tibble, you’re ready to render the reports. The code to do so is only one line long:\n\npwalk(reports, render)\n\nThis pwalk() function (from the purrr package) has two arguments: a data frame or tibble (reports, in this case) and a function that runs for each row of this tibble, render().\n\n\n\n\n\n\nYou don’t include the open and closing parentheses when passing the render() function to pwalk().\n\n\n\nRunning this code runs the render() function for each row in reports, passing in the values for input, output_file, and params. This is equivalent to entering code like the following to run the render() function 51 times (for 50 states plus the District of Columbia):\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alabama.html\",\n  params = list(state = \"Alabama\")\n)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alaska.html\",\n  params = list(state = \"Alaska\")\n)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Arizona.html\",\n  params = list(state = \"Arizona\")\n)\n\nHere’s the full R script file:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(rmarkdown)\n\n# Create a vector of all states and the District of Columbia\nstate &lt;- tibble(state.name) %&gt;%\n  rbind(\"District of Columbia\") %&gt;%\n  pull(state.name)\n\n# Create a tibble with information on the:\n# input R Markdown document\n# output HTML file\n# parameters needed to knit the document\nreports &lt;- tibble(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = str_glue(\"{state}.html\"),\n  params = map(state, ~ list(state = .))\n)\n\n# Generate all of our reports\npwalk(reports, render)\n\nAfter running the pwalk(reports, render) code, you should see 51 HTML documents appear in the files pane in RStudio. Each document consists of a report for that state, complete with a customized graph and accompanying text.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#best-practices",
    "href": "parameterized-reporting.html#best-practices",
    "title": "7  Parameterized Reporting",
    "section": "Best Practices",
    "text": "Best Practices\nWhile powerful, parameterized reporting can present some challenges. For example, make sure to consider outliers in your data. In the case of the state reports, Washington, DC, is an outlier because it isn’t technically a state. The Urban Institute team altered the language in the report text so that it didn’t refer to Washington, DC, as a state by using an if_else() statement, as you saw at the beginning of this chapter.\nAnother best practice is to manually generate and review the reports whose parameter values have the shortest (Iowa, Ohio, and Utah in the state fiscal briefs) and longest (District of Columbia) text lengths. This way, you can identify places where the text length may have unexpected results, such as cut-off chart titles or page breaks disrupted by text running onto multiple lines. A few minutes of manual review can make the process of autogenerating multiple reports much smoother.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#conclusion",
    "href": "parameterized-reporting.html#conclusion",
    "title": "7  Parameterized Reporting",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter, we recreated the Urban Institute’s state fiscal briefs using parameterized reporting. You learned how to add a parameter to your R Markdown document, then use an R script to set the value of that parameter and knit the report.\nAutomating the production of reports can be a huge time-saver, especially as the number of reports to generate grows. Consider another project at the Urban Institute: making county-level reports. With over 3,000 counties in the United States, making these reports by hand is not realistic. Additionally, if the Urban Institute were to make its reports using SPSS, Excel, and Word, they would have to copy and paste values between programs. Humans are fallible, and mistakes occur, no matter how hard we try to avoid them. Computers, on the other hand, do not make copy-and-paste errors. Letting computers handle the tedious work of making multiple reports significantly reduces the chance of error.\nWhen you’re starting out, parameterized reporting might feel like a heavy lift, as you have to make sure that your code works for all versions of your report. But once you have your R Markdown document and accompanying R script file, you’ll find it easy to produce multiple reports at once, saving you work in the end.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#learn-more",
    "href": "parameterized-reporting.html#learn-more",
    "title": "7  Parameterized Reporting",
    "section": "Learn More",
    "text": "Learn More\nConsult the following resources to learn how the Urban Institute has created parameterized reports and how you can make them yourself:\n“Using R Markdown to Track and Publish State Data” by the Data@Urban team (2021), https://urban-institute.medium.com/using-r-markdown-to-track-and-publish-state-data-d1291bfa1ec0\n“Iterated fact sheets with R Markdown” by the Data@Urban team (2018), https://urban-institute.medium.com/iterated-fact-sheets-with-r-markdown-d685eb4eafce",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#report-templates-in-r-markdown",
    "href": "parameterized-reporting.html#report-templates-in-r-markdown",
    "title": "7  Parameterized Reporting",
    "section": "",
    "text": "Make a report template in R Markdown.\nAdd a parameter (for example, one representing US states) in the YAML of your R Markdown document to represent the values that will change between reports.\nUse that parameter to generate a report for one state, to make sure you can knit your document.\nCreate a separate R script file that sets the value of the parameter and then knits a report.\nRun this script for all states.\n\n\n\n\n\n\n\n\nDefining Parameters\nIn R Markdown, parameters are variables that you set in the YAML to allow you to create multiple reports. Take a look at these two lines in the YAML:\n\nparams:\n  state: \"Alabama\"\n\nThis code defines a variable called state. You can use the state variable throughout the rest of the R Markdown document with the params$variable_name syntax, replacing variable_name with state or any other name you set in the YAML. For example, consider this inline R code:\n\n# `r params$state`\n\nAny instance of the params$state parameter will be converted to “Alabama” when you knit it. This parameter and several others appear in the following code, which sets the first-level heading visible in Figure 7.1:\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\nAfter knitting the document, you should see the following text:\n\nIn the state of Alabama, there were 26,573 cases per 100,000 people in the last seven days. This puts Alabama at number 18 of 50 states and the District of Columbia.\n\nThis text is automatically generated. The inline R code `r state_text` prints the value of the variable state_text, which is determined by a previous call to if_else(), shown in this code chunk:\n\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nIf the value of params$states is “District of Columbia”, this code sets state_text equal to “the District of Columbia”. If params$state isn’t “District of Columbia”, then state_text gets the value “state of”, followed by the state name. This allows you to put state_text in a sentence and have it work no matter whether the state parameter is a state or the District of Columbia.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#summary",
    "href": "parameterized-reporting.html#summary",
    "title": "7  Parameterized Reporting",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you re-created the Urban Institute’s state fiscal briefs using parameterized reporting. You learned how to add a parameter to your R Markdown document, then use an R script to set the value of that param- eter and knit the report.\nAutomating report production can be a huge time-saver, especially as the number of reports you need to generate grows. Consider another project at the Urban Institute: making county-level reports. With over 3,000 counties in the United States, creating these reports by hand isn’t realistic. Not only that, but if the Urban Institute employees were to make their reports using SPSS, Excel, and Word, they would have to copy and paste values between programs. Humans are fallible, and mistakes occur, no matter how hard we try to avoid them. Computers, on the other hand, never make copy-and-paste errors. Letting computers handle the tedious work of generating multiple reports reduces the chance of error significantly.\nWhen you’re starting out, parameterized reporting might feel like a heavy lift, as you have to make sure that your code works for every version of your report. But once you have your R Markdown document and accompanying R script file, you should find it easy to produce multiple reports at once, saving you work in the end.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#additional-resources",
    "href": "parameterized-reporting.html#additional-resources",
    "title": "7  Parameterized Reporting",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nData@Urban Team, “Iterated Fact Sheets with R Markdown,” Medium, July 24, 2018, https://urban-institute.medium.com/iterated-fact-sheets-with-r-markdown-d685eb4eafce.\nData@Urban Team, “Using R Markdown to Track and Publish State Data,” Medium, April 21, 2021, https://urban-institute.medium.com/using-r-markdown-to-track-and-publish-state-data-d1291bfa1ec0.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  }
]